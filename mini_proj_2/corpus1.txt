Samsung’s tablet range is vast, so choosing one isn’t easy. This eight-inch Galaxy Tab 4 also comes in 7- and 10-inch versions, all of which are available in black or white, and each model is also available as a 4G version, which we have here. Also see: The best tablets of 2014. Strangely enough, the SIM card isn’t just for data: you can also make phone calls, making this the biggest phablet we’ve ever tested. The Tab 4 is aimed price-conscious end of the market as opposed to the Tab S, which is Samsung’s flagship tablet. The design is only subtly different to the previous model – the Tab 3 – now with a centrally mounted rear camera and the same rear-facing speaker. Also, the microUSB port is now on the bottom edge instead of the side. Below the power and volume buttons are are two pop-out covers: one for the micro SIM card and one for a microSD card. You can use up to 64GB cards. Also see our Samsung Galaxy Tab S 10.5 vs Apple iPad Air comparison review. We don’t like the two touch-sensitive controls either side of the home button because they don’t light up, which means you can’t find them in the dark. We’re also not too keen on the obviously plastic finish – the Tab 4 looks like a budget tablet. Despite theThis makes text look fuzzier than on the cheaper LG G Pad 8.3. Some people may not find this an issue, but it’s a disappointment considering the price. It appears to be the same screen used in the Tab 3, with vivid colours and decent viewing angles.  relatively steep price, the screen has a low resolution. One upgrade over the Tab 3 is a quad-core processor, but it runs slower than the Tab 3’s dual-core chip. It’s fast enough for basic tasks such as email and web browsing as well as running two apps on screen at the same time, but we’d expect better for the money. Although you’ll be able to play the latest games, graphics quality is likely to be pared back in order to maintain smooth gameplay. One upgrade over the Tab 3 is a quad-core processor, but it runs slower than the Tab 3’s dual-core chip. It’s fast enough for basic tasks such as email and web browsing as well as running two apps on screen at the same time, but we’d expect better for the money. Although you’ll be able to play the latest games, graphics quality is likely to be pared back in order to maintain smooth gameplay. The Tab 4 comes with Android KitKat and Samsung’s usual Touchwiz interface. It’s fairly easy to use, and it’s sometimes handy to be able to run two apps on screen at once. So, you could have a YouTube video playing while you browse the web or check your email and it’s easy to adjust how much space to give each app by dragging the dividing line. See also: Best Android tablets 2014: here's where we bring to you the 22 best Android tablets in the UK right now. There are both front and rear cameras, but both are poor quality. The main camera is only three megapixels, and the photos and videos it takes aren’t great. Video is shaky and lacks detail, and photos are ok only if you’re uploading snaps to Facebook. Even though the Tab 4 8.0 is one of Samsung’s cheapest tablets, it’s no bargain. You’ll save around £45 by opting for the Wi-Fi-only version, but even then, you’re better off with LG’s G Pad 8.3 which has a full-HD screen and a better processor. What happens when you are not able to go to hospital for an ECG scan? Then, the ECG can come to you. And the physician will conduct the scan with a click on hisher smartphone. Call it using social media to deliver cardiac care. That's what over 50 physicians learnt on Monday at a conference in Sri Sathya Sai Institute of Higher Medical Sciences. A group of 20 well-known physicians and senior cardiac sonographers from the American Society of Echocardiography Foundation (ASEF), the world's leading specialty organization dedicated to the use of echocardiography (ultrasound-based heart scan), took part in the conference to equip medicos with recent technology using smartphones. The team discussed rheumatic heart disease that is very common in India but rare in the West, and transfer of technology that will help doctors here.The conference aims at developing an international scientific database that will be used to modify the current treatment pattern for patients with rheumatic heart disease. "It's about taking hospital to a patient and car rying a doctor in pocket in the form of a smartphone. These days, everyone has a smartphone and it can be used in diagnosis. For example, a smartphone android application called iscan works like a single lead ECG, which physicians can use in cases suspected. Physicians can in turn get in touch with specialists. All this makes it easier for us to deliver quality cardiac care in remote parts of the country," said Dr Srikanth Sola, senior consultant -cardiology, at Sai hospital. The programme will assess the utility of smartphone-based technologies and hand-held ultrasound in the diagnosis and treatment of patients with complex valvular heart disease, he added. ASEF members demonstrated latest technologies in the arena of cardiac care such as use of a smartphone to run a heart scan and diagnose conditions such as rheumatic heart disease. They also have many applications using the social media to deliver cardiac care. During the conference in the next two days, the team of doctors from the US will be assessing a full spectrum of abnormalities seen in rheumatic valvular heart disease using state-of-the-art technology developed by them. The hospital aims to screen totally 300 patients in three days (August 11-13). The physicians were also trained in using Vscan, a handy device developed by GE Healthcare that helps in diagnosing rheumatic heart disease. 'With the click of a button' has been the catchphrase of technical innovations for sometime now. The new breed of innovators may begin luring you with the phrase, 'with just a movement of your hand'. With gesture control becoming a buzzword, a team of innovators from the city has come up with a band that can be worn on one's wrists to control all electronic devices it is connected to. The company called Tecxtra Technologies was formed when five engineering students from Gondia, one each from Punjab University and Nagpur University met at Asia'a largest robotic competition held in 2009 in Pune. Having common goals like spreading technical knowledge kids and developing indigenous market of high-end technological products, they soon started working out of Nagpur. The latest of these products is "Techno-band", a wristband that can help control electronic gadgets with hand gestures. The device also has some additional features like location tracking, health monitoring and other related applications. "The band basically helps you control the entire electronic system of your home or office, even from a distance of up to 50 metre. Utilizing the GSM system, Wi-Fi, Bluetooth or other wireless technologies, its range can be increased as per requirement. This way it can be used for remote access to all the connected electronic devices. It can help a user to communicate, transfer, share, all the needed information by using simple motion gesture," said Shyam Prasad, one of the seven directors of the company. The band can be used to monitor and control all the compatible electronics devices like computers, mobiles, cameras among others, he added. Another director Ranjit Kumar said from simple things like making it change presentation slides, switching on or off devices like television to controlling electronic locking system on the main door of a house the device is capable of saving the life of a user. Elaborating the last point, Dilip Pardhi of the group explained, "The heartbeat, blood pressure and other vital signs can be monitored by the band. And, in case, something goes wrong, a message regarding this, along with the person's location, would be sent to the family doctor and a relative." He said that the band can even be controlled through an Android App on a smartphone.
"The scope of these inventions extends all the way to provide safety mechanism to women, girls and children in real time by making use of location system using GPRS to the paired devices. Several other uses can be added on to the device as per the requirement or request of the user," said partner Neeraj Yadav. Formation to create three-dimensional shapes. Researchers at Harvard University have successfully built a huge horde of tiny bots that use infrared lights and vibration motors to swarm together like insects. Nicknamed Kilobots, these small machines measure about an inch (2.5 centimeters) across (about the same as a quarter), and stand just three-quarters of an inch (2 cm) off the ground (about as high as a penny standing on its edge). But despite their dimensions, the Kilobots can accomplish big things. Researchers program each robot with advanced algorithms that enable it to move around on its own while simultaneously communicating with the other bots around it. The Kilobot. The robots are simple, free of many of the sensors and state-of-the-art hardware typically found on other bots. While that means that the Kilobots are somewhat limited in function — they have trouble moving in a straight line, for example — it also means that each bot is inexpensive and easy to build, said Michael Rubenstein, a postdoctoral fellow at Harvard University and lead researcher for the Kilobot project.
In the past, building a giant robot swarm was out of the question for researchers because of the time and money it would cost to develop each bot, Rubenstein said. But at $14 apiece, the Kilobots are a bargain. And each bot takes only 5 minutes to assemble, according to the researchers. At present, the Kilobots are just working together to form 3D shapes — the letter "K" being their favorite. They can also transform themselves into common tools, such as wrenches and keys. "In the future, we'd like [the Kilobots] to do something functional, but for now, they're just a research platform," Rubenstein told Live Science. But Rubenstein and his colleagues have big plans for their tiny robots. One day, they'd like the bots to function as "programmable matter," which is based on a concept similar to 3D printing — only without the printer, he said.  In 3D printing, you tell a printer what shape you want it to make, and it produces it in plastic filament, Rubenstein said. But when you're dealing with programmable robots, the robots behave like the filament. In other words, you tell a swarm of bots what shape you want them to take, and they come together to form that 3D object, he said. And programmable robots have potential uses that exceed the capabilities of the average 3D printer, Rubenstein noted. For example, he said Kilobots could be used in space one day. "If an astronaut goes to Mars, they're going to bring all their tools with them," Rubenstein said. "Traditionally, they'd bring a huge toolbox with all the tools they need. But imagine if instead they can just bring a little box of robots and make any tool that they want out of these robots." [Infographic: Steps on the Road to Programmable Matter]. Of course, this out-of-this-world idea is just that — an idea. The researchers are still developing the algorithms that could make a Kilobot space journey possible.The power of the swarm. But even with the algorithms that have already been developed, Rubenstein and his colleagues have made a considerable contribution to the field of robotics, said James McLurkin, a professor of computer science at Rice University in Texas. McLurkin wasn't involved in the Kilobot project, but he has observed the bots in action."Large groups of robots working together can solve problems in fundamentally different ways," McLurkin told Live Science. "The goal that we're trying to move forward is understanding the relationship between simple, local interactions and complex group behaviors." Robots that can function as both individuals and team players are far more versatile than robots that only behave independently, McLurkin said. Swarming robots can be used in variety of ways, he said, from mapping underwater or extraterrestrial terrain to searching for victims of earthquakes or other disasters."There are a few classes of tasks that are ideal for a large number of robots: tasks where you need to spread robots over a large geographic area, such as searching and exploration, and tasks where you need to do many operations at the same time, like in construction," McLurkin said.McLurkin, who is also the roboticist in residence at Manhattan's Museum of Mathematics (MoMath), is currently developing an interactive exhibit for kids that aims to demonstrate just how mighty (and useful) tiny robots can be when they take a page from swarming bugs.  A futuristic-looking robot suit that has its own mechanical "limbs" may help paralyzed patients walk again. The robotic exosuit, called HAL for “Hybrid Assistive Limb," was originally developed in Japan, but has now been tested on paralyzed patients at the Center for Neurorobotic Movement Training in Bochum, Germany. When a person wants to move, the brain sends a signal through the spinal cord and the nerves that surround it, to a muscle in a body part such as a leg or an arm. However, in paralyzed patients, these spinal nerve structures are damaged, and the muscles operate with signals that are too weak to reach the leg or the arm. This is why these patients cannot walk or move certain body parts.  But the HAL robot suit can actually pick up these weakened signals through sensors that are attached to a patient's skin, and can set its motors, located in the pelvic and knee-joint regions, in motion. This effectively connects the robotic suit to the patient's nervous system and helps the individual to regain some mobility. "This exoskeleton robot suit differs from other exoskeleton systems in its unique and successful neuronal control directly by the patient," said Dr. Thomas Schildhauer, medical director at the Bergmannsheil, a university hospital in Bochum. "[T]he patient is in 'direct control' of the robot and is not passively moved by the robotic exoskeletal system." The robot suit is already being used at the Bergmannsheil by paraplegic, quadriplegic, stroke and muscular dystrophy patients in controlled trials, he said. (Paraplegic patients can move their arms, hands and fingers, whereas quadriplegic patients have limited movement or no movement from the neck down.)
"In some subgroups of spinal cord injury patients the application is already very successful, whereas the other patient groups are still under formal investigation," Schildhauer told Live Science. Schildhauer was surprised by "the amount of functional gain in some subgroups of chronic spinal cord injury patients," he said. For instance, a wheelchair-bound patient may be able to walk short distances with the support of a walking frame after training with the exosuit five times a week for three months, the researchers said. The Bergmannsheil is the only hospital in Germany where the robot suit has been used. Similar suits have been tested in about 200 geriatric rehab centers in Japan. The HAL was developed by Dr. Yoshiyuki Sankai, a professor in the Graduate School of Systems and Information Engineering at the University of Tsukuba in Japan. Sankai also founded Cyberdyne Inc. (from "cybernics" — an interdisciplinary research field that incorporates cybernetics and other disciplines), a company that produces the HAL. Sankai designed the robot "for the benefits of humankind in the field of medicine, caregiving, welfare, labor, heavy works, entertainment and so on," according to Cyberdyne's website. Sankai has said in the past that he aims to create new technologies that will benefit people and not serve to destroy them. For instance, he reportedly rejected offers from the U.S. Department of Defense and the government of South Korea to develop robots for military use. Research describing the application of the robot suit in paralyzed patients was published in April in the Spine Journal. Robo Brain -- a large-scale computational system that learns from publicly available Internet resources -- is currently downloading and processing about 1 billion images, 120,000 YouTube videos, and 100 million how-to documents and appliance manuals. The information is being translated and stored in a robot-friendly format that robots will be able to draw on when they need it. To serve as helpers in our homes, offices and factories, robots will need to understand how the world works and how the humans around them behave. Robotics researchers have been teaching them these things one at a time: How to find your keys, pour a drink, put away dishes, and when not to interrupt two people having a conversation. This will all come in one package with Robo Brain. "Our laptops and cell phones have access to all the information we want. If a robot encounters a situation it hasn't seen before it can query Robo Brain in the cloud," said Ashutosh Saxena, assistant professor of computer science at Cornell University. Saxena and colleagues at Cornell, Stanford and Brown universities and the University of California, Berkeley, say Robo Brain will process images to pick out the objects in them, and by connecting images and video with text, it will learn to recognize objects and how they are used, along with human language and behavior. If a robot sees a coffee mug, it can learn from Robo Brain not only that it's a coffee mug, but also that liquids can be poured into or out of it, that it can be grasped by the handle, and that it must be carried upright when it is full, as opposed to when it is being carried from the dishwasher to the cupboard. Saxena described the project at the 2014 Robotics: Science and Systems Conference, July 12-16 in Berkeley. The system employs what computer scientists call "structured deep learning," where information is stored in many levels of abstraction. An easy chair is a member of the class of chairs, and going up another level, chairs are furniture. Robo Brain knows that chairs are something you can sit on, but that a human can also sit on a stool, a bench or the lawn. A robot's computer brain stores what it has learned in a form mathematicians call a Markov model, which can be represented graphically as a set of points connected by lines (formally called nodes and edges). The nodes could represent objects, actions or parts of an image, and each one is assigned a probability -- how much you can vary it and still be correct. In searching for knowledge, a robot's brain makes its own chain and looks for one in the knowledge base that matches within those limits. "The Robo Brain will look like a gigantic, branching graph with abilities for multi-dimensional queries," said Aditya Jami, a visiting researcher art Cornell, who designed the large-scale database for the brain. Perhaps something that looks like a chart of relationships between Facebook friends, but more on the scale of the Milky Way Galaxy. Like a human learner, Robo Brain will have teachers, thanks to crowdsourcing. The Robo Brain website will display things the brain has learned, and visitors will be able to make additions and corrections.  Improving the design of the world's tiniest drones could start with taking a closer look at one of nature's smallest and most efficient flyers, the hummingbird. In a new study, researchers at Stanford University's Bio-Inspired Research and Design (BIRD) lab compared the flight of 12 different species of hummingbird with that of a Black Hornet Nano, one of the world's smallest drones. Despite the sparrow-sized surveillance drone's extremely efficient flying, it still can't match the best of the hummingbird species tested, the Calypte anna, the findings suggest."The hummingbird is such an exceptional bird. It's the only bird that hovers. It's got so much power," said lead study researcher David Lentink, an assistant professor of mechanical engineering at Stanford.  To find out just how powerful a hummingbird really is, Lentink and his team used a standard mechanical approach. This method measures the different mechanisms determining the flight efficiencies of winged animals. Known as a quasi-steady model, the tool consists of two small motors that spin dried hummingbird wings on a hollow axle.  If you're driving along Canada's highways, you may encounter a hitchhiking robot with his right thumb out, Wellington boots on and a charming personality — not to mention the ability to tweet, Instagram and update his Facebook profile. Meet hitchBOT.
The little robot, the brainchild of Canadian researchers, is making his way across Canada, relying on strangers to give him a ride. The journey is part of an experiment in both the culture of human kindness and the current state of artificial intelligence technology. The collaborative art and robotics project aims to explore two diverse issues: the growing utility for social or companion robots, and a growing aversion toward risk and adventure (in Canada at least), said David Harris Smith, a professor of communications at McMaster University in Hamilton, Ontario, and one of hitchBOT's founding fathers.  "Though statistically safer, Canadians have become much more cautious and worried, evident in the shrinking amount of unsupervised playtime they allow their children," Smith told Live Science in an email. "HitchBOT combines these issues by sending a robot to explore cultural life in Canada, taking the someway risky, but adventurous, approach of hitchhiking across the country." HitchBOT left Nova Scotia, on the East Coast of Canada, on July 27, and the researchers hope he'll reach British Columbia, on the West Coast, by the end of the summer. Some of the robot's features include one hitchhiking arm that can raise itself, the ability to converse with traveling companions and comfy rain boots for those long hours camped on the side of the road. The bot also boasts an LED screen for a face, a 3G and Wi-Fi network, an audio and visual camera, speech-recognition processing and the ability to share bits of trivia from a Wikipedia database. HitchBOT has been described as "hardware-store chic," with pool noodle arms, a garbage can hat and a torso made out of a beer pail. HitchBOT also has a "personality" designed by Smith and Frauke Zeller, a professor at Ryerson University in Toronto. The robot's persona includes a "family" of researchers and artists, hobbies (horseback riding, trivia and baking) and a favorite movie, "Wall-E."HitchBOT has limited speech abilities, but can engage in small talk, apologize that he might be kind of weird and even tell you if he doesn't understand something you said. "HitchBOT chatters, asking about your favorite music, or what book you are reading — these are strategies to get people talking, so hitchBOT can listen for some key words that it might recognize," Smith said. As he travels, hitchBOT asks drivers to charge his battery in the cigarette lighters of their cars, and takes 60-second images or videos that are then uploaded to his social media accounts. This way, the robot's "family" in Toronto can track what he encounters on his cross-country journey. "From a cultural and sociological perspective, hitchBOT might be used to fathom differences in cultural attitudes toward social robotics," Smith said. "We have seen some of this in the social media feeds, which [have] an international audience of over 20,000 followers."But the project is doing more than simply documenting the kindness of Canadians. Smith said the researchers are also aiming to design low-cost robots that are intuitive in their interactions with humans, have flexible power-supply options, can be integrated with social media and are able to geo-locate, among other features. HitchBOT may also be useful in calibrating the number of wireless services it encounters over the course of the cross-country expedition, since the robot posts its GPS location every 15 minutes. ROBOTS came into the world as a literary device whereby the writers and film-makers of the early 20th century could explore their hopes and fears about technology, as the era of the automobile, telephone and aeroplane picked up its reckless jazz-age speed. From Fritz Lang’s “Metropolis” and Isaac Asimov’s “I, Robot” to “WALL-E” and the “Terminator” films, and in countless iterations in between, they have succeeded admirably in their task. Since moving from the page and screen to real life, robots have been a mild disappointment. They do some things that humans cannot do themselves, like exploring Mars, and a host of things people do not much want to do, like dealing with unexploded bombs or vacuuming floors (there are around 10m robot vacuum cleaners wandering the carpets of the world). And they are very useful in bits of manufacturing. But reliable robots—especially ones required to work beyond the safety cages of a factory floor—have proved hard to make, and robots are still pretty stupid. So although they fascinate people, they have not yet made much of a mark on the world.So far, the response to hitchBOT has been positive, the researchers said. "Although we planned to create a robot that would be 'approachable' and appealing, we have been deeply impressed by how hitchBOT has been embraced by people of all ages," Smith said. "HitchBOT is receiving artworks and affectionate notes from children." So, what's next for hitchBOT? The robo traveler is currently outside of Toronto, according to a map on the project's website. Likely, hitchBOT is standing on the side of a road, arm out, awaiting his next ride. ROBOTS came into the world as a literary device whereby the writers and film-makers of the early 20th century could explore their hopes and fears about technology, as the era of the automobile, telephone and aeroplane picked up its reckless jazz-age speed. From Fritz Lang’s “Metropolis” and Isaac Asimov’s “I, Robot” to “WALL-E” and the “Terminator” films, and in countless iterations in between, they have succeeded admirably in their task. Since moving from the page and screen to real life, robots have been a mild disappointment. They do some things that humans cannot do themselves, like exploring Mars, and a host of things people do not much want to do, like dealing with unexploded bombs or vacuuming floors (there are around 10m robot vacuum cleaners wandering the carpets of the world). And they are very useful in bits of manufacturing. But reliable robots—especially ones required to work beyond the safety cages of a factory floor—have proved hard to make, and robots are still pretty stupid. So although they fascinate people, they have not yet made much of a mark on the world. That seems about to change. The exponential growth in the power of silicon chips, digital sensors and high-bandwidth communications improves robots just as it improves all sorts of other products. And, as our special report this week explains, three other factors are at play. One is that robotics R&D is getting easier. New shared standards make good ideas easily portable from one robot platform to another. And accumulated know-how means that building such platforms is getting a lot cheaper. A robot like Rethink Robotics’s Baxter, with two arms and a remarkably easy, intuitive programming interface, would have been barely conceivable ten years ago. Now you can buy one for $25,000. C3 IPO. A second factor is investment. The biggest robot news of 2013 was that Google bought eight promising robot startups. Rich and well led (by Andy Rubin, who masterminded the Android operating system) and with access to world-beating expertise in cloud computing and artificial intelligence, both highly relevant, Google’s robot programme promises the possibility of something spectacular—though no one outside the company knows what that might be. Amazon, too, is betting on robots, both to automate its warehouses and, more speculatively, to make deliveries by drone. In South Korea and elsewhere companies are moving robot technology to new areas of manufacturing, and eyeing services. Venture capitalists see a much better chance of a profitable exit from a robotics startup than they used to. The third factor is imagination. In the past few years, clever companies have seen ways to make robots work as grips and gaffers on film sets (“Gravity” could not have been shot without robots moving the cameras and lights) and panel installers at solar-power plants. More people will grasp how a robotic attribute such as high precision or fast reactions or independent locomotion can be integrated into a profitable business; eventually some of them will build mass markets. Aerial robots—drones—may be in the vanguard here. They will let farmers tend their crops in new ways, give citizens, journalists and broadcasters new perspectives on events big and small (see article), monitor traffic and fires, look for infrastructure in need of repair and much more besides. As consumers and citizens, people will benefit greatly from the rise of the robots. Whether they will as workers is less clear, for the robots’ growing competence may make some human labour redundant. Aetheon’s Tugs, for instance, which take hospital trolleys where they are needed, are ready to take over much of the work that porters do today. Kiva’s warehouse robots make it possible for Amazon to send out more parcels with fewer workers. Driverless cars could displace the millions of people employed behind the wheel today. Just as employment in agriculture, which used to provide almost all the jobs in the pre-modern era, now accounts for only 2% of rich-world employment so jobs in today’s manufacturing and services industries may be forced to retreat before the march of the robots. Whether humanity will find new ways of using its labour, or the future will be given over to forced leisure, is a matter of much worried debate among economists. Either way, robots will probably get the credit or blame. Invisible and otherwise. Robotic prowess will to some extent be taken for granted. It will be in the nature of cars to drive themselves, of floors to be clean and of supplies to move around hospitals and offices; the robotic underpinning of such things will be invisible. But robots will not just animate the inanimate environment. They will inhabit it alongside their masters, fulfilling all sorts of needs. Some, like Baxter, will help make and move things, some will provide care, some just comfort or companionship. A Japanese robot resembling a baby seal, which responds amiably to stroking and can distinguish voices, seems to help elderly patients with dementia.The more visible robots are, the better they can help humanity discuss questions like those first posed in fiction. Is it necessary that wars always be fought by people who can feel pity and offer clemency, and yet who can also be cruel beyond all tactical requirements? (Already America is arguing about whether drone pilots deserve medals—see article.) Does it matter if the last kindnesses a person feels are from a machine? What dignifies human endeavour if the labour of most, or all, humans becomes surplus to requirements? People, companies and governments find it hard to discuss the ultimate goals of technological change in the abstract. The great insight of Asimov et al was that it is easier to ask such questions when the technology is personified: when you can look it in the face. Like spacefarers gazing back at the home planet, robots can serve not just as workers and partners, but as purveyors of new perspectives—not least when the people looking at them see the robots looking back, as they one day will, with something approaching understanding. SUVs, ovens, surgical devices and hard drives … What do they have in common? Assembly robots are building them right now. From the largest vehicles to the smallest electronics, robotic assembly is increasingly being used to manufacture the world’s goods. The latest statistics released by the Robotic Industries Association (RIA) show record-breaking sales for the first half of 2014. Assembly orders were up 96 percent in the second quarter from the same period last year. Reshoring and next-shoring efforts are bringing manufacturing home. Rising global wages and an aging workforce are driving automation worldwide. Meanwhile, miniaturization is demanding superhuman precision. A new breed of compact, highly dexterous robots with advanced sensors are answering the call. “The only way to make assembly processes cost-effective for North American manufacturing is with robotics and automation,” says Chris Blanchette. “Using advanced technologies like vision, whether it’s 2D, 2-1/2D or 3D, to locate parts and guide robots and then force sensing to determine how parts go together are making robots more desirable, increasing their productivity, and making them more cost-effective and easier to deploy and support,” says Tim DeRosett, RIA Board Member and Director of Strategic Initiatives at Yaskawa Motoman in Miamisburg, Ohio. Combined, these trends are transforming our assembly lines and clean rooms. Assembling More in Less Space. Robotic assembly that unites multiple operations and multiuse tooling is shrinking the footprint of the work cell, the assembly line, and manufacturing as a whole. “In our powertrain assembly area, where we’re putting together large complex systems, the end-of-arm tooling is mind-bogglingly complicated,” says Nicholas Hunt, Manager of Technology and Support at ABB Robotics in Auburn Hills, Michigan. “It’s not unusual for the end-of-arm tooling to cost more than the robot. There is a lot of sensing and control capability in the end effector itself. We’re doing so much with those tools, because we can. And all in one cell.” “When the part ships out of that cell, it’s assembled and it’s checked,” adds Hunt. “So we’re shrinking the footprint of manufacturing all over the world.” “One of the other trends is using robot control technology to control more than robots,” says Hunt. “One example is what we do with body-in-white tooling with our GateFramer, or these flexible systems where we control and manage the transfer of body sides through the assembly line. You can introduce up to five new body styles in midstream, for a total of six car models, one right after the other.” In this video courtesy of ABB we ride the assembly line for a fish-eye view of how robot control technology is being used to handle multiple car models.Market Expansion. New applications in mature and emerging industries, plus growing interest in automation among manufacturers further down the supply chain, are expanding the market for robotic assembly. Small parts assembly, particularly in the 3C (computer, communication and consumer electronics) industry, continues to be a major area of growth for automated assembly. “Another area that typically fits with small parts assembly and handling is medical device, which is a relatively small market compared to automotive but has potential for strong growth,” says DeRosett. Probably one of the most interesting developments is the adoption and consideration of robotics for contract manufacturing services. The robot OEMs are paying close attention to this growing trend. “Primarily, the markets that we’re tracking right now for contract manufacturing are computer microelectronics and consumer electronics,” says FANUC’s Blanchette. “But a lot of those companies do business in aerospace, medical device and other industries.” He says that new industries are evaluating contract manufacturing, so automation and robotics could potentially play an important role in their adoption. Miniaturization and Precision. While contract manufacturing and the 3C industry are expected to grow, consumer electronics are getting smaller and the assembly tolerances tighter. Robotics becomes critical to the production process when the product cannot be made to satisfactory precision, consistency, or cost without flexible automation. “In our world, in electronics manufacturing, there are robotic solutions and flexible automation that’s capable of getting to the level of precision that we can now take advantage of the technology,” says Rush LaSelle, Director of Automation at Jabil Circuit Inc. “Labor rates in China and other parts of the world are rising, and so everybody is looking for a lower cost alternative.” Headquartered in St. Petersburg, Florida, Jabil is a contract manufacturer for some of the world’s most recognized brands. “If you’re buying a smartphone or other consumer electronic device,” says LaSelle, “likely some part of the product is being manufactured by Jabil.” He notes miniaturization, greater precision, and short product life cycles as some of the key drivers sparking the demand for automated electronics assembly. “People can’t do the really fine tasks that are called upon for assembling next-generation mobile devices or wearables,” says LaSelle. “The product life cycles aren’t long enough to support customized machine tools.” This video courtesy of Jabil discusses the trend toward miniaturization and precision assembly.
Jabil also serves the automotive, healthcare, printing and energy generation industries. They have identified aerospace and wearable technology as emerging industries for contract manufacturing. LaSelle credits this market expansion to lower-cost sensor technology. See, Feel and Adapt. Whether it’s machine vision for part identification, tracking, inspection and robot guidance, or force sensing for mating delicate, complex assemblies, sensor-based technologies are getting better and cheaper. It’s a happy balance for robotic assembly. “FANUC has really focused on intelligent-based features for the robot,” says Blanchette. “Some of those features are low-cost integrated vision, including 2D bar code reading so you can track your product. Then we have guidance, so you can locate product and parts in the assembly process. You can actually do the inspection while you’re doing the assembly with a low-cost vision system.” The robots in the previous FANUC video are equipped with a patented 3-axis wrist. “It’s a unique enabler,” says Blanchette. “Having the 3-axis wrist on a delta-style robot makes it an assembly robot. Otherwise it’s just a picking/packing robot.” “All of FANUC delta-style robots have a 3-axis option,” he says. “It’s a really cool solution because it allows for very small part manipulation in all 6 degrees of freedom.” Equipped with force sensing, assembly robots also gain the sense of touch in all 6 degrees of freedom. “Force feedback is just one more sensor set that will allow you to get better precision in a cost-effective manner. It better replicates how people assemble devices, because we still design components with human assembly in mind,” says Jabil’s LaSelle. “As more of these sensory packages come online, become more cost-effective and are more easily deployed, so too will be the growth of deployment into the electronics manufacturing arena.”Force Sensing and Control. The force sensor measures the forces and torque applied at the end-of-arm tool and provides feedback to the robot control system. The robot can then adjust its movements based on that feedback. This is especially useful when assembling complex mechanisms such as meshing gears. “The technology is not new,” says DeRosett. “It’s been used for the last 25 years in robotics. What is changing is how well integrated the force sensor is with the robot controls. Products like ABB Integrated Force Control make the technology easier to use. “The concepts can be complicated, so we give you these program tools that make it simple,” says Hunt. “We reduced the size and the amount of hardware, and the complexity of the package. We integrated more into the robot system.” “With force control, we can use it in a broader way than we have in the past,” explains Hunt. “Normally, we use it in a straight force torque sensing configuration, where we’re trying to assemble a torque converter and we’re feeling the interference of the gears that we’re trying to mesh together. But we can also use the force torque sensor as a strain gauge. We’re assembling the valve stem in the engine along with the spring and we’re checking for the absence of an impulse. So we’re not using force control directly to apply a consistent force. We’re using it as a poka-yoke. That’s another example of getting the most out of the end-of-arm tool.” “Parts used to have to go to another cell for quality assurance checks,” says Hunt. “With advances in control technology, processing power and sensor technology, we now have the ability to do assembly and inspection simultaneously.” In this video a FANUC M-1iA delta-style robot assembles gears of different sizes using a six degree-of-freedom remote force sensor attached to the assembly instead of the robot. About 1:10 into the demonstration, you can see a close up of the phase matching process. Gear Meshing. In the previous example, the force sensor is attached to the assembly. Often the force sensor is located between the end-of-arm mounting flange and the tool itself, as is the case with this automotive clutch assembly. This video courtesy of Kawasaki Robotics (USA) Inc. demonstrates the process. “The six degree-of-freedom force sensor measures forces in x, y and z, as well as the torque forces about the three axes,” says Samir Patel, Director of Product and Advanced Engineering at Kawasaki in Wixom, Michigan. “The measurement is fed back to the robot controller for hybrid force-compliance control of the end-of-arm tool.” “One of the other key features is called groping,” explains Patel. “The robot moves the end-of-arm tool with the part close to where it needs to be assembled, and then it transitions into this groping feature where the robot makes a pattern around the part where it’s supposed to be assembled, moving slowly and progressively trying to make contact. As it is making contact, the robot is sensing the forces, motion and direction, and accordingly driving the parts together.” Patel says the ‘groping’ process involves both preprogramming and adaptive robotics. The groping feature offers different motion patterns, like a rectangular or flower pattern. The robot programmer selects one of these patterns as he is writing the routine. Based on this preprogramming, the robot receives force feedback and makes the decision whether to keep progressively moving down or sideways to complete the assembly. This Kawasaki clutch assembly was a special project for Ford Motor Co. The robotics solution helped alleviate ergonomic issues associated with assembling these heavy parts. Bearing Assembly. In another complex assembly application, robots help assemble gear boxes for Kawasaki motorcycles. With a wide product range, flexibility is important. “What we did is build a flexible cell that has fixtures for different gear boxes,” says Patel. “The robot carries the crankcase, moves it over the fixture where the bearings are mounted, and then just holds it there while the press comes down and drives the case into the bearing.” “Now, robots are designed to move parts from one place to another,” he continues, “or in other words, the principle on which they operate is position control. If the robot is holding a part in one place and you exert an abnormally high external force, it could damage the robot or it could push the part off the tooling. So we have this soft absorber mode, or software compliance feature. As the robot is holding the crankcase over the bearing, the robot transitions to soft absorber mode so that it is compliant, and then the bearing is pressed into the casing. Then the robot goes back to position control mode and carries on with the job.” This video shows the motorcycle crankcase bearing assembly, which is an ongoing operation at Kawasaki’s manufacturing facility in Japan. Recommended for assembly applications, the Kawasaki R-Series robots (pictured) have a smaller footprint, internal cabling, and a tight radius that make them ideal for high-density applications with two or more robots in a small space. Compact, Highly Dexterous Robots. Compact, highly dexterous robots are playing an ever-increasing role in the assembly space. Nimble 6-axis robots engage assemblies from all angles, while dual-arm humanoids transform manual operations. “Force sensing certainly helps,” he continues. “But what we’ve seen with the dual-arm robot is handling flexible parts or components, such as tubing and hoses, which when used with the traditional single-arm robots require the end-of-arm tooling to be more complex. Whereas if you have two arms working together, then that can simplify the tooling and enable the application to be done in a better way.” Yaskawa is on the fifth generation of its dual-arm robots. The SDA-series have two 7-axis, kinematically redundant arms mounted to a common torso. This video shows the dual-arm assembling an office chair.
“Being able to get into small, tight spaces was one of the things that led us to create the 7-axis arm,” says DeRosett. Check out this video to see a single 7-axis arm insert flexible tubing in an assembly demo.  Learn more about kinematic redundancy in this primer on collaborative robotics. There’s another kinematically redundant dual-arm on the horizon. The ABB Dual-Arm Concept Robot (DACR) is expected to hit the market by mid-2015. “The launching of the DACR will definitely be a boon to our 3C offering,” says Hunt. “For basic assembly of relays, contactors, and anything that is not precision assembly. There’s a lot of stuff out there.” As Hunt notes, the DACR is not their entry point to the 3C market. This video courtesy of ABB demonstrates various assembly applications. “The IRB 120 was the major jumping-off point for ABB into the 3C area,” he says. “It’s a tabletop and our smallest 6-axis robot.” First debuted at Automatica 2014 (watch the video), its big brother is the six-axis IRB 1200. It will be officially released at the International Manufacturing Technology Show this fall. ABB’s newest robot is even more agile and dexterous to address the growing demand for miniaturization. “There’s a lot of product miniaturization going on,” say FANUC’s Blanchette. “They need high-precision, high-speed articulated mechanisms to be able to put those assemblies together. With our new LR Mate 200iD series (pictured), we managed to create a robot that is articulated in 6 axes of freedom and has the majority of characteristics of a SCARA, but is 20 percent faster than the SCARAs in its class. We now have a price that is very competitive with a SCARA, as well.” “With visual servoing, we’ve been able to achieve precision assembly of better than 10 microns,” adds Blanchette. “Usually you only think of that kind of precision with machine tools or custom Cartesian-based robot systems. But we’re able to do that with a small LR Mate robot.” Also entering the assembly space are collaborative robots, a rapidly emerging area of industrial robotics. Here, specially designed robots such as ABB’s DACR and Universal Robots’ UR can operate alongside human coworkers on an assembly line without the need for safety guarding. In other cases, conventional robots are equipped with safety mechanisms to allow them to work in close proximity to humans, such as in progressive assembly applications. This article surveys the current state of collaborative robotics. Safe Collaborative Assembly. Traditional robot manufacturers offer software-based safety features that are integrated into the robot controller, and when coupled with safety sensors, allow conventional robots to safely collaborate with humans. Each manufacturer has its own flavor. ABB calls it SafeMove, Yaskawa has its Functional Safety Unit (FSU), then there’s KUKA Safe Robot, and FANUC Dual-Check Safety (DCS). Each software solution helps reduce the manufacturing footprint. “Floor space is everything,” says ABB’s Hunt. “We pack robots denser and denser. The only way to do that with large robots and when people are around is with a product like SafeMove. There’s really no reason to have hard fencing for robots with the kind of technology we have available today. SafeMove is an enabler for human-robot collaborative assembly.” Redeployable Modularity. Modularity is another trend that is helping shrink the manufacturing footprint. Drop them in and modular cells provide a turnkey solution for robotic assembly, especially where product life spans are fleeting. Modular assembly cells are also attractive to contract manufacturers looking for redeployability. “One of the things that contract manufacturers are looking for is reusability of the automation equipment,” says FANUC’s Blanchette. “The product life cycles are very short. So they need to invest in equipment that is reusable, that they can turnaround and deploy in a matter of a month or so. The kind of solutions that they are looking for are modular-based solutions that can be fit into any portion of a line and then adapted and adjusted to fit any type of process, or a multitude of processes.” “Jabil is actively investing in the redeployability of automation platforms,” says LaSelle. “What it’s speaking to is this desire to get technology into production faster. This isn’t novel, but it’s come to a fever pitch right now. Especially in electronics, where we really only have maybe 24 months and it’s a reverse ramp, so you produce your maximum production right before Christmas and then it starts to decay over the next year or two. So people are trying to find ways to get standardized solutions.” “It needs to be 70-80% redeployable, so all of the equipment that goes in there has to be reusable in the next solution or contract,” says Blanchette. “Robots are traditionally flexible, so a robot is a great tool to add to that modular solution because it can be reprogrammed and has the ability to adapt to different part geometries.” Whether we’re building Corvettes or circuit boards, we’re asking more from our automation than ever before. As costs continue to align with demand, nimble robots bolstered by integrated sensors are assembling a promising future. Stay tuned. The origins of origami trace back to the sixth century, when intricately folded paper was first used for ceremonial occasions in Japan. Now, over 1,500 years later, engineers are revisiting the ancient art form with a modern twist: A team of researchers from Harvard and MIT have just unveiled a paper robot that can autonomously fold to transform itself into a functioning machine and walk away within 4 minutes. And if you think the cost of a transformer is out of your budget, think again. The self-folding robot cost just $100 to build. Researchers envision a day when people can order a design for a robot, get it shipped, watch it fold and then enjoy while it entertains your cat or sweeps your porch. “This approach to fabricating robots and tooling would democratize access to robots,” says study co-author Daniela Rus of MIT. Robotic Origami. You may remember Shrinky-Dinks, the popular children’s toys that shrink when you put them in the oven. Well, researchers said those toys served as the inspiration for the researchers’ self-folding robot. Shrinky-Dinks are a type of material called a memory polymer, which changes shape when it’s heated above a certain temperature. Researchers coated a thick paper substrate with the memory polymer. They then cut a two-dimensional pattern that included all the robot’s hinges. Each hinge housed embedded circuits that warmed to 212 degrees Fahrenheit, providing the heat necessary to alter the polymer’s shape and cause a fold. Researchers manipulated the fold angles by varying the gap between the paper and the hinge — the greater the gap, the greater the fold. Then, motors, a power source and other electrical components were added to the flat paper frame, and the robot was programmed to run a five-step pattern of sequential folds to build its structure. When researchers connected the batteries, the program automatically ran, and all they needed to do was watch. They published their findings Thursday in the journal Science. Researchers say self-assembling robots offer several advantages over traditional “nuts and bolts” bots. For one, they’re inexpensive; the team built their prototype with readily available materials and technology. Theoretically, all a future robot manufacturer would need is these materials, a computer-generated design and a programmed chip to build custom robots that perform myriad functions. Currently, entire assembly line processes would need to be reconfigured at a high cost to build a new robot on the fly. “Pushing a button would generate the design files, send them to the printers for fabrication, and also produce a programming environment for using the (robot),” Rus says. “This approach could extend to everyday life.” Additionally, going from flat to functional paves the way for robots that are easily transportable and can get into tight places. Researchers envision satellites that self-assemble in space, or search-and-rescue robots that can be deployed in dangerous environments. Several mechanisms could initiate the folding sequence, such as environmental factors or a wireless remote. And if this technology goes mainstream, it’ll only be a matter of time before robot manufacturers are swamped with orders for a self-folding Optimus Prime. Collaborative Robots, or robots that work side by side with humans without being caged in, are one of the newest trends in the robotics industry. The RIA International Collaborative Robots Workshop will examine all of the key issues in a fast-paced one day event. You’ll learn about the technology, the applications, capabilities and limitations, safety issues, key research efforts, and get a better understanding of where this trend is taking the industry and how quickly! This one day event will include speakers from leading industrial robot companies, a panel of robot users as well as a networking reception featuring exhibits that demonstrate collaborative robot technology. Robots are deployed in an ever expanding number of industries and applications. Accidents can occur especially during non-routine operating conditions, such as programming, testing, setup, adjustment and maintenance. Help ensure your personnel are protected by attending the 26th National Robot Safety Conference the world’s leading event for industrial robot safety and compliance. You’ll be given in-depth coverage of the latest ANSI/RIA R15.06 -2012 Robot Safety Standard, which is harmonized with the ISO 10218 standards. All conference registrants will also receive a copy of the latest Robot Safety Standard. Additionally, this three-day conference will cover all aspects of industrial robotics and process automation. Topics include proper functional safety circuit design, risk assessments and a workshop covering practical robot safety requirements for integrators and robot users. RIA offers a certification program for robot integrators. The program provides robot integrators with a way to benchmark against industry best practices and allows robot users to develop a baseline for evaluating robot integrators. If you’re an integrator looking to become certified, conference attendance helps meet established criteria for attaining RIA Certified Robot Integrator status. For many people it is a machine that imitates a human—like the androids in Star Wars, Terminator and Star Trek: The Next Generation. However much these robots capture our imagination, such robots still only inhabit Science Fiction. People still haven't been able to give a robot enough 'common sense' to reliably interact with a dynamic world. However, Rodney Brooks and his team at MIT Artificial Intelligence Lab are working on creating such humanoid robots. The type of robots that you will encounter most frequently are robots that do work that is too dangerous, boring, onerous, or just plain nasty. Most of the robots in the world are of this type. They can be found in auto, medical, manufacturing and space industries. In fact, there are over a million of these type of robots working for us today. Some robots like the Mars Rover Sojourner and the upcoming Mars Exploration Rover, or the underwater robot Caribou help us learn about places that are too dangerous for us to go. While other types of robots are just plain fun for kids of all ages. Popular toys such as Teckno, Polly or AIBO ERS-220 seem to hit the store shelves every year around Christmas time. And as much fun as robots are to play with, robots are even much more fun to build. In Being Digital, Nicholas Negroponte tells a wonderful story about an eight year old, pressed during a televised premier of MITMedia Lab's LEGO/Logo work at Hennigan School. A zealous anchor, looking for a cute sound bite, kept asking the child if he was having fun playing with LEGO/Logo. Clearly exasperated, but not wishing to offend, the child first tried to put her off. After her third attempt to get him to talk about fun, the child, sweating under the hot television lights, plaintively looked into the camera and answered, "Yes it is fun, but it's hard fun." As strange as it might seem, there really is no standard definition for a robot. However, there are some essential characteristics that a robot must have and this might help you to decide what is and what is not a robot. It will also help you to decide what features you will need to build into a machine before it can count as a robot. Well it is a system that contains sensors, control systems, manipulators, power supplies and software all working together to perform a task. Designing, building, programming and testing a robots is a combination of physics, mechanical engineering, electrical engineering, structural engineering, mathematics and computing. In some cases biology, medicine, chemistry might also be involved. A study of robotics means that students are actively engaged with all of these disciplines in a deeply problem-posing problem-solving environment. A community wants to construct a robot zoo in which the "animals" move their heads, open their mouths and make appropriate sounds when they sense that someone is coming towards them. Design and build a prototype device which could satisfy this need. A local pet shop wishes to sell a range of devices which automatically feed small cage pets (such as rabbits, gerbils, mice etc.) when their owners are away for the weekend. Design and build a prototype device which could satisfy this need. You need to determine what problem you are trying to solve before you attempt to design and build a robot to solve a problem. Take the time to study a number of different situations and once you have decided what the situation is and you understand exactly what the problem is then write a design brief in a log book (this will be your working document as you work on your robot. This log book can be a paper notebook or an electronic document.) This is a short statement which explains the problem that is to be solved. Having written a brief, you are now ready to gather information which will help you to produce a successful design. First you will need to decide what information you require. This will be different from project to project and will also depend on the amount of information and knowledge you already have. A useful step will be to use the following chart. Ask the five questions, then read the column headed Gathering Information. This will help you plan the type of information you will need to gather. Now it is time to program your robot. This can be achieved in many different ways. Use can achieve rudimentary intelligence in your robot by using only relays, potentiometers, bump switches and some discrete components. You can increase complexity in intelligence in your robot by adding more sensors and continuing in the same vein of using hardwired logic. By introducing a more sophisticated control element, the microprocessor, you introduce a significant new tool in solving the robot control problem. For our robots we used the RCX Brick that was first developed by Fred Martin at MIT as the Programmable Brick. 
As building and programming work progresses, and the design begins to take shape, you will automatically carry out tests on the design. You will also need to complete systems tests at various stages of the construction. If any of the tests show that you have failure in a joint, or that part of your structure is not meeting specifications, then you will have to make modifications in your plan. When building and programming is complete, the entire project must be tested to see if it does the job for which it was designed. An evaluation needs to then be written. This should be a statement outlining the strengths and weaknesses in your design. It should describe where you have succeeded and where you have failed to achieve the aims set out in the specifications. Imagine being present at the birth of a new industry. It is an industry based on groundbreaking new technologies, wherein a handful of well-established corporations sell highly specialized devices for business use and a fast-growing number of start-up companies produce innovative toys, gadgets for hobbyists and other interesting niche products. But it is also a highly fragmented industry with few common standards or platforms. Projects are complex, progress is slow, and practical applications are relatively rare. In fact, for all the excitement and promise, no one can say with any certainty when--or even if--this industry will achieve critical mass. If it does, though, it may well change the world. Of course, the paragraph above could be a description of the computer industry during the mid-1970s, around the time that Paul Allen and I launched Microsoft. Back then, big, expensive mainframe computers ran the back-office operations for major companies, governmental departments and other institutions. Researchers at leading universities and industrial laboratories were creating the basic building blocks that would make the information age possible. Intel had just introduced the 8080 microprocessor, and Atari was selling the popular electronic game Pong. At homegrown computer clubs, enthusiasts struggled to figure out exactly what this new technology was good for. But what I really have in mind is something much more contemporary: the emergence of the robotics industry, which is developing in much the same way that the computer business did 30 years ago. Think of the manufacturing robots currently used on automobile assembly lines as the equivalent of yesterday's mainframes. The industry's niche products include robotic arms that perform surgery, surveillance robots deployed in Iraq and Afghanistan that dispose of roadside bombs, and domestic robots that vacuum the floor. Electronics companies have made robotic toys that can imitate people or dogs or dinosaurs, and hobbyists are anxious to get their hands on the latest version of the Lego robotics system.Meanwhile some of the world's best minds are trying to solve the toughest problems of robotics, such as visual recognition, navigation and machine learning. And they are succeeding. At the 2004 Defense Advanced Research Projects Agency (DARPA) Grand Challenge, a competition to produce the first robotic vehicle capable of navigating autonomously over a rugged 142-mile course through the Mojave Desert, the top competitor managed to travel just 7.4 miles before breaking down. In 2005, though, five vehicles covered the complete distance, and the race's winner did it at an average speed of 19.1 miles an hour. (In another intriguing parallel between the robotics and computer industries, DARPA also funded the work that led to the creation of Arpanet, the precursor to the Internet.) What is more, the challenges facing the robotics industry are similar to those we tackled in computing three decades ago. Robotics companies have no standard operating software that could allow popular application programs to run in a variety of devices. The standardization of robotic processors and other hardware is limited, and very little of the programming code used in one machine can be applied to another. Whenever somebody wants to build a new robot, they usually have to start from square one. Despite these difficulties, when I talk to people involved in robotics--from university researchers to entrepreneurs, hobbyists and high school students--the level of excitement and expectation reminds me so much of that time when Paul Allen and I looked at the convergence of new technologies and dreamed of the day when a computer would be on every desk and in every home. And as I look at the trends that are now starting to converge, I can envision a future in which robotic devices will become a nearly ubiquitous part of our day-to-day lives. I believe that technologies such as distributed computing, voice and visual recognition, and wireless broadband connectivity will open the door to a new generation of autonomous devices that enable computers to perform tasks in the physical world on our behalf. We may be on the verge of a new era, when the PC will get up off the desktop and allow us to see, hear, touch and manipulate objects in places where we are not physically present. A robot that can open doors and find electrical outlets to recharge itself. Computer viruses that no one can stop. Predator drones, which, though still controlled remotely by humans, come close to a machine that can kill autonomously. Impressed and alarmed by advances in artificial intelligence, a group of computer scientists is debating whether there should be limits on research that might lead to loss of human control over computer-based systems that carry a growing share of society’s workload, from waging war to chatting with customers on the phone. Their concern is that further advances could create profound social disruptions and even have dangerous consequences. As examples, the scientists pointed to a number of technologies as diverse as experimental medical systems that interact with patients to simulate empathy, and computer worms and viruses that defy extermination and could thus be said to have reached a “cockroach” stage of machine intelligence. While the computer scientists agreed that we are a long way from Hal, the computer that took over the spaceship in “2001: A Space Odyssey,” they said there was legitimate concern that technological progress would transform the work force by destroying a widening range of jobs, as well as force humans to learn to live with machines that increasingly copy human behaviors. The researchers — leading computer scientists, artificial intelligence researchers and roboticists who met at the Asilomar Conference Grounds on Monterey Bay in California — generally discounted the possibility of highly centralized superintelligences and the idea that intelligence might spring spontaneously from the Internet. But they agreed that robots that can kill autonomously are either already here or will be soon. They focused particular attention on the specter that criminals could exploit artificial intelligence systems as soon as they were developed. What could a criminal do with a speech synthesis system that could masquerade as a human being? What happens if artificial intelligence technology is used to mine personal information from smart phones? The researchers also discussed possible threats to human jobs, like self-driving cars, software-based personal assistants and service robots in the home. Just last month, a service robot developed by Willow Garage in Silicon Valley proved it could navigate the real world. A report from the conference, which took place in private on Feb. 25, is to be issued later this year. Some attendees discussed the meeting for the first time with other scientists this month and in interviews. The conference was organized by the Association for the Advancement of Artificial Intelligence, and in choosing Asilomar for the discussions, the group purposefully evoked a landmark event in the history of science. In 1975, the world’s leading biologists also met at Asilomar to discuss the new ability to reshape life by swapping genetic material among organisms. Concerned about possible biohazards and ethical questions, scientists had halted certain experiments. The conference led to guidelines for recombinant DNA research, enabling experimentation to continue. The meeting on the future of artificial intelligence was organized by Eric Horvitz, a Microsoft researcher who is now president of the association. Dr. Horvitz said he believed computer scientists must respond to the notions of superintelligent machines and artificial intelligence systems run amok. The idea of an “intelligence explosion” in which smart machines would design even more intelligent machines was proposed by the mathematician I. J. Good in 1965. Later, in lectures and science fiction novels, the computer scientist Vernor Vinge popularized the notion of a moment when humans will create smarter-than-human machines, causing such rapid change that the “human era will be ended.” He called this shift the Singularity. This vision, embraced in movies and literature, is seen as plausible and unnerving by some scientists like William Joy, co-founder of Sun Microsystems. Other technologists, notably Raymond Kurzweil, have extolled the coming of ultrasmart machines, saying they will offer huge advances in life extension and wealth creation. “Something new has taken place in the past five to eight years,” Dr. Horvitz said. “Technologists are providing almost religious visions, and their ideas are resonating in some ways with the same idea of the Rapture.” The Kurzweil version of technological utopia has captured imaginations in Silicon Valley. This summer an organization called the Singularity University began offering courses to prepare a “cadre” to shape the advances and help society cope with the ramifications. “My sense was that sooner or later we would have to make some sort of statement or assessment, given the rising voice of the technorati and people very concerned about the rise of intelligent machines,” Dr. Horvitz said. The A.A.A.I. report will try to assess the possibility of “the loss of human control of computer-based intelligences.” It will also grapple, Dr. Horvitz said, with socioeconomic, legal and ethical issues, as well as probable changes in human-computer relationships. How would it be, for example, to relate to a machine that is as intelligent as your spouse? Dr. Horvitz said the panel was looking for ways to guide research so that technology improved society rather than moved it toward a technological catastrophe. Some research might, for instance, be conducted in a high-security laboratory.
The meeting on artificial intelligence could be pivotal to the future of the field. Paul Berg, who was the organizer of the 1975 Asilomar meeting and received a Nobel Prize for chemistry in 1980, said it was important for scientific communities to engage the public before alarm and opposition becomes unshakable.
“If you wait too long and the sides become entrenched like with G.M.O.,” he said, referring to genetically modified foods, “then it is very difficult. It’s too complex, and people talk right past each other.” Tom Mitchell, a professor of artificial intelligence and machine learning at Carnegie Mellon University, said the February meeting had changed his thinking. “I went in very optimistic about the future of A.I. and thinking that Bill Joy and Ray Kurzweil were far off in their predictions,” he said. But, he added, “The meeting made me want to be more outspoken about these issues and in particular be outspoken about the vast amounts of data collected about our personal lives.” Despite his concerns, Dr. Horvitz said he was hopeful that artificial intelligence research would benefit humans, and perhaps even compensate for human failings. He recently demonstrated a voice-based system that he designed to ask patients about their symptoms and to respond with empathy. When a mother said her child was having diarrhea, the face on the screen said, “Oh no, sorry to hear that.”
A physician told him afterward that it was wonderful that the system responded to human emotion. “That’s a great idea,” Dr. Horvitz said he was told. “I have no time for that.” Over two years after HP - a company we all know for its desktops and laptops - killed WebOS, it is taking another stab at the smartphone market. But this time HP is doing it with Android. The company today formally launched Slate 6 and Slate 7. Both are part of VoiceTab series, which means they support 3G connectivity and you can make calls using them. Both are first launching in India and are created keeping in mind the needs of customers in developing market. We have Slate 6 with us for review today. It is a phablet with 6-inch screen and is meant for consumers who like to have one device through which they can make calls as well as enjoy web browsing, video playback and games on the large screen. In India, the phablet category is quite crowded. Almost all phone makers have at least one phablet - a phone with a screen size of more than 5-inch - in the market. Can HP match or surpass the existing phablets with Slate 6? Dive in for the answer. Sleek and with a touch of gold Getting the design of a phablet right is not easy due to its size. But with Slate 6, HP has done well. With a thickness of less than 9mm, Slate 6 is among the slimmest devices in its class. The slim design means it feels compact and easy to hold and carry despite its large screen. Another thing that helps is the weight. At 160gm, Slate 6 is surprisingly light for its size. It is definitely not a device that you will like to - or will be able to - use with a single hand. But it is not as unwieldy as Huawei Ascend Mate. The phone has the usual rounded-corners, flat-box design but there are some design elements that make the Slate 6 stand apart from other devices in the market. The back cover, which can be removed, is made of plastic with matte finish. But the highlight of the back cover is the glass-weave finish. It looks really nice and gives the device a premium feel. The camera industry is facing a serious challenge. Smartphones are getting better at taking pictures. The image sensor inside phones is improving. And so is the quality of images they can shoot.
Smartphones are also convenient. They are easy to use. You carry them with you all the time. They have photo apps. They connect to the internet, and consequently Facebook, Flickr and Instagram, the websites where you share photos with friends. Camera companies are worried. For camera makers, the only way to meet this challenge is to up their game when it comes to image quality and make their devices sexy again by adding a few extra features. With Powershot S200, Canon is trying to do exactly the same. But does it succeed? F2.0 in a compact body. S200 is a camera that belongs to Canon's premium range of S series cameras. At least, it looks and feels premium. S200 has a body made of glossy plastic but the build quality and finishing is better than what you get in other point-and-shoot (P&S) cameras with a similar price. The flap on the connectivity ports fit snuggle. The buttons in their sockets neither feel loose nor too hard to press. S200 is a compact camera and it is possible to carry it in a jeans pocket. The most highlighted part of the camera is the large ring around the lens. By default you can use this ring to control the aperture by rotating left and right while taking pictures. Other than the front ring, the rest of the camera is a standard affair. On the top, there are three buttons - power button, shutter release button and a dial through which you can select between various scenes. On the back you will find usual buttons like menu, play and video recording are placed around a clickable wheel. In sum, if you have used a P&S camera before, S200 will feel very familiar. The camera has a 3-inch screen on the back. The quality of the screen is very good. It shows punchy colours and is bright enough to be usable during outdoor use, which tends to happen a lot with a camera. In terms of specifications, S200 is a cross between high-end and mid-range compact cameras. It packs in a CCD 1/1.7" type image sensor. The key bit to note here is that the size of image sensor is slightly bigger than the average. Most P&S camera have an image sensor (1/2.3") which equates to around 25mm image sensor. S200 has an image sensor that is around 44mm. Theoretically this should allow S200 to click better pictures but the reality is a bit more nuanced. Another piece of hardware that stands out in S200 is its lens, which has a wide aperture of F2. Most of the other P&S cameras have an aperture of F3. The bigger aperture (lower values means bigger aperture) in S200 means it can capture more light, which may give better images in low light. S200 has a 5X zoom and it covers an effective focal length of 24 - 120mm. This is sufficient for daily shooting but if you want high zoom to click pictures of birds or animals while hiking in hills, S200 is not the good camera to carry. S200 also lacks ability to capture RAW photos. As we said earlier it is a camera meant to be used by more mainstream users, who don't usually shoot RAW. But the option would have allowed users to get slightly better images, if they wanted. Even a phone - Lumia 1020 - can shoot in RAW nowadays. Videos are shot in 720P resolution. If you can ignore the lack of FullHD option, S200 is not that a performer when it comes to shooting videos. It captures footage that is clear and doesn't have any focus-related issues.
Should you buy it? Other than its good build quality, compact form factor and a bright F2 lens, S200 doesn't have much to show. In a camera, the most important attribute is the quality of images it clicks. On this parameter S200 is not bad. But it is not good either. It is strictly in the average territory. This is the primary reason why we don't recommend S200 at its current MRP of Rs 19,995. Instead, we recommend Canon Powershot S110. Powershot S110 too features a 1/1.7" image sensor, a F2.0 lens and Wi-Fi. It shoots images in 12MP, supports RAW and can capture videos in FullHD. The overall image quality is slightly better than what S200 can manage. But what really makes us recommend S110 over S200 is that it also costs Rs 2,000 less. Yes, S200 is a newer camera but it offers no advantage over S110. n 1995, laptops were ugly. Not just ugly, but also heavy and expensive. A couple thousand dollars bought a small 4:3 screen, chunky keyboard and a hard drive measured in megabytes, not gigabytes. Thankfully, modern laptops are entirely different beasts. They're lighter, faster, infinitely more versatile and cheaper than ever. And there are a ton of them: In the first quarter of 2010, about 50 million tablets were sold worldwide [source: CNET].

The vast options in today's laptop market makes finding the right system a bit of a challenge. There are desktop replacement laptops, ultralights, high-end systems for gamers and cheap netbooks for taking notes. You have to know exactly what you're looking for.

Here's how you get started: Read through the next 10 pages to figure out what you should keep an eye out for in a new laptop. If you know why each of the next 10 features is important, you'll be prepared to pick the perfect notebook for your needs. 

We're not all looking for the same qualities in a laptop, and the kind of programs you want to run determine your demands in the categories that follow. First, consider why you're buying a laptop. Is it to make PowerPoint presentations, take notes and do other simple business tasks? Or do you plan on watching HD movies, playing video games and video chatting with your friends?

Figure out how much you can afford to spend on a laptop and find the best system in that price range with the features you need. Our first example above, for business use, would be a pretty cheap laptop -- you can easily spend less than $1000 on a model that will run Microsoft Office and other productivity software. Another factor to consider: Do you want a Mac or a PC?

While Apple's laptops are pricier than many PCs, there are only a few Apple notebook models to choose from. Apple's extremely light MacBook Air models start at $1000 and are fast, capable machines, but lack disc drives unlike the larger, more powerful MacBook Pros. The least expensive 13-inch MacBook Pro starts at $1200, while the 15-inch jumps up to a pricey $1800.

If you're looking for a PC, there's a whole lot of hardware to be familiar with before you pick out a winning system. First up: battery life. 

Laptop computing is all about mobility, and battery life is perhaps the most crucial consideration when picking a laptop that's going to be used regularly on the go. If you're in the market for a desktop replacement system -- meaning you'll mostly just leave it on your desk and don't plan on regularly taking it on trips -- battery life isn't quite as critical. Otherwise, pay close attention to how long a laptop's battery will last.

As laptops get slimmer and designers pay more attention to making them sleek and compact, more and more systems use integrated non-removable batteries. The trade off for that sleeker laptop body is that it's impossible to buy a backup battery and swap the two out to double battery life. Finally, always be skeptical of claimed battery life times. The numbers that laptop makers convey often refer to light usage with a dimmed screen. Assume you'll get 1 to 2 hours less than claimed while browsing the Web and running multiple applications -- and possibly even less if you're playing games or doing something else that taxes the computer.

The size and power of a laptop affects how much room there is in the body for a battery (and how quickly it's drained). Next up: finding that size and weight sweet spot. 

Heavy laptops kill mobility. In 2011, computer processor maker Intel began pushing an ultrabook category that focuses on lightweight systems -- usually weighing about 3 pounds (1.3 kilograms) -- to make use of its ultra low voltage CPUs. Ultrabooks do away with disc drives and focus on portability, long battery life and a starting price range of $800 to $1000. These laptops aren't powerful enough for some users, and do away with a disc drive others find important, but they demonstrate a trend towards lightweight notebooks that are extremely portable.

Pounds add up quickly. Most 15-inch (38-centimeter) laptops often weigh around 5 pounds (2.3 kilograms), which is a manageable weight to carry between home and the office every day. But some laptops creep up into the 7 to 8 pound (3.2 to 3.6 kilogram) range, and laptops that heavy can easily be relegated to at-home machines due to their weight.

There are a couple ways to get around the weight issue. You can always shop for a brand like Sony's Vaio, which makes weight a priority concern. The downside: Vaios are more expensive than other notebooks with similar performance. Second option: Go for a computer with a smaller screen. 

There are three common screen sizes in the notebook industry: 13 inches (33 centimeters), 15 inches (38 centimeters) and 17 inches (43 centimeters). The smallest in this group of laptops obviously prioritize portability, and often forgo DVD drives to make their bodies thinner and lighter. The mid-size category has a bit more range: Some heavier systems operate as desktop replacements, while others are light enough to still be easily portable while offering large screens. The largest category of laptops are, well, pretty huge. They always offer high resolution displays and powerful hardware, but can easily weigh up to 10 pounds (4.5 kilograms).

Generally, 15-inch (38-centimeter) and especially 17-inch (43-centimeter) laptops are large enough to be decent TV/computer monitor substitutes for watching video, especially on the go. Smaller notebooks, measuring 11 to 14 inches (27.9 to 35.6 centimeters) may be a bit small for watching movies, depending on your personal taste. More importantly, their displays are often lower resolution. The resolution is the total number of pixels contained in the display -- more pixels allow for more content to be displayed on screen at once.

Resolutions typically range from 1366 by 768 -- just a bit larger than 720p -- to 1920 by 1080, aka 1080p. Laptop makers will often use lower resolution displays to cut costs. On the next page, we'll get into the processor and graphics card, two other critical components. But if you're comparing two computers, go for the higher resolution screen when possible, or choose an HD upgrade option if it's available. 

A computer's processor determines how efficiently it can run programs, multi-task and basically do everything we expect of modern computers. Processors get faster and more efficient every year. Most Windows-based computers run on Intel's processors; smaller ultraportables, such as Apple's thin MacBook Air, run on ultra low voltage processors that draw less power than some of Apple's other chips. Quad-core chips deliver more powerful performance, but even dual-core processors are up to the task of playing 1080p video and running system-intensive programs like Photoshop.

The graphics processor, or GPU, is important when it comes to playing HD video and running games. Many laptops use integrated graphics rather than dedicated graphics chips. These are less powerful (and also less battery intensive), but powerful enough to decode 1080p video. Dedicated graphics chips are important for playing video games, but for the average computer user, the two components on the next page -- hard drive storage memory -- are more important. 

For years, all laptops stored data on spinning physical discs called hard drives. Most of them still do, but faster solid state drives that use silicon-based memory are becoming more affordable and more prevalent in mobile computers. Because solid state drives don't rely on moving parts, they're more reliable in computers that tend to get bumped and jostled around. The downside: They're expensive and don't offer nearly as much data storage.

Storage space may not be a big concern for you -- if you store most of your data in the cloud and don't plan to load a computer up with gigabytes of music and video, a SSD or small HDD will suit you just fine. Random access memory (RAM) is a different story. Every piece of software running on a computer and the operating system itself (usually meaning Windows) stores data in RAM to function. The more RAM you have, the better -- it's smart to upgrade this component when possible. To run Windows 7, 4 gigabytes is a comfortable minimum.

Apple's MacBook Air and similar computers in the Ultrabook category of PCs are beginning to offer SSDs at reasonable prices. If you need more than 128GB of storage, best stick to a traditional hard drive, or be prepared to pay a pretty penny for an upgrade. Speedy SSD storage is fantastic, but in many cases going with a thin-and-light computer means giving up a disc drive and embracing the Internet cloud. 

Optical storage mediums have been key computer components since the first CD drives arrived on the scene, but cheap flash storage in the form of USB drives and cloud storage on the Internet have nearly eradicated their necessity. When was the last time you used a CD burner? Do you watch DVDs on your computer, or do you stream movies from Netflix? If you do either of those things regularly, or need to be able to burn DVDs or CDs for work, that's okay -- there are still plenty of laptops outfitted with CD/DVD combo drives. Blu-ray drives are even optional in a small selection of laptops, most commonly the media-focused 15-inch (38-centimeter) and 17-inch (43-centimeter) models.

If you're on the fence about needing a disc drive, educate yourself on cloud storage. Dropbox makes it easy to transfer files between multiple computers. Netflix makes it easy to stream movies and TV shows from the cloud. Spotify, Rdio, iTunes Match and a host of other music services allow you to stream music to your computer as long as you have an Internet connection. And there's always the option of using an external hard drive or flash drive to transfer files from a computer that does have a disc drive.

That brings us to another important consideration: making sure your laptop has all the ports you need. 

Every computer user is familiar with the USB port, but not everyone is aware that a much faster update to the standard, USB 3.0, is slowly spreading through the computer industry. USB 3.0 can be 10 times faster than its predecessor in real-world usage, delivering transfer speeds of up to 400 megabytes per second [source: EverythingUSB]. Even if you don't own any USB 3.0 hardware right now, consider future proofing when buying a new laptop. You might be really thankful you have that USB 3.0 port down the road.

There are other ports to consider as well. Do you want HDMI to output video to a TV? Do you need an SD card slot for downloading digital camera photos to your laptop? Will you need an Ethernet port for Internet or will a laptop's built-in WiFi connection be enough?

If you plan on keeping a laptop for several years, learn how customizable or expandable it is. For example, Apple's laptops are known for their build quality -- and for being locked down and difficult to perform maintenance on. The batteries are integrated, rather than removable. Many PCs have removable batteries that can easily be swapped out or replaced.

Google your laptop to see if owners have found it easy to add additional RAM or swap out the hard drive. Either of those upgrades could give your laptop an extended lease on life sometime down the road, but some casings are much easier to open up than others. On a similar note, some laptops are especially designed to keep your data more secure. That's our next topic. 

We keep mountains of personal information on our computers. There's always a risk when storing information digitally and on the Internet, but laptops elevate that risk by being much easier to steal than desktop machines. Some laptops, specifically those aimed at business and enterprise users, are built with these concerns in mind. For example, fingerprint scanners are found on plenty of business PCs and require users to pass a scan before logging into the operating system.

Many laptops also include Kensington security slots. Security cables are attached to these slots to bolt a laptop to a table or kiosk. Ever look at laptops at a store like Best Buy? Then you've probably seen one of these in action. Durability is another factor when it comes to security: To ensure no data loss happens due to a butterfingers moment, Panasonic sells ToughBook laptops designed to survive a nasty drop onto unforgiving concrete.

Laptop security features can help protect you, but never assume they'll keep your data safe from determined crooks. Being careful with your laptop is the best way to protect it. And that brings us to our final tip: choosing a warranty and judging laptop release cycles to know when to buy. 

Buying consumer electronics is always a battle against irrelevancy. It's tough to buy something that will be outdated in just a few months, but there's always new gear just beyond the horizon -- you can't put off a purchase forever. To maximize the value of a laptop purchase, buy shortly after a product refresh. New systems usually come out shortly after Intel launches new processors. Look up review for laptops. See a model that was highly rated but came out nine months ago? A newer version will probably be along within a few months. Don't buy a laptop months into its release cycle unless you're shopping on a budget and spy a killer deal.

Picking a warranty isn't an easy decision, either. If you're clumsy and accident prone (or paranoid), opting for a long warranty will put your mind at ease. But it will also cost you an extra hundred dollars or more, depending on the kind of coverage you choose. Total coverage packages for accidents are expensive, but you'll be glad you have it if you accidentally step on your screen and break the LCD. And there's nothing wrong with going for the cheapest limited warranty you can -- just don't drop your new laptop! The personal computer (PC) defines a computer designed for general use by a single person. While a Mac is a PC, most people relate the term with systems that run the Windows operating system. PCs were first known as microcomputers because they were a complete computer but built on a smaller scale than the huge systems in use by most businesses. A PC that is not designed for portability is a desktop computer. The expectation with desktop systems are that you will set the computer up in a permanent location. Most desktops offer more power, storage and versatility for less cost than their portable brethren. Also called notebooks, laptops are portable computers that integrate the display, keyboard, a pointing device or trackball, processor, memory and hard drive all in a battery-operated package slightly larger than an average hardcover book. Netbooks are ultra-portable computers that are even smaller than traditional laptops. The extreme cost-effectiveness of netbooks (roughly $300 to $500) means they're cheaper than almost any brand-new laptop you'll find at retail outlets. However, netbooks' internal components are less powerful than those in regular laptops. Personal Digital Assistants (PDAs) are tightly integrated computers that often use flash memory instead of a hard drive for storage. These computers usually do not have keyboards but rely on touchscreen technology for user input. PDAs are typically smaller than a paperback novel, very lightweight with a reasonable battery life. A slightly larger and heavier version of the PDA is the handheld computer. Another type of computer is a workstation. A workstation is simply a desktop computer that has a more powerful processor, additional memory and enhanced capabilities for performing a special group of task, such as 3D Graphics or game development. A computer that has been optimized to provide services to other computers over a network. Servers usually have powerful processors, lots of memory and large hard drives. The next type of computer can fill an entire room. In the early days of computing, mainframes were huge computers that could fill an entire room or even a whole floor! As the size of computers has diminished while the power has increased, the term mainframe has fallen out of use in favor of enterprise server. You'll still hear the term used, particularly in large companies to describe the huge machines processing millions of transactions every day. The latest trend in computing is wearable computers. Essentially, common computer applications (e-mail, database, multimedia, calendar/scheduler) are integrated into watches, cell phones, visors and even clothing. For more information see these articles on computer clothing, smart watches and fabric PCs. As personal computers became affordable, must-have Internet gateways in the late 1990s, individual models took a backseat to larger brands. Dell didn't bother advertising special model names. It just advertised one major selling point: cheap. When Apple made a comeback with iMacs, and later MacBooks and MacBook Pros, you were either a Mac person or a PC person. Whether that PC was a Dell, or an HP, or an ASUS didn't make much difference.

But when the PC market was younger, smaller and much more expensive, things were different. Your PC was everything. In the late 1970s and 1980s, buying a computer was a huge investment, likely costing thousands of dollars and determining what kind of software you'd be running for the next several years. As a result, computer hobbyists picked favorites. And they stuck by them.

The wars between IBM fans, Tandy owners, Apple devotees and Commodore diehards were fiercer than any Mac versus PC argument. As a result, those early systems had an immense impact on those early home computer users, creating a generation of tech-savvy programmers. Ask any of them about their first (or favorite) computer, and they'll be able to tell you exactly what it was.

A few extremely popular breakout models sold millions of units. These are 10 of the most popular computers ever built. Your favorite may be among them.  Business success means to deliver the right solutions at the right time to the right markets. Requirements Engineering is the discipline within systems and software engineering that bridges the entire life cycle and thus determines success or failure of a product or project. This blog provides a fresh look on requirements engineering, and why you need to improve it...

Imagine woodcutters in a forest. They work hard and cut tree after tree to build a street. It is a huge physical effort and their foreman drives them hard to stay on schedule. He wants to cut a certain amount of trees per day and provides the workers with all they need to achieve this objective. Suddenly the client who ordered the street to be build comes back and shouts: "wrong direction". Despite all the operational efficiency of the foreman and his team, they did not manage to deliver the intended customer experience.

Sounds familiar? Indeed, this is what we observe with many software products. Complex features must be developed at a low cost, but once they hit the market they won't sell as expected. Or customers demand many changes during the development process, thus reducing margins dramatically from initial targets. Software requirements engineering is the systematic approach to providing direction before and during software development. It provides the means to deliver the right products or solutions at the right time for the right markets.

Requirements Engineering (RE) is the disciplined and systematic approach – that is the "engineering approach" – to elicit, specify, analyze, commit, validate and manage requirements while considering user, technical, economic and business-oriented needs and objectives. The goal of RE is to develop good – not perfect – requirements and to manage them during development with respect to risks and quality. In short, good RE makes the difference between a winning product and a mere set of features.

Insufficient requirements engineering is the major contributor to software projects risks and problems. One might argue that this is an issue for all disciplines because requirements will only be clear when an artifact is finished, but certainly in software is causing much more damage due to not considering its impact and even thinking that software is "soft" and therefore changeable. Other disciplines since long have built very clear boundaries when changes are impossible or at least very costly. Take hardware, where it is clear that an ASIC has a cycle time of some weeks and each change will mean a new production circle. In civil engineering changes are possible, but rather early in the life-cycle they become expensive or impossible – without destroying the product and re-creating it.

Why is that? Why do we have such difficulties with requirements and their handling? Handling software requirements represents an ill-defined problem because within a software system, requirements are not fully known until it is practically used. Not managing this problem results in requirements instability, which means project delays. Typical results from poor RE are insufficient project planning, continuous changes in the project, delays, configuration problems, defects, and overall customer dissatisfaction due to not keeping commitments or not getting the product they expect.

Here some hard facts which show the project impacts of insufficient RE. Only half of the originally allocated requirements appear in the final released version. For each project, there is an average sum of roughly one million Euro that is spent in excess before the project is terminated – for good or bad. Most software systems use less than half of their features. 80% of all defects in test result from missing (31%) or wrong (49%) requirements. 43% of all failures of embedded systems during operations are due to insufficient system specification and analysis.

There is a clear business case for good RE. Typically only 3-6% of all project effort is spent on requirements engineering and management. Doubling this effort would yield a 20% reduction of product life-cycle cost: Less rework, shorter cycle time, efficiency. Some concrete data points have been assembled from longitudinal studies of many similar projects in almost identical environments. If the preparation effort, specifically on requirements is below 5% there will be large delays. Moving this share to 10-20% of total project effort would reduce delays to below 30%. Preparation means all activities in front of project start, that is requirements elicitation, analysis, specification, project planning, stakeholder agreements and resource allocation. Projects with 5% relative effort on RE activities would show a budget overrun of 80-200%. Doubling this effort towards 8-14% reduces the cost overrun to below 60%. hurdle has now been overcome." 


Theories show how computing devices that operate according to quantum mechanics can solve problems that conventional (classical) computers, including super computers, can never solve. These theories have been experimentally tested for small-scale quantum systems, but the world is waiting for the first definitive demonstration of a quantum device that beats a classical computer.

Now, researchers from the Centre for Quantum Photonics (CQP) at the University of Bristol together with collaborators from the University of Queensland (UQ) and Imperial College London have increased the likelihood of such a demonstration in the near term by discovering a new way to run a quantum algorithm with much simpler methods than previously thought.

The first definitive defeat for a classical computer could be achieved with a quantum device that runs an algorithm known as Boson Sampling, recently developed by researchers at MIT.

Boson Sampling uses single photons of light and optical circuits to take samples from an exponentially large probability distribution, which has been proven to be extremely difficult for classical computers.

Unlike other quantum algorithms, Boson Sampling has the benefit of being practical for near-term implementations, with the only experimental drawback being the difficulty of generating the dozens of single photons required for the important quantum victory.

However, the Bristol-UQ-Imperial researchers have found that the Boson Sampling algorithm can still be proven to be hard for classical computers when using standard probabilistic methods to generate single photons.

Dr Anthony Laing who led the CQP elements of the research said: "We realised we could chain together many standard two-photon sources in such a way as to give a dramatic boost to the number of photons generated."

Dr Austin Lund from UQ and currently on sabbatical in CQP added: "Once we had the idea for the boosted source, we needed to prove that it could solve a version of the Boson Sampling algorithm. We hope that the last major experimental hurdle has now been overcome." team of Berkeley Lab researchers believes it has uncovered the secret behind the unusual optoelectronic properties of single atomic layers of transition metal dichalcogenide (TMDC) materials, the two-dimensional semiconductors that hold great promise for nanoelectronic and photonic applications.

Using two-photon excitation spectroscopy, the researchers probed monolayers of tungsten disulfide, one of the most promising of 2D materials, and found evidence for the existence of excitonic dark states -- energy states in which single photons can be neither absorbed nor emitted. These excitons were predicted from ab initio calculations by members of the research team to have an unusual energy sequence, plus excitonic binding energy and bandgaps that are far larger than was previously suspected for 2D TMDC materials.

"Discovery of very large excitonic binding energy and bandgaps and its nonhygrogenic nature in 2D semiconductor materials is important not only for understanding the unprecedented light-matter interaction arising from strong many-body effect, but also for electronic and optoelectronic applications, such as ultra-compact LEDs, sensors and transistors," says Xiang Zhang, director of Berkeley Lab's Materials Sciences Division and the leader of this study. "Such a large binding energy -- 0.7eV -- could also potentially make room-temperature excitons stable for future quantum computing efforts."

Zhang holds the Ernest S. Kuh Endowed Chair Professor at the University of California (UC) Berkeley, directs the National Science Foundation's Nano-scale Science and Engineering Center, and is a member of the Kavli Energy NanoSciences Institute at Berkeley. He and Berkeley Lab theoretical physicist Steven Louie, also with the Materials Sciences Division and UC Berkeley, are the corresponding authors of a paper in Nature that describes this research. The paper is titled "Probing excitonic dark states in single-layer tungsten disulphide." Co-authors are Ziliang Ye, Ting Cao, Kevin O'Brien, Hanyu Zhu, Xiaobo Yin, and Yuan Wang.

Excitons are bound pairs of excited electrons and holes that may cause significant deviations between photon absorption or emission energies and the electronic bandgaps that enable semiconductors to function in devices. 2D TMDC materials have generated quite a buzz in the electronics industry because they offer superior energy efficiency and carry much higher current densities than silicon. Furthermore, unlike graphene, the other highly touted 2D semiconductor, TMDCs have finite bandgaps. This makes them more device-ready than graphene, which has no natural bandgaps. However, questions marks hovering over the bandgap size and excitonic effect in 2D TMDCs have hampered their development.

"By experimentally revealing 2D excitonic dark states in a TMDC monolayer, we have demonstrated intense many-electron effects in this class of 2D semiconductors," says Ziliang Ye, a member of Zhang's research group and one of two lead authors of the Naturepaper. "Our discovery provides a basis for exploiting the unusual interactions between light and matter that result from strong excitonic effects, and should also enable better designs of heterostructures that involve TMDC monolayers."

In addition to LEDs and photodetectors, the discovery of strongly bound excitonic dark states could also hold important implications for "valleytronics," a highly promising potential new route to novel electronics and ultrafast data-processing.

"In valleytronics, information is encoded in a wave quantum number that describes which valley of the energy-momentum landscape a carrier belongs to as it moves through a crystal lattice," says Louie. "Our work provides new understanding and information on the photo-excited states, and on the resulting carriers where the valley information is encoded."

Says Ting Cao, a member of Louie's research group and the other lead author of the Nature paper, "2D TMDCs should be also well-suited for the next generation of flexible devices and wearable electronics."

This work was supported by the DOE Office of Science with computer time provided by the DOE National Energy Research Scientific Computing Center.

Objects in space tend to spin -- and spin in a way that's totally different from the way they spin on earth. Understanding how objects are spinning, where their centers of mass are, and how their mass is distributed is crucial to any number of actual or potential space missions, from cleaning up debris in the geosynchronous orbit favored by communications satellites to landing a demolition crew on a comet.

In a forthcoming issue of the Journal of Field Robotics, MIT researchers will describe a new algorithm for gauging the rotation of objects in zero gravity using only visual information. And at the International Conference on Intelligent Robots and Systems this month, they will report the results of a set of experiments in which they tested the algorithm aboard the International Space Station.

On all but one measure, their algorithm was very accurate, even when it ran in real time on the microprocessor of a single, volleyball-size experimental satellite. On the remaining measure, which indicates the distribution of the object's mass, the algorithm didn't fare quite as well when running in real time -- although its estimate may still be adequate for many purposes. But it was much more accurate when it had slightly longer to run on a more powerful computer.

Space trash

"There are satellites that are basically dead, that are in the 'geostationary graveyard,' a few hundred kilometers from the normal geostationary orbit," says Alvar Saenz-Otero, a principal research scientist in MIT's Department of Aeronautics and Astronautics. "With over 6,000 satellites operating in space right now, people are thinking about recycling. Can we get to that satellite, observe how it's spinning, and learn its dynamic behavior so that we can dock to it?"

Moreover, "there's a lot of space trash these days," Saenz-Otero adds. "There are thousands of pieces of broken satellites in space. If you were to send a supermassive spacecraft up there, yes, you could collect all of those, but it would cost lots of money. But if you send a small spacecraft, and you try to dock to a small, tumbling thing, you also are going to start tumbling. So you need to observe that thing that you know nothing about so you can grab it and control it."

Joining Saenz-Otero on the paper are lead author Brent Tweddle, who was an MIT graduate student in aeronautics and astronautics when the work was done and is now at NASA's Jet Propulsion Laboratory; his fellow grad student Tim Setterfield; AeroAstro Professor David Miller; and John Leonard, a professor of mechanical and ocean engineering.

The researchers tested their algorithm using two small satellites deployed to the space station through MIT's SPHERES project, which envisions that herds of coordinated satellites the size of volleyballs would assist human crews on future space missions. One SPHERES satellite spun in place while another photographed it with a stereo camera.

Cart and horse

One approach to determining the dynamic characteristics of a spinning object, Tweddle explains, would be to first build a visual model of it and then base estimates of its position, orientation, linear and angular velocity, and inertial properties on analysis of the model. But the MIT researchers' algorithm instead begins estimating all of those characteristics at the same time.

"If you're just building a map, it'll optimize things based on just what you see, and put no constraints on how you move," Tweddle explains. "Your trajectory will likely be very jumpy when you finish that optimization. The spin rate's going to jump around, and the angular velocity is not going to be smooth or constant. When you add in the dynamics, that actually constrains the motion to follow a nice, smooth, flowing trajectory."

In order to build up all its estimates simultaneously, the algorithm needs to adopt a probabilistic approach. Its first guess about, say, angular velocity won't be perfectly accurate, but the relationship to the correct measurement can be characterized by a probabilistic distribution. The standard way to model probabilities is as a Gaussian distribution -- the bell curve familiar from, for example, the graph of the range of heights in a population.

"Gaussian distributions have a lot of probability in the middle and very little probability out at the tails, and they go from positive infinity to negative infinity," Tweddle says. "That doesn't really map well with estimating something like the mass of an object, because there's a zero probability that the mass of the object is going to be negative." The same is true of inertial properties, Tweddle says, which are mathematically more complicated but are, in some respects, analogously constrained to positive values.

Rational solution

What Tweddle discovered -- "through trial and error," he says -- is that the natural log of the ratio between moments of inertia around the different rotational axes of the object could, in fact, be modeled by a Gaussian distribution. So that's what the MIT algorithm estimates.

"That's a novel modeling approach that makes this type of framework work a lot better," Tweddle says. "Now that I look at it, I think, 'Why didn't I think of that in the first place?'"

"The rationale there is that we may need to, or want to, go up and service satellites that have been up there a long time for which those properties [such as center of mass and inertia] aren't well known," says Larry Matthies, supervisor of the Computer Vision Group at the Jet Propulsion Laboratory, "either because the original design data is hard to get our hands on, or not available, or -- this may be less plausible -- the numbers have changed on orbit from the original design, or maybe it's actually somebody else's satellite that we want to go up and mess around with."

Moreover, Matthies says, even in cases in which design data are available, satellites may have mobile appendages whose position significantly affects their moments of inertia. "Maybe there are articulations on the satellite, and how those properties are affected by the various booms and things has to be considered," he says. "If there's some uncertainty in that geometry, again, you would need this kind of information."

The work was funded by both NASA and the U.S. Defense Advanced Projects Research Agency (DARPA). Computer scientists at the University of California, San Diego have successfully funded on Kickstarter a new and improved version of CodeSpells, a first-person player game they developed that teaches players how to code.

The game's previous iteration, developed by UC San Diego computer science Ph.D. students Sarah Esper and Stephen Foster, has been in use in dozens of schools throughout the world for more than a year. The researchers have been using the game as a platform to learn about the best ways to teach children how to code. They have presented their findings at a wide range of academic conferences, including the upcoming Koli Calling International Conference on Computing Education Research Nov. 20 to 23 in Koli, Finland.

In this latest paper, "CodeSpells: Bridging Educational Language Features with Industry-Standard Languages," the researchers demonstrate that after playing CodeSpells for either four hours over four weeks or 10 hours over seven days, children ages 8 to 12 were able to write code by hand in Java.

"It is the goal of CodeSpells to provide a rich experience of computer science education to students who may not have access to an educator," Esper said.

Researchers now want to make the game more attractive and more fun to play. But they need funds to improve the game's graphics and coding interface. Enter Kickstarter, where the project has already met and exceeded its $50,000 fundraising goal.

"We want the game to be educational, but our biggest goal is to make sure it's fun," Foster said.

He and Esper have co-founded ThoughtSTEM, along with UC San Diego biochemistry Ph.D. student Lindsey Handley, to teach children ages 8 to 18 how to code, via onsite classes and video games, including CodeSpells and Minecraft.

In its previous iteration, CodeSpells sent players on quests, which helped them master spells, written in Java. This new version is more open-ended much like Minecraft -- a so-called sandbox game. The players are wizards that can modify the world around them at will. They can build mountains and valleys, levitate objects and start fires. They do so by using Blocky, a visual programming language created by Google, or Javascript.

The hope is that players will come up with their own quests. Researchers also hope that as players tinker with the game, they'll come up with their own exciting spells and share those. The goal is to create a vibrant online community, much like the one that has developed around Minecraft.

The game will feature several modes out of the box, but players will be able to create their own modes too. They'll have the tools to create everything from modes to survive in the wilderness to modes to balance an eco-system. They can even create multi-player magic-based sports to play with their friends.

The game will feature four elements: earth, fire, water and air, which the players can manipulate via spells. So far, computer scientists have completed an early version of gameplay for earth magic. The Kickstarter will fund the development of magic for fire, air and water, with an alpha version to be released on Christmas Day 2014, a beta version in June 2015, and the final copy of the game's creative mode to be released September 2015.

If the Kickstarter exceeds its $50,000 goal, the game's multiplayer functions will be enhanced. In addition, the game will add a fifth element, life, which would give wizards control over animals and plants within the game. That feature would be released in summer 2016. By early 2017, players would be able to create their own species and custom characters within the game.

Rewards for the Kickstarter range from a digital copy of CodeSpells for a $10 donation to access to district-wide licenses for CodeSpells in alpha and beta versions and computational thinking courses to a teacher at each school within a school district for a $5,000 donation. 

Quantum computing relies on the laws of quantum mechanics to process vast amounts of information and calculations simultaneously, with far more power than current computers. However, development of quantum computers has been limited as researchers have struggled to find a reliable way to increase the power of these systems, a power measured in Q-Bits.

Previous attempts to find the elusive Majorana particle have been very promising but have not yet provided definitive and conclusive evidence of its existence.

Now, researchers from the University of Surrey and the Ben-Gurion University in Israel believe they have uncovered a key method for detection of the Majorana particle, potentially enabling reliable Q-Bits to be developed. This new research proposes using photons (particles of light) and super-conducting circuits to probe and measure semiconductor nanowires, where it is thought these particles exist at certain controlled conditions. If the particles are present, they will be revealed through a specific pattern with microwave spectroscopy.

Currently the most powerful quantum computer in existence has a power of eight Q-Bits. Once the particle is confirmed, researchers believe it will enable functioning topological Q-Bits to be produced, breaking the barriers on the way to scaling up quantum computation to many Q-Bits.

"We know what we are looking for, we just haven't found it yet -- it's the ultimate physics treasure hunt! We are confident that the method we are proposing will bring us closer to unlocking the untapped potential of quantum computing in areas such as code breaking, complicated mathematical problem-solving and scientific simulation of advanced materials" said lead-author Dr Eran Ginossar, the University of Surrey.

The new method has attracted the interest of leading experimental groups and it is hoped that the new method will be trialled within the next year.

Quantum computing is one pillar of quantum technology, an area where the UK is posed to make a large investment. Last year the government announced funding of £270million for the development and application of this technology. smart headlight developed at Carnegie Mellon University's Robotics Institute enables drivers to take full advantage of their high beams without fear of blinding oncoming drivers or suffering from the glare that can occur when driving in snow or rain at night.

The programmable headlight senses and tracks virtually any number of oncoming drivers, blacking out only the small parts of the headlight beam that would otherwise shine into their eyes. During snow or rain showers, the headlight improves driver vision by tracking individual flakes and drops in the immediate vicinity of the car and blocking the narrow slivers of headlight beam that would otherwise illuminate the precipitation and reflect back into the driver's eyes.

"Even after 130 years of headlight development, more than half of vehicle crashes and deaths occur at night, despite the fact there is much less traffic then," said Srinivasa Narasimhan, associate professor of robotics. "With our programmable system, however, we can actually make headlights that are even brighter than today's without causing distractions for other drivers on the road."

Robert Tamburo, the project's lead engineer, will present findings from tests of the system in the lab and on the streets of Pittsburgh on Sept. 10 at the European Conference on Computer Vision in Zurich, Switzerland. More information, including a video, is available on the project website. The system devised by Narasimhan, Tamburo and the rest of the research team uses a DLP (Digital Light Processing) projector instead of a standard headlight or cluster of LEDs. This enables the researchers to divide the light into a million tiny beams, each of which can be independently controlled by an onboard computer.

A camera senses oncoming cars, falling precipitation and other objects of interest, such as road signs. The one million light beams can then be adjusted accordingly, some dimmed to spare the eyes of oncoming drivers, while others might be brightened to highlight street signs or the traffic lane. The changes in overall illumination are minor, however, and generally not noticeable by the driver.

System latency -- the time between detection by the camera and a corresponding adjustment in the illumination -- is between 1 and 2.5 milliseconds, Tamburo said. This near-instantaneous reaction means that in most cases the system doesn't have to employ sophisticated algorithms to predict where an oncoming driver or a flake of snow will be by the time the headlight system responds.

"Our system can keep high beams from blinding oncoming drivers when operating at normal highway speeds," Narasimhan said. Rain and snow present a more difficult problem, he noted; the system reduces glare at low speeds, but becomes less effective as speed increases.

In addition to preventing glare, the projector can be used to highlight the traffic lane -- a helpful driving aid when roads have unmarked lanes or edges, or when snow obscures lane markings. When tied to a navigation system, the programmable headlights also can project arrows or other directional signals to visually guide drivers.

"We can do all this and more with the same headlight," Narasimhan said. That's in contrast to new headlight systems that some automakers are installing. These include multi-LED systems that reduce glare to oncoming drivers by darkening some LEDs as well as swiveling headlights that help drivers see down curved roads. "Most of these are one-off systems, however, with different headlights required for different specialized tasks," he added.

The research team assembled their experimental system from off-the-shelf parts and mounted the system atop the hood of a pickup truck, serving as the equivalent of a third headlight during street tests. The team plans to install a smaller version next year in the headlight slot of a truck.

Though currently larger than standard headlights, Narasimhan said the smart headlights could be accommodated by trucks and buses, whose headlights are especially prone to causing glare because they are positioned high off the ground. Eventually, miniaturization should make the smart headlights compatible with smaller vehicles.

The research team includes Takeo Kanade, professor of computer science and robotics; Anthony Rowe, assistant research professor of electrical and computer engineering (ECE); Abhishek Chugh, a master's degree student in computer science; Subhagato Dutta and Vinay Palakkode, both master's degree students in ECE; and Eriko Nurvitadhi and Mei Chen of Intel Research.

The research was supported by Ford Motor Co., the Intel Science and Technology Center for Embedded Computing, the Office of Naval Research and the National Science Foundation. It is part of the Technologies for Safe and Efficient Transportation Center, a U.S. Department of Transportation University Transportation Center at Carnegie Mellon. Researchers from North Carolina State University have developed artificial intelligence (AI) software that is significantly better than any previous technology at predicting what goal a player is trying to achieve in a video game. The advance holds promise for helping game developers design new ways of improving the gameplay experience for players.

"We developed this software for use in educational gaming, but it has applications for all video game developers," says Dr. James Lester, a professor of computer science at NC State and senior author of a paper on the work. "This is a key step in developing player-adaptive games that can respond to player actions to improve the gaming experience, either for entertainment or -- in our case -- for education."

The researchers used "deep learning" to develop the AI software. Deep learning describes a family of machine learning techniques that can extrapolate patterns from large collections of data and make predictions. Deep learning has been actively investigated in various research domains such as computer vision and natural language processing in both academia and industry.

In this case, the large collection of data is the sum total of actions that players have made in a game. The predictive AI software can then draw on all of that data to determine what an individual player is trying to accomplish, based on his or her actions at any given point in the game. And the software is capable of improving its accuracy over time, because the more data the AI program has, the more accurate it becomes.

"At some point that improvement will level off, but we haven't reached that point yet," Lester says.

To test the AI program, the researchers turned to an educational game called "Crystal Island," which they developed years earlier. While testing Crystal Island, the researchers amassed logs of player behavior (tracking every action a player took in the game) for 137 different players. The researchers were able to test the predictive AI software against the Crystal Island player logs to determine its accuracy in goal recognition. In other words, they could tell the AI everything a player had done in Crystal Island up to a certain point and see what goal the AI thought the player was trying to accomplish. By checking the AI's response against the player log, the researchers could tell whether the AI was correct.

"For games, the current state-of-the-art AI program for goal recognition has an accuracy rate of 48.4 percent," says Wookhee Min, a Ph.D. student at NC State and lead author of the paper. "The accuracy rate for our new program is 62.3 percent. That's a big jump."

The paper, "Deep Learning-Based Goal Recognition in Open-Ended Digital Games," will be presented at the Tenth Annual Conference on Artificial Intelligence and Interactive Digital Entertainment, being held Oct. 5-7 in Raleigh, North Carolina. The research was supported by the National Science Foundation under grants IIS-1138497 and IIS-1344803. Pioneering research at the University of Essex into making ultra-high definition TV (UHDTV) available to the masses will be showcased at the world's biggest international broadcasting event this week.

The technology has already been used to successfully live stream the University's Graduation ceremonies across the globe earlier this year via the internet and will now form the basis of new research into the challenge of live steaming 8K images, which are 16 times higher resolution than current HD.

Led by Professor Stuart Walker, from the School of Computer Science and Electronic Engineering, the team at Essex managed to live stream the 4K UHDTV -- four times the current HD resolution -- by adapting off-the-shelf video compression equipment.

The beauty of their project is that they were able to compress the ultra-high definition image so it could be live streamed at just 8 Mbit/s via ordinary broadband connections without loss of quality and in real time -- avoiding the frustrations of waiting for the stream to buffer.

"This type of live streaming involves a huge amount of raw data, equivalent to about 63,000 phone calls being made all at once. It was a major challenge to be able to compress this signal to a size which could be accessed by even the most basic broadband connection around the world. But we did and the results were amazing," explained Professor Walker.

To fully appreciate the high quality image, the recipients would have needed a special 4K TV, but with prices becoming more affordable for these high-tech TVs it is only a matter of time before they are commonplace in homes.

"The barrier for 4K images being streamed to homes is artificial," added Professor Walker. "We have shown that by using off-the-shelf equipment we can deliver live 4k streaming so it is accessible to most people in the planet."

The team will now focus on the challenge of making live 8K images affordable to all by compressing the data so it too can be live streamed via regular broadband connections.

Their research will be showcased at the International Broadcasting Convention (IBC 2014) in Amsterdam, which starts on Thursday. Malware is a type of malicious program whose general aim is to profit economically by carrying out actions without the user's consent, such as stealing personal information or committing economic fraud. We can find it "in any type of device ranging from traditional cell phones to today's smartphones, and even in our washing machine," explained one of the researchers, Guillermo Suarez de Tangil, from the Computer Science Department at UC3M.

With the massive sales of smartphones in recent years (more than personal computers in all of their history), malware developers have focused their interest on these platforms. The amount of malware is constantly increasing and it is becoming more intelligent; for that reason, "security analysts and market administrators are overwhelmed and cannot afford exhaustive checking for each app," noted Guillermo Suarez de Tangil. The development of this type of malicious programs has become a large industry that incorporates code reuse methodology. "They don't create a program from scratch, but rather they create a new sample," he stated.

The tool, developed by these UC3M researchers, baptized DENDROID and detailed in a study published in the review Expert Systems with Applications, allows security analysts to scrutinize a large quantity of apps to determine the origins of a malware sample and the family to which it belongs. In addition, if a classification not directly matching a specific family is found, it allows a phylogenetic tree to be extracted from the application to determine the malware's possible ancestors. "The developers generally reuse components of other malwares, and that precisely is what allows us to construct this genetic map," Guillermo Suárez de Tangil explained. This information allows security analysts to take on the challenge of analyzing samples of malware never seen before.

The antiviruses used in smartphones employ detection engines based on signatures, which identify a specific type of malware from some features previously observed. "For this reason, its effectiveness is questionable," elaborated Guillermo, because smartphone resources are more limited that those of a PC. Furthermore, the high frequency of new pieces of malware makes it impossible to incorporate signatures at the same time," he pointed out. In contrast, the new tool they have developed "will help an analyst to protect markets and ensure that users will not need to completely depend on detectors in smartphones," the researcher concluded. Yadira Alatriste, researcher at the Autonomous Metropolitan University of Mexico (UAM) designed a remote medical care system that supports the rehabilitation of people with spasticity, an alteration of the nervous system related to increased tone muscle making motor skills difficult or impossible for those affected.

The eSpasti system was created as part of her doctoral research at the Polytechnic University of Catalonia, integrating computer and video technologies, which serve as the platform for issuing targeted medical therapies in spastic patients.

Such technology allows the patient to receive care and rehabilitation exercises without the obligatory presence of a physical therapist in the hospital; it can be done at home with the help of a caregiver or family member. This diminishes a considerable amount of the costs of relocating the patient, while allowing the observation of the progress in real time by a specialist.

Currently the eSpasti system, designed in synergy with Ana Belén Cerezuela Jordán, a specialist in physical rehabilitation at the Hospital de la Santa Creu and Sant Pau in Barcelona, is being implemented in one of the hospitals of the Spanish city, as a pilot program.

The first results of the technology assessment indicate a significant reduction in stress for both the patient and the therapist, since it reduces hospital transfers and the number of repetitions of the therapy.

"The eSpasti system is broadly useful at monitoring those affected, allowing physicians and specialists to advise therapy patients from home, who can send the results, either through video or by rating scales," said Martinez Alatriste.

Remote healthcare technology is supported by a web platform to integrate the contents of the Graphic User Interface. According to the specialist at UAM, this factor allows the patient and the specialist to contribute in customizing therapy, since the rehabilitation physician and physiotherapist give indications to both the person with spasticity and the caregiver, regarding which steps to follow for each session. A new combination of materials can efficiently guide electricity and light along the same tiny wire, a finding that could be a step towards building computer chips capable of transporting digital information at the speed of light.

Reporting today in The Optical Society's (OSA) journal Optica, optical and material scientists at the University of Rochester and Swiss Federal Institute of Technology in Zurich describe a basic model circuit consisting of a silver nanowire and a single-layer flake of molybendum disulfide (MoS2).

Using a laser to excite electromagnetic waves called plasmons at the surface of the wire, the researchers found that the MoS2 flake at the far end of the wire generated strong light emission. Going in the other direction, as the excited electrons relaxed, they were collected by the wire and converted back into plasmons, which emitted light of the same wavelength.

"We have found that there is pronounced nanoscale light-matter interaction between plasmons and atomically thin material that can be exploited for nanophotonic integrated circuits," said Nick Vamivakas, assistant professor of quantum optics and quantum physics at the University of Rochester and senior author of the paper.

Typically about a third of the remaining energy would be lost for every few microns (millionths of a meter) the plasmons traveled along the wire, explained Kenneth Goodfellow, a graduate student at Rochester's Institute of Optics and lead author of the Optica paper.

"It was surprising to see that enough energy was left after the round-trip," said Goodfellow.

Photonic devices can be much faster than electronic ones, but they are bulkier because devices that focus light cannot be miniaturized nearly as well as electronic circuits, said Goodfellow. The new results hold promise for guiding the transmission of light, and maintaining the intensity of the signal, in very small dimensions.

Ever since the discovery of graphene, a single layer of carbon that can be extracted from graphite with adhesive tape, scientists have been rapidly exploring the world of two-dimensional materials. These materials have unique properties not seen in their bulk form.

Like graphene, MoS2 is made up of layers that are weakly bonded to each other, so they can be easily separated. In bulk MoS2, electrons and photons interact as they would in traditional semiconductors like silicon and gallium arsenide. As MoS2 is reduced to thinner and thinner layers, the transfer of energy between electrons and photons becomes more efficient.

The key to MoS2's desirable photonic properties is in the structure of its energy band gap. As the material's layer count decreases, it transitions from an indirect to direct band gap, which allows electrons to easily move between energy bands by releasing photons. Graphene is inefficient at light emission because it has no band gap.

Combining electronics and photonics on the same integrated circuits could drastically improve the performance and efficiency of mobile technology. The researchers say the next step is to demonstrate the ir primitive circuit with light emitting diodes. Arthrosis is excessive wear of joints beyond the usual age-related degeneration. For this reason, about 150,000 Germans are provided with an artificial knee joint every year. Early diagnosis and corresponding therapies could delay or even help to avoid these operations. Joints, however, degrade slowly over several years before causing pain and prompting the persons affected to see a doctor. In cooperation with the Sana Gelenk- und Rheumazentrum (Center for Degenerative Joint and Rheumatic Diseases), Bad Wildbad, researchers of Karlsruhe Institute of Technology (KIT) are presently working on a system that detects first indications of arthrosis based on changed motion patterns.

If joints do no longer work as usual, humans tend to compensate this by unconsciously adapting their motions. In the case of knee arthrosis, for instance, they shift the weight to the healthy leg. This relieves the worn knee joint, but also delays the pain that would indicate a starting arthrosis. Wear of joints, therefore, often remains undiscovered at the early stage. "Based on a computer-supported gait analysis, we want to develop an early warning system for routine prevention," says Professor Stefan Sell, Head of the Chair for "Sports Orthopedics and Load Analysis" of the Institute of Sports and Sports Science (IfSS) of KIT and Head Physician for Joint Surgery at the Sana Gelenk- und Rheumazentrum Bad Wildbad. "In this way, we can also develop and test gentler motion sequences for patients. At an early stage, even sports might be helpful, provided that it is executed correctly: Whoever wishes to play tennis in spite of a knee arthrosis, should keep running instead of stopping short. It will take another two years, until the early warning system will be sufficiently mature for commercialization."

Presently, IfSS researchers are working on compiling a catalog of human motion patterns. Deviations in execution are described mathematically by the probability of their occurrence. In parallel, the sports scientists also collect motion data of patients who are already suffering from knee arthrosis. "With them we observe common features of motion sequences, which are highly improbable for healthy people," says Andreas Fischer, who manages the project at the BioMotion Center of IfSS. In many cases, the knee angle is limited. In addition, weight is shifted much more slowly to the aching leg in order to reduce the shock when the foot is put down. In the advanced stage, this behavior is clearly visible and accompanied by pain. But already first indications can be described mathematically as a deviation from the normal probability distribution.

For motions to be analyzed mathematically on the computer, the scientists first have to image them digitally. For this purpose, they attach 39 markers to the body of the test person using adhesive tape. "It is important to mark the pivots of the joints as precisely as possible," Fischer says. When the test person moves under infrared light, this light is reflected by the markers and recorded by cameras. On the computer, the joint markers appear as image points, with the help of which the body can be modeled. In addition, the values of two force measurement plates are recorded. If the test person walks across these plates, they measure when and where a foot touches the plate and which forces act between the floor and the test person. Light barriers in front of and behind the force measurement plates measure the average speed.

"Using these values, our calculation models can identify various motion patterns: They can determine whether somebody walks or runs, whether he moves on plane ground or climbs a slope," Fischer says. The system can distinguish persons by their gait alone. However, this only works, if the scientists have "trained" their computer models accordingly. For this purpose, the test persons have to repeat a motion several times, such that the system can identify typical features. "In this way, our system learns how humans normally move. By comparison, deviations indicating a disease can be detected," Fischer explains. Social networks have become a dominant force in society. Family, friends, peers, community leaders and media communicators are all part of people's social networks. Individuals within a network may have different opinions on important issues, but it's their collective actions that determine the path society takes.

To understand the process through which we operate as a group, and to explain why we do what we do, researchers have developed a novel computational model and the corresponding conditions for reaching consensus in a wide range of situations. The findings are published in the August 2014 issue on Signal Processing for Social Networks of the IEEE Journal of Selected Topics in Signal Processing.

"We wanted to provide a new method for studying the exchange of opinions and evidence in networks," said Kamal Premaratne, professor of electrical and computer engineering, at the University of Miami (UM) and principal investigator of the study. "The new model helps us understand the collective behavior of adaptive agents--people, sensors, data bases or abstract entities--by analyzing communication patterns that are characteristic of social networks."

The model addresses some fundamental questions: what is a good way to model opinions and how these opinions are updated, and when is consensus reached.

One key feature of the new model is its capacity to handle the uncertainties associated with soft data (such as opinions of people) in combination with hard data (facts and numbers).

"Human-generated opinions are more nuanced than physical data and require rich models to capture them," said Manohar N. Murthi, associate professor of electrical and computer engineering at UM and co-author of the study. "Our study takes into account the difficulties associated with the unstructured nature of the network," he adds. "By using a new 'belief updating mechanism,' our work establishes the conditions under which agents can reach a consensus, even in the presence of these difficulties."

The agents exchange and revise their beliefs through their interaction with other agents. The interaction is usually local, in the sense that only neighboring agents in the network exchange information, for the purpose of updating one's belief or opinion. The goal is for the group of agents in a network to arrive at a consensus that is somehow 'similar' to the ground truth -- what has been confirmed by the gathering of objective data.

In previous works, consensus achieved by the agents was completely dependent on how agents update their beliefs. In other words, depending on the updating scheme being utilized, one can get different consensus states. The consensus in the current model is more rational or meaningful.

"In our work, the consensus is consistent with a reliable estimate of the ground truth, if it is available," Premaratne said. "This consistency is very important, because it allows us to estimate how credible each agent is."

According to the model, if the consensus opinion is closer to an agent's opinion, then one can say that this agent is more credible. On the other hand, if the consensus opinion is very different from an agent's opinion, then it can be inferred that this agent is less credible.

"The fact that the same strategy can be used even in the absence of a ground truth is of immense importance because, in practice, we often have to determine if an agent is credible or not when we don't have knowledge of the ground truth," Murthi said.

In the future, the researchers would like to expand their model to include the formation of opinion clusters, where each cluster of agents share similar opinions. Clustering can be seen in the emergence of extremism, minority opinion spreading, the appearance of political affiliations, or affinity for a particular product, for example. Despite the revolutionary biotechnological advancements of the last few decades, an ideal anti-cancer treatment -- one that's immediately lethal to cancer cells, harmless to healthy cells, and resistant to cancer's relapse -- is still a dream.

But a concept called "synthetic lethality" holds great promise for researchers. Two genes are considered synthetically lethal when their combined inactivation is lethal to cells, while inhibiting just one of them is not. Synthetic lethality promises to deliver personalized, more effective, and less toxic therapy. If a particular gene is found to be inactive in a tumor, then inhibiting its synthetic lethal partner with a drug is likely to kill only the cancer cells, causing little damage to healthy cells.

While this promising approach has been widely anticipated for almost two decades, its potential could not be realized due to the difficulty experimentally identifying synthetic lethal pairs in cancer. Now new research published in the journal Cell overcomes this fundamental hurdle and presents a novel strategy for identifying synthetic lethal pairs in cancer with the potential to bust cancer cells.

Tel Aviv University researchers have developed a computational data-driven algorithm, which identifies synthetic lethal interactions. In their comprehensive, multidisciplinary study, Dr. Eytan Ruppin of TAU's Blavatnik School of Computer Science and the Sackler School of Medicine and Ms. Livnat Jerby-Arnon of TAU's Blavatnik School of Computer Science worked together with other researchers from TAU, The Beatson Institute for Cancer Research (Cancer Research UK), and the Broad Institute of Harvard and MIT.

Taking cancer personally

Analyzing large sets of genetic and molecular data from clinical cancer samples, the scientists were able to identify a comprehensive set of synthetic lethal pairs that form the core synthetic lethality network of cancer. They have demonstrated for the first time that such a network can be used to successfully predict the response of cancer cells to various treatments and predict a patient's prognosis based on personal genomic information.

"We started this research from a very simple observation: If two genes are synthetically lethal, they are highly unlikely to be inactive together in the same cell," said Dr. Ruppin. "As cancer cells undergo genetic alterations that result in gene inactivation, we were able to identify synthetic lethal interactions by analyzing large sets of cancer genetic profiles. Genes that were found to be inactive in some cancer samples, but were almost never found to be inactive together in the same sample, were identified as synthetically lethal."

The crux of the study, according to Ms. Jerby-Arnon, is the synergy between the computational research and the ensuing experiments, conducted at the Beatson Institute and the Broad Institute, to verify the predictive power of the new algorithm.

A road to new therapies

In addition to their promising role in tailoring personalized cancer treatment, the synthetic lethal pairs discovered may also be used to repurpose drugs, which are currently used to treat other non-cancer disorders, to target specific cancer types. "We applied our pipeline to search for drugs that may be used to treat certain forms of renal cancer. We identified two such drugs, currently used to treat hypertension and cardiac dysrhythmia, that may be quite effective," said Dr. Ruppin. "Experiments in cell lines performed by the Gottlieb lab at the Beatson Institute support these findings, and we are now working on additional validations in mice."

The researchers are hopeful that their study will help boost the experimental detection of synthetic lethality in cancer cells and offer further insight into the unique susceptibilities of these pathological cells. "In this study, we have demonstrated the clinical utility of our framework, showing that it successfully predicts the response of cancer cells to various treatments as well as patient survival," said Ms. Jerby-Arnon. "In the long-run, we hope this research will help improve cancer treatment by tailoring the most effective treatment for a given patient."

The researchers are in the process of forming experimental and clinical international collaborations to test key emerging leads for novel drug targets and drug repurposing. A growing number of household operations can be managed via the Internet. Today's "Smart Home" promises efficient building management. But often the systems are not secure and can only be retrofitted at great expense. Scientists are working on a software product that defends against hacker attacks before they reach the building.

Botnet. A term from the world of computers is gradually tiptoeing its way into the world of building automation. You have to anticipate this kind of attack scenario, according to Dr. Steffen Wendzel of the Fraunhofer Institute for Communications, Information Processing and Ergonomics FKIE in Bonn. The researcher from the "Cyber Defense" department is the expert in hacker methods and, working jointly with Viviane Zwanger and Dr. Michael Meier, meticulously examines them. Attackers infiltrate multiple computers -- "bots" (from the word "robots") -- without their owners' knowledge, weave the computers together into nets, and misuse them for computer attacks.

The researchers studied something that does not yet exist at all today: attacks by Botnets on "Smart Homes" using Internet-linked buildings or building operations. The finding: The threat is absolutely real: Internet-controlled electric roller shutters, HVAC and locking systems could all be used for these kinds of attacks."Our experiments in the laboratory revealed that the typical IT building is not adequately protected against Internet-based attacks. Their network components could be highjacked for use in botnets," Wendzel continues. In the process, the hackers do not have to seek out the PCs as in the past; instead, they look for the components in building automation that link the buildings with the Internet. These are small boxes installed in the buildings that look and work like routers for home computers."However, they are configured quite simply, can only be upgraded with some difficulty, and are loaded with security gaps. The communications protocol that they use is obsolete," explains Wendzel.

Sentinel software switches between Internet and building IT

To ensure that the heating, lighting, and ventilation of buildings can be controlled via the Internet, it is necessary to install special equipment: This involves mini-computers that measure temperature, light or humidity and are incorporated into networks. "Keeping them up to the latest standards is expensive," Wendzel says. At FKIE, the team has developed security software that can easily switch between Internet and building IT. The technology filters out potential attacks from communications protocols even before they reach the four walls of the actual brick-and-mortar home or office building. No matter what technologies are being used within the building: With this approach, they do not have to be replaced.

The researchers additionally examined the conventional communications standards of building automation, and building upon these, they have developed rules for data traffic. If arriving data do not adhere to these rules, then the communications flow is modified. "The software operates like a firewall with normalization components," explains Wendzel. All the results that are sent on their way to the systems are tested for plausibility by an "analyzer." If the alarm goes off, then the incident is immediately dispatched to the "normalizer." This either blocks the incident in its entirety or modifies it accordingly. The basic research has been concluded successfully. "In the next stage, we want to make the technology production-ready with an industrial firm. In no later than two years, there should be a product on the market," states Wendzel.

In their analysis of Botnet attacks, the researchers sketched out definitive threat scenarios for smart homes."From my perspective, the most compelling issue is 'monitoring,'" the cyber defense researcher says. When the attacker hacks into the building operations IT, he or she will learn where the residents or tenants are located and what they are doing, in a worst case scenario. That includes everything, right down to going to the toilet. Intruders, for example, could use this data in order to prepare for a burglary or raid. In this case, the hacker is acting in a passive capacity, simply tapping data. However, he or she could be equally capable of actively invading the systems. Take a contractor from the energy industry, for example. He could profit from more oil or gas sold if the consumption of multiple heating systems is artificially elevated. A recent example demonstrates how real this scenario is: Last year, there was a gap in the security system of a heating system connected to the Internet. Attackers had the ability to shut down or damage heaters. Therefore, security expert Wendzel is currently advising against carelessly linking all building functions in private homes to the Internet. A team of University of Maryland physicists has published new nanoscience advances that they and other scientists say make possible new nanostructures and nanotechnologies with huge potential applications ranging from clean energy and quantum computing advances to new sensor development.

Published in the September 2, issue of Nature Communications the Maryland scientists' primary discovery is a fundamentally new synthesis strategy for hybrid nanostructures that uses a connector, or "intermedium," nanoparticle to join multiple different nanoparticles into nanostructures that would be very difficult or perhaps even impossible to make with existing methods. The resultant mix and match modular component approach avoids the limitations in material choice and nanostructure size, shape and symmetry that are inherent in the crystalline growth (epitaxial) synthesis approaches currently used to build nanostructures.

"Our approach makes it possible to design and build higher order [more complex and materially varied] nanostructures with a specifically designed symmetry or shape, akin to the body's ability to make different protein oligomers each with a specific function determined by its specific composition and shape," says team leader Min Ouyang, an associate professor in the department of physics and the Maryland NanoCenter.

"Such a synthesis method is the dream of many scientists in our field and we expect researchers now will use our approach to fabricate a full class of new nanoscale hybrid structures," he says.

Among the scientists excited about this new method is the University of Delaware's Matt Doty, an associate professor of materials science and engineering, physics, and electrical and computer engineering and associate director of the UD Nanofabrication Facility. "The work of Weng and coauthors provides a powerful new tool for the 'quantum engineering' of complex nanostructures designed to implement novel electronic and optoelectronic functions. [Their] new approach makes it feasible for researchers to realize much more sophisticated nanostructure designs than were previously possible." he says.

Lighting a Way to Efficient Clean Power Generation

The team's second discovery may allow full realization of a light-generated nanoparticle effect first used by ancient Romans to create glass that changes color based on light.. This effect, known as surface plasmon resonance, involves the generation of high energy electrons using light.

More accurately explains Ouyang, plasmon resonance is the generation of a collective oscillation of low energy electrons by light. And the light energy stored in such a "plasmonic oscillator" then can be converted to energetic carriers (i.e., "hot" electrons)" for use in various applications.

In recent years, many scientists have been trying to apply this effect to the creation of more efficient photocatalysts for use in the production of clean energy. Photocatalysts are substances that use light to boost chemical reactions. Chlorophyll is a natural photocatalyst used by plants.

"The ingenious nano-assemblies that Professor Ouyang and his collaborators have fabricated, which include the novel feature of a silver-gold particle that super-efficiently harvests light, bring us a giant step nearer the so-far elusive goal of artificial photosynthesis: using sunlight to transform water and carbon dioxide into fuels and valuable chemicals," says Professor Martin Moskovits of the University of California at Santa Barbara, a widely recognized expert in this area of research and not affiliated with the paper.

Indeed, using sunlight to split water molecules into hydrogen and oxygen to produce hydrogen fuel has long been a clean energy "holy grail." However, decades of research advances have not yielded photocatalytic methods with sufficient energy efficiency to be cost effective for use in large scale water splitting applications.

"Using our new modular synthesis strategy, our UMD team created an optimally designed, plasmon-mediated photocatalytic nanostructure that is an almost 15 times more efficient than conventional photocatalysts," says Ouyang.

In studying this new photocatalyst, the scientists identified a previously unknown "hot plasmon electron-driven photocatalysis mechanism with an identified electron transfer pathway."

It is this new mechanism that makes possible the high efficiency of the UMD team's new photocatalyst. And it is a finding made possible by the precise materials control allowed by the team's new general synthesis method.

Their findings hold great promise for future advances that could make water splitting cost effective for large-scale use in creating hydrogen fuel. Such a system would allow light energy from large solar energy farms to be stored as chemical energy in the form of clean hydrogen fuel. And the UMD team's newly-discovered mechanism for creating hot (high energy) electrons should also be applicable to research involving other photo-excitation processes, according to Ouyang and his colleagues. Ion channels are involved in many physiological and pathophysiological processes throughout the human body. A young team of researchers led by pharmacologist Anna Stary-Weinzinger from the Department of Pharmacology and Toxicology, University of Vienna investigated how ion flux through a voltage gated sodium ion channel works in detail. Since this process is incredibly fast (up to 100 million ions per seconds), computer simulations were performed to visualize sodium flux "in slow motion."

The time consuming calculations were performed using the high performance computer cluster (VSC), which is currently the fastest computer in Austria. Recently, the results were published in PLOS Computational Biology.

Electrical signals, generated by voltage gated ion channels are essential for survival. Without these proteins, fundamental body functions such as heart beat, signal transduction in our brain, or muscle contraction would not be possible. A remarkable feature of these extraordinary proteins is that they enable extremely rapid and selective ion flux. X-ray structures provided tremendous insights into the structure of these proteins. Guided by this structural information, investigating the details of ion selectivity and conductance becomes feasible. Crystal structures revealed a short, water filled "ion filter" structure at the extracellular side of the protein, surrounded by four negatively charged glutamic acid side chains, which enables selective sodium flux. However, from these "static" structures it is not easy to deduce how the dynamic process of ion flux works in detail. Thus, computer simulations are a great means to provide mechanistic insights into this process.

Computer simulations visualize ion movements

To watch these fascinating proteins at work, computer simulations, so called molecular dynamics simulations were performed. With the help of the fastest high performance computer in Austria, the Vienna Scientific Cluster (VSC), detailed insights into the mechanism of sodium flux became possible. A team of researchers from the Department of Pharmacology and Toxicology in Vienna discovered that ion influx from the extracellular environment is much faster compared to ion efflux. "The reason behind this remarkable difference lies in the fact that a key amino acid, glutamic acid 53 undergoes a rotational movement ('flip'), thereby modulating ion flux," explains doctoral student Song Ke.

Glutamic acid (E53) regulates channel flux

Molecular dynamics simulations reveal that one specific amino acid, E53, has two distinct configurations depending on the ion flux directions. "E53 allows a large amount of sodium influx, when it is in the non-flipped, outward-facing state. Free energy calculations revealed that efflux is much harder for the ions, due to a 'barrier', which slows ion movement. Thus, to help ions 'overcome' this barrier and to enable efflux of sodium ions, the E53 side chain flips to an inward-facing conformation," explains Song Ke, PhD student from the University of Vienna. Further, we consider it likely that the flipped glutamic acid could play a substantial role in triggering channel inactivation, a  key mechanism to allow repolarization of the membrane. How do you prevent an earthquake from destroying expensive computer systems?

That's the question earthquake engineer Claudia Marin-Artieda, PhD, associate professor of civil engineering at Howard University, aims to answer through a series of experiments conducted at the University at Buffalo.

"The loss of functionality of essential equipment and components can have a disastrous impact. We can limit these sorts of equipment losses by improving their seismic performance," Marin-Artieda said.

In buildings such as data centers, power plants and hospitals, it could be catastrophic to have highly-sensitive equipment swinging, rocking, falling and generally bashing into things.

In high-seismic regions, new facilities often are engineered with passive protective systems that provide overall seismic protection. But often, existing facilities are conventional fixed-base buildings in which seismic demands on sensitive equipment located within are significantly amplified. In such buildings, sensitive equipment needs to be secured from these damaging earthquake effects, Marin-Artieda said. CAPTCHA services that require users to recognize and type in static distorted characters may be a method of the past, according to studies published by researchers at the University of Alabama at Birmingham. Researchers in Bangladesh have designed a computer programme that can accurately recognise users' emotional states as much as 87% of the time, depending on the emotion.

Writing in the journal Behaviour & Information Technology, A.F.M. Nazmul Haque Nahin and his colleagues describe how their study combined -- for the first time -- two established ways of detecting user emotions: keystroke dynamics and text-pattern analysis.

To provide data for the study, volunteers were asked to note their emotional state after typing passages of fixed text, as well as at regular intervals during their regular ('free text') computer use; this provided the researchers with data about keystroke attributes associated with seven emotional states (joy, fear, anger, sadness, disgust, shame and guilt). To help them analyse sample texts, the researchers made use of a standard database of words and sentences associated with the same seven emotional states.

After running a variety of tests, the researchers found that their new 'combined' results were better than their separate results; what's more, the 'combined' approach improved performance for five of the seven categories of emotion. Joy (87%) and anger (81%) had the highest rates of accuracy.

This research is an important contribution to 'affective computing', a growing field dedicated to 'detecting user emotion in a particular moment'. As the authors note, for all the advances in computing power, performance and size in recent years, a lot more can still be done in terms of their interactions with end users. "Emotionally aware systems can be a step ahead in this regard," they write.

"Computer systems that can detect user emotion can do a lot better than the present systems in gaming, online teaching, text processing, video and image processing, user authentication and so many other areas where user emotional state is crucial."

While much work remains to be done, this research is an important step in making 'emotionally intelligent' systems that recognise users' emotional states to adapt their music, graphics, content or approach to learning a reality. 

Imagine, he says, that you and your co-workers plan via social media to head for lunch about 12:30 p.m. most Thursdays. Usually that Italian place downtown. Frequently tweet about traffic on the way.

Now imagine that at 10 a.m., you're tweeted a coupon from the Chinese place near the Italian joint -- and directions around a traffic jam that will start in about 90 minutes. Score one Sichuan hot pot.

Yoon can make that happen. He and fellow Binghamton University systems scientist Sarah Lam have been working with Binghamton alumnus Nathan Gnanasambandam, a senior researcher at the Palo Alto Research Center (PARC), a division of Xerox Research. They used 500 million tweets to develop algorithms that not only paint a picture of everyday human dynamics, but can predict an individual's behavior hours in advance. The team, which also included graduate students Keith Thompson and Bichen Zheng, recently published their findings in Industrial Engineer.

Think about what your typical social media post says about you: when you posted, where you were. Your networking relationships can be learned -- and with context-based algorithms like those PARC and Binghamton University have developed -- what you plan. They use what is called an artificial neural network.

How sure are they? Better than 90 percent for a typical social media user in a three-hour horizon. "If you look at the picture, it's very static. But the individuals are all over the place," Yoon says.

Some people are very careful about what data they give out, but the algorithms can work pretty well with anonymized data. Usable predictions can be made more than 60 percent of the time, if the right data are aggregated. And that data isn't just coming from social media: Think about sources such as credit card transactions, monitored telephone calls, e-mail, GPS data.

Creepy, perhaps, but this type of analysis also has benefits. Xerox, which has funded and participated in the team's ongoing research, can apply the tools to traffic. (It helps run the New York State Thruway's EZ-Pass system and parking services in several cities across the country.) Imagine getting directions during an emergency that not only get you out of harm's way, but get you to someplace personal where you're safe, reducing the burden on emergency shelters. Or imagine directions that prevent a traffic jam, rather than simply route you around one.

Now apply that research tool to call and contact centers, which Xerox also runs. These methods can fuse data from call centers, online chat and e-mail help desks. "We give it structure -- not all feeds have structure," says Gnanasambandam, who is also a visiting professor in Binghamton's department of systems science and industrial engineering.

"What if you call a company..." Yoon says, and Lam completes: ." .. And they know why you're calling before you call?"

Help desk associates can be cross-trained in topics so they face less downtime, or calls could be routed faster to the best specialist. Data about problems can be analyzed in near-real time, perhaps allowing fixes to be made before the customer realizes there's a problem. "That's not too far away from what's happening," Gnanasambandam says.

Now direct this approach toward healthcare -- which provides about $2 billion of Xerox's annual business -- and researchers can build tools to help patients, doctors, hospitals, insurers and pharmaceutical companies better understand the complexities of public health or ferret out prescription or Medicaid fraud.

"There's a lot of different directions you can go," Lam says.

Including to Yoon's next Chinese meal. Smartphones have seen wide adoption among Americans in recent years because of their ease of use and adaptability. With that in mind, researchers from Arizona State University examined how smartphone use affected weight loss goals and determined that smartphones may offer users an advantage over traditional methods when tracking diet data.

Roughly 83% of Americans now own a mobile phone and 45% own smartphones with Internet access. For this study, researchers recruited healthy, weight-stable adults and semirandomly divided them into groups based on their diet-tracking method. The groups consisted of those who used the "Lose It!" app, those who recorded dietary intake using the memo function of their smartphone, and those who used traditional paper and pencil to record their diet. Although smartphone use did not affect total weight loss among the 47 participants who completed the study, the researchers observed better diet tracking results among those in the smartphone-use groups.

"Participants using a commercially available app more consistently entered complete days of dietary data compared with the paper-and-pencil group and also withdrew from the study less often than the other groups," lead author Christopher Wharton, PhD, said. "It's possible that app technology offers a less burdensome method for tracking data compared with traditional tools."

The memo and paper-and-pencil groups reported twice the number of missing days as the group using the app, but diet quality was not improved among app users. Therefore, it was concluded that food and nutrition professionals should consider using app technology in conjunction with dietary counseling for weight management. Because little data about smartphone use as it relates to weight management and dieting is available, this study should help inform further research in this area.
Many people are unaware that the U.S. Food and Drug Administration's mandated nutrition labels are based on a 2,000-calorie-a-day diet, but a simple weekly text message reminder can greatly improve that awareness, according to a new study from the Johns Hopkins Bloomberg School of Public Health.

While not an outright recommendation, the 2,000-calorie benchmark is what the FDA considers a reasonable daily calorie intake for many adults. More importantly, nutrition labels on food products sold in the U.S. are based on it.

The key to translating nutrition labels and using them to make healthy food choices, researchers say, may be an understanding of this basic fact.

The study, published online in Health Promotion Practice, surveyed 246 participants dining in the Johns Hopkins Hospital cafeteria to assess their initial knowledge of the 2,000-calorie value. The cafeteria included calorie labels for food choices but no information on the daily context.

Participants were then randomly assigned to receive either a weekly text message reminder, a weekly email reminder, or no weekly reminder about the 2,000-calorie value. Participants received the reminder messages each Monday for four weeks; after the four weeks, their knowledge of the 2,000-calorie value was assessed with a follow-up survey.

Prior to receiving the weekly reminders, 58 percent of participants could not correctly identify the 2,000-calorie value, even those with college or graduate degrees. After the study period, those receiving the weekly text messages were twice as likely to correctly identify the 2,000-calorie value as compared to those who received no weekly reminder.

"While daily energy needs vary, the 2,000-calorie value provides a general frame of reference that can make menu and product nutrition labels more meaningful," says study leader Lawrence J. Cheskin, MD, director of the Johns Hopkins Weight Management Center at the Bloomberg School of Public Health. "When people know their calorie 'budget' for the day, they have context for making healthier meal and snack choices."

The FDA has proposed new menu-labeling regulations, which will soon require chain restaurants with 20 or more outlets to list calories on menus, menu boards, and drive-through displays. Cheskin says that those calorie counts are not helpful tools for making good food choices if people don't understand roughly how many calories they should consume each day.

"Given the low level of calorie literacy, simply posting calorie counts on menu boards is not sufficient," Cheskin says.

The weekly text and email reminders were based on The Monday Campaigns' model for health communications, which leverages the idea that Monday provides a weekly opportunity to start fresh and commit to new healthy habits, such as exercise regimens, healthy eating plans or smoking cessation. The Monday Campaigns is a nonprofit organization that started in 2003 with research support from the Johns Hopkins Bloomberg School of Public Health.

"There are many simple ways to convey calorie information to consumers, including point of sale communication, text messages, emails and even smart phone apps," Cheskin notes. "Ideally, these could work together, with calories posted on menus, restaurant signage and food labels along with personal reminders delivered through the latest technology. Our data indicate that weekly text messages are one element in this mix that can be effective." Researchers at Ben-Gurion University of the Negev have developed a new program that automates classification of personality traits of prominent individuals -- both friend and foe -- according to a paper soon to be published in the American Intelligence Journal.

"This new field, termed 'Computational Personality,' gives us the ability to better understand the minds of military and political leaders, which is an important aspect of strategic intelligence," explains BGU Prof. Yair Neuman of the Homeland Security Institute. "Psychologists have been building personality profiles manually for years; however, there are serious methodological difficulties associated with this practice."

The new computer-supported methodology for personality profiling uses "vector semantics." This involves constructing a number of vectors representing personality dimensions and disorders and measuring the similarity with texts written by the human subject. BGU researchers Prof. Golan Shahar of the Department of Psychology and programmer Yochai Cohen also participated in the research.

The team used the new program to evaluate President Obama's State of the Union addresses from 2009 and his most recent in 2014. According to Neuman, "Both State of the Union speeches are 'assertive' and 'organized' as expected from a political leader," Neuman explains. "However, the main difference in the 2014 speech is the 'loner' personality trait that appears. This dimension reveals a type of withdrawal from painful social interaction. In addition, the 2014 speech exhibits higher levels of 'anger' and 'fear'."

Evaluating the leader of Hamas, Khaled Mashal, "If we characterize Mashal as someone with a psychopathic personality, then we would expect him to feel omnipotent, fearless, to perceive others (particularly Israel) as weak and vulnerable, and that his relationships revolve around games of 'predator-prey,''' Neuman explains. "According to the computational personality results, a man like that won't be significantly affected by injury to innocent citizens or the destruction of infrastructure because he lacks the ability to empathize. For his adversaries, any attempt to simulate empathy, or to try and appeal to his emotions is a strategy doomed to fail. These insights are highly important in understanding the personality and planning a campaign against it," says Neuman.

"The CIA personality profile of former Iraqi dictator Saddam Hussein was actually the Middle Eastern version of "showing off," not irrational behavior as the Bush administration inaccurately thought," according to Neuman. "So long as the leader is not defeated or publicly humiliated, even if it results in a catastrophic price to his fighters and citizens, it will be perceived as a victory. This conclusion offers a clear lesson for the current struggle against Hamas, as well."

While the methodology can be applied to any leader with available texts and speeches, the actual paper was a case study on former Egyptian President Mohammed Morisi's speech to the United Nations in 2012. The takeaway from that analysis was simply that Morisi is an "obsessive" personality who was out of touch with the Egyptian people and did not see the big picture. If you want to make a mark in the world of computer games you had better have a good English vocabulary. It has now also been scientifically proven that someone who is good at computer games has a larger English vocabulary. This is revealed by a study at the University of Gothenburg and Karlstad University.

The study confirms what many parents and teachers already suspected: young people who play a lot of interactive English computer games gain an advantage in terms of their English vocabulary compared with those who do not play or only play a little.

The study involved 76 young people aged 10-11. Data was collected via questionnaires and a so-called language diary. This was used to list all encounters with the English language outside school, such as using the computer and playing digital games. Among other things, the study investigated whether there was any correlation between playing digital games and motivation to learn English, self-assessed English linguistic ability and strategies used to speak English.

Major difference between the genders

The results indicate that there is a major difference between the genders when it comes to computer gaming. Boys spend an average of 11.5 hours a week playing, while girls spent less than half that time, 5.1 hours. Girls instead spent far more time (11.5 hours) than boys (8 hours) on language-related activities online, primarily on Facebook.

The computer games that appear to be most effective for the development of English vocabulary are those known as Massively multiplayer online role-playing games (MMORPG), a genre of role-playing computer games in which a large number of players interact with one another in a virtual world.

"As a player you simply have to be able to understand what's being said, to read English and to interact yourself by both writing and speaking English," says Liss Kerstin Sylvén, Associate Professor at the University of Gothenburg, who conducted the study together with Pia Sundqvist, Senior Lecturer in English at Karlstad University.

English outside school important

The results from the study underline the results from other studies conducted by both researchers. Regular gamers have a significantly better English vocabulary than others.

"The importance of coming into contact with English outside school, for example by reading English or, as in this case, by playing computer games, means a lot in terms of young people's English vocabulary. It also has positive effects on what happens at school in the classroom. The subject of English at school and the English that the young people encounter and use in their leisure time are not two separate worlds," says Liss Kerstin. The future of movie, sports and concert broadcasting lies in 4K definition, which will bring cinema quality TV viewing into people's homes. 4K Ultra HD has four times as many pixels as today's Full HD. And thanks to the new HEVC video compression standard, broadcasters can now transmit live video in the 4K digital cinema standard. From September 12 to 16, 2014, Fraunhofer researchers will be presenting latest hardware and software for high efficiency video encoding and decoding at the International Broadcasting Convention IBC in Amsterdam, Netherlands.

A few months ago the broadcaster Sky achieved a world first by transmitting a Bundesliga scoccer game live in 4K definition -- the current digital cinema standard. With its 3840 x 2160 resolution, 4K provides four times as many pixels as today's Full HD TV resolution. Higher resolutions inevitably creates more than four times the data volumes of standard HD, so the broadcaster used the new HEVC video compression standard to transmit the huge amount of data. HEVC, short for High Efficiency Video Coding, was developed by major electronics manufacturers together with researchers from the Fraunhofer Institute for Telecommunications, Heinrich-Hertz-Institut, HHI. The key advantage of HEVC is that it requires only half the bitrate of its predecessor H.264 to provide the same picture quality -- allowing it to transmit twice as much data on the same channel. This means that HEVC is practically made for the ultra-high definition of 4K television.

Live coding of 4K images with HEVC

The video compression technique was used for the game between FC Bayern Munich and SV Werder Bremen held in the Allianz Arena on April 26, 2014. The live coding of the 4K camera images from the stadium enabled the successful application of the technique developed by the HHI researchers together with their colleagues at Rohde & Schwarz. The HHI´s software and hardware solutions convert the camera images into a coded HEVC bitstream, which is then transmitted to televisions via satellite. Benjamin Bross, who heads up the HEVC project at the HHI, explains why this process is so special: "For the first time, we were able to encode 4K recordings live."

This type of real-time encoding throws up numerous challenges. While the previous standard, H.264, divides the image to be transmitted into blocks of 16 x 16 pixels, HEVC subdivides using variable block sizes. The encoder must therefore decide which block sizes make the most sense on an image-by-image basis. The HHI researchers will be demonstrating the complete HEVC processing chain at the IBC trade fair.

Right down to the smallest detail…

So far, so good. But how will viewers benefit from being able to watch 4K content with HEVC coding in their living rooms? The biggest plus is undoubtedly the astonishing level of detail in the razor-sharp picture. In a scoccer match, for example, one camera can capture the entire playing field. That gives viewers at home a strategic overview while still allowing them to make out every last detail. The first 4K televisions are already on the market, and web streaming services are starting to offer content in ultra HD resolution -- so the 4K evening news is sure to be coming soon! Children's social skills may be declining as they have less time for face-to-face interaction due to their increased use of digital media, according to a UCLA psychology study.

UCLA scientists found that sixth-graders who went five days without even glancing at a smartphone, television or other digital screen did substantially better at reading human emotions than sixth-graders from the same school who continued to spend hours each day looking at their electronic devices.

"Many people are looking at the benefits of digital media in education, and not many are looking at the costs," said Patricia Greenfield, a distinguished professor of psychology in the UCLA College and senior author of the study. "Decreased sensitivity to emotional cues -- losing the ability to understand the emotions of other people -- is one of the costs. The displacement of in-person social interaction by screen interaction seems to be reducing social skills."

The research will be in the October print edition of Computers in Human Behavior and is already published online.

The psychologists studied two sets of sixth-graders from a Southern California public school: 51 who lived together for five days at the Pali Institute, a nature and science camp about 70 miles east of Los Angeles, and 54 others from the same school. (The group of 54 would attend the camp later, after the study was conducted.)

The camp doesn't allow students to use electronic devices -- a policy that many students found to be challenging for the first couple of days. Most adapted quickly, however, according to camp counselors.

At the beginning and end of the study, both groups of students were evaluated for their ability to recognize other people's emotions in photos and videos. The students were shown 48 pictures of faces that were happy, sad, angry or scared, and asked to identify their feelings.

They also watched videos of actors interacting with one another and were instructed to describe the characters' emotions. In one scene, students take a test and submit it to their teacher; one of the students is confident and excited, the other is anxious. In another scene, one student is saddened after being excluded from a conversation.

The children who had been at the camp improved significantly over the five days in their ability to read facial emotions and other nonverbal cues to emotion, compared with the students who continued to use their media devices.

Researchers tracked how many errors the students made when attempting to identify the emotions in the photos and videos. When analyzing the photos, for example, those at the camp made an average of 9.41 errors at the end of the study, down from 14.02 at the beginning. The students who didn't attend the camp recorded a significantly smaller change. For the videos, the students who went to camp improved significantly, while the scores of the students who did not attend camp showed no change. The findings applied equally to both boys and girls.

You can't learn nonverbal emotional cues from a screen in the way you can learn it from face-to-face communication," said lead author Yalda Uhls, a senior researcher with the UCLA's Children's Digital Media Center, Los Angeles. "If you're not practicing face-to-face communication, you could be losing important social skills."

Students participating in the study reported that they text, watch television and play video games for an average of four-and-a-half hours on a typical school day. Some surveys have found that the figure is even higher nationally, said Uhls, who also is the Southern California regional director of Common Sense Media, a national nonprofit organization.

Greenfield, director of the CDMC, considers the results significant, given that they occurred after only five days.

She said the implications of the research are that people need more face-to-face interaction, and that even when people use digital media for social interaction, they're spending less time developing social skills and learning to read nonverbal cues.

"We've shown a model of what more face-to-face interaction can do," Greenfield said. "Social interaction is needed to develop skills in understanding the emotions of other people."

Uhls said that emoticons are a poor substitute for face-to-face communication: "We are social creatures. We need device-free time." Leaf measurements are often critical in plant physiological and ecological studies, but traditional methods have been time consuming and sometimes destructive to plant samples. Researchers at the University of California, Davis, have developed Easy Leaf Area -- a free software written in an open-source programming language -- to allow users to accurately measure leaf area from digital images in seconds.

"It has always been a challenge to measure leaf surface area without damaging the plants or spending long hours in the lab, so I decided to attempt to write software to automatically measure leaf and scale area from smartphone images," explains Hsien Ming Easlon, a researcher at UC Davis and one of the developers of Easy Leaf Area. "Leaf area measurements are essential for estimating crop yields, water usage, nutrient absorption, plant competition, and many other aspects of growth."

The digital images he uses are taken with the Apple IPhone 4, but any current smartphone camera or digital camera will do. Once the images are uploaded to a computer, Easy Leaf Area can process hundreds of images and save the results to a spreadsheet-ready CSV file. The Windows executable software is free to download and can be modified to suit specific experimental requirements. A full report including links to additional resources is available in a recent issue of Applications in Plant Sciences.

Easlon recalls, "Our lab started using digital cameras when I was a graduate student. We figured out how to use Photoshop to measure areas in digital images, but this method still required one to five minutes of human input per image."

Five minutes per image may not seem like a long time, but multiply that by hundreds of plants -- a normal sample size -- and those minutes add up fast. By automating data analysis, researchers can save countless hours of manual labor, improve the accuracy and consistency of their results, and reduce potential damages to their plant samples.

Easlon and his team developed Easy Leaf Area using Arabidopsis plants, and also tested Easy Leaf Area on photographs of field-grown tomatoes and wheat, and photographs and scans of detached leaves of a common tree poppy, California redwood, chaparral currant, Jeffrey pine, and Valley oak. Manual adjustments to the automatic algorithm can be saved for different plants and field conditions, making this a practical tool for researchers in many plant science fields.

Easlon's next step is to develop a mobile version so that leaf area measurements can be made on the fly without a PC. He also plans to add handwriting recognition or barcode reading to the software. This will automatically interpret labeled plant stakes and assign the proper file names to each image.

"Most researchers don't have the time or knowledge to develop software for themselves, so scientific use of smartphones is primarily limited to built-in features. The processing power, connectivity, built-in sensors, storage capacity, and low price give smartphones great potential to replace many single-purpose devices for scientific data collection," explains Easlon.

Calculating plant surface area could soon be as easy as using Instagram.  A new graphics system that can easily produce complex 3-D shapes from simple professional sketches will be unveiled by University of British Columbia computer scientists at the SIGGRAPH 2014 Conference in Vancouver, Canada this week.

The technology has the potential to dramatically simplify how designers and artists develop new product ideas.

Converting an idea into a 3-D model using current commercial tools can be a complicated and painstaking process. UBC researchers developed True2Form, a software algorithm inspired by the work of professional designers, effectively communicating ideas through simple drawings.

"In line-drawings, designers and artists use descriptive curves and informative viewpoints to convey the full shape of an object," says Alla Sheffer, a professor in UBC's Dept. of Computer Science. "Our system mimics the results of human three-dimensional shape inference to lift a sketch curve network into 3-D, while preserving fidelity to the original sketch."

True2Form uses powerful mathematics to interpret artists' strokes automatically lifting drawings off of the page. It produces convincing, complex 3-D shapes computed from individual sketches, automatically corrected to account for inherent drawing inaccuracy.

The software is designed to render a wider range of geometric complexity than current sketch-based modelling frameworks.

Sheffer, her team from UBC, and colleagues from the University of Toronto and INRIA France will present a technical paper on True2Form on Wednesday, August 13 at the Vancouver Convention Centre as part of SIGGRAPH 2014. Researchers have found a link between digital literacy and a reduction in cognitive decline, according to a study published in The Journals of Gerontology, Series A: Medical Sciences on July 8th.

Led by Andre Junqueira Xavier at the Universidade do Sul de Santa Catarina, this is the first major study to show that digital literacy, or the ability to engage, plan and execute digital actions such as web browsing and exchanging emails, can improve memory. Drawn from the English Longitudinal Study of Ageing, the study followed 6442 participants in the UK between the ages of 50 and 89 for 8 years.

The data measures delayed recall from a 10-word-list learning task across 5 separate measurement points. Higher wealth, education and digital literacy improved delayed recall, while people with functional impairment, diabetes, cardiovascular diseases, depressive symptoms or no digital literacy showed decline. The researchers' findings suggest that "digital literacy increases brain and cognitive reserve or leads to the employment of more efficient cognitive networks to delay cognitive decline."

The authors write, "countries where policy interventions regarding improvement in DL are implemented may expect lower incidence rates for dementia over the coming decades." The smart phone has changed our behavior, sometimes for the better as we are now able to connect and engage with many more people than ever before, sometimes for the worse in that we may have become over-reliant on the connectivity with the outside world that these devices afford us. Either way, there is no going back for the majority of users who can almost instantaneously connect with hundreds if not thousands of people through the various social media and other applications available on such devices and not least through the humble phone call.

However, our dependence brings anxiety. The loss of one's smart phone not only represents an immediate disconnection from one's online contacts but is also a potential privacy and security risk should the lost phone wend its way into the hands of a malicious third party. Writing in the International Journal of Mobile Communications, a Canadian team outlines the possible coping mechanisms that might be needed following loss or theft and the security problems that the user might face. The researchers point out that the same anxieties apply equally to lost or stolen laptops, tablet computers and other digital devices.

Zhiling Tu, Yufei Yuan and Norm Archer of McMaster University in Hamilton, Ontario, explain that the convenience of mobility, wireless communication and the information processing power of smart phones and other portable digital devices has led to more and more people carrying with them valuable data assets wherever they go. These assets may include personal and business contacts, private pictures and videos, meeting and lecture notes and the like, banking details, utility statements, company spreadsheets and much more. All such assets are potentially sensitive to abuse by third parties.

The researchers add that as many companies now have a BYOD (bring-your-own-device) policy rather than dispensing a standard corporate device to all employees there are additional security issues that arise from their being centralized control of the data on a given device. The value of lost hardware might be negligible when compared to the loss of sensitive or proprietary data. Perhaps more troubling is that while there are various countermeasures that can be used to cope with mobile device loss and theft, users are either unaware of their existence or unwilling to use them. The cost and convenience of security countermeasures also need to be weighed up.

The team has investigated how general mobile phone users might not cope with the threat of losing their device. They found that a few active and security-conscious users were aware of countermeasures but many users were either not aware of "time bomb" data deletion settings and remote device locks and such or were simply in denial of the risk of their losing their phone. Their findings suggest that an awareness campaign might be needed to encourage general users to make their devices more secure and that organizations must enforce certain features on their employees and members to protect sensitive data that might be on those devices beyond their direct control. A team of researchers has discovered a way to cool electrons to -228 °C without external means and at room temperature, an advancement that could enable electronic devices to function with very little energy.

A chip, which contains nanoscale structures that enable electron cooling at room temperature, is pictured.

The process involves passing electrons through a quantum well to cool them and keep them from heating.

The team details its research in "Energy-filtered cold electron transport at room temperature," which is published in Nature Communications on Wednesday, Sept. 10.

"We are the first to effectively cool electrons at room temperature. Researchers have done electron cooling before, but only when the entire device is immersed into an extremely cold cooling bath," said Seong Jin Koh, an associate professor at UT Arlington in the Materials Science & Engineering Department, who has led the research. "Obtaining cold electrons at room temperature has enormous technical benefits. For example, the requirement of using liquid helium or liquid nitrogen for cooling electrons in various electron systems can be lifted."

Electrons are thermally excited even at room temperature, which is a natural phenomenon. If that electron excitation could be suppressed, then the temperature of those electrons could be effectively lowered without external cooling, Koh said.

The team used a nanoscale structure -- which consists of a sequential array of a source electrode, a quantum well, a tunneling barrier, a quantum dot, another tunneling barrier, and a drain electrode -- to suppress electron excitation and to make electrons cold.

Cold electrons promise a new type of transistor that can operate at extremely low-energy consumption. "Implementing our findings to fabricating energy-efficient transistors is currently under way," Koh added.

Khosrow Behbehani, dean of the UT Arlington College of Engineering, said this research is representative of the University's role in fostering innovations that benefit the society, such as creating energy-efficient green technologies for current and future generations.

"Dr. Koh and his research team are developing real-world solutions to a critical global challenge of utilizing the energy efficiently and developing energy-efficient electronic technology that will benefit us all every day," Behbehani said. "We applaud Dr. Koh for the results of this research and look forward to future innovations he will lead."

Usha Varshney, program director in the National Science Foundation's Directorate for Engineering, which funded the research, said the research findings could be vast.

"When implemented in transistors, these research findings could potentially reduce energy consumption of electronic devices by more than 10 times compared to the present technology," Varshney said. "Personal electronic devices such as smart phones, iPads, etc., can last much longer before recharging."

In addition to potential commercial applications, there are many military uses for the technology. Batteries weigh a lot, and less power consumption means reducing the battery weight of electronic equipment that soldiers are carrying, which will enhance their combat capability. Other potential military applications include electronics for remote sensors, unmanned aerial vehicles and high-capacity computing in remote operations.

Future research could include identifying key elements that will allow electrons to be cooled even further. The most important challenge of this future research is to keep the electron from gaining energy as it travels across device components. This would require research into how energy-gaining pathways could be effectively blocked.

Co-authors of the paper are Pradeep Bhadrachalam, Ramkumar Subramanian, Vishva Ray and Liang-Chieh Ma from UT Arlington, and Weichao Wang, Prof. Jiyoung Kim and Prof. Kyeongjae Cho from UT Dallas who also were part of the research team. If you want to learn more about the people in urban communities -- from their health habits to what their neighborhood needs -- save a stamp on mailing a survey. Just text them.

A new pilot study among low-income African-Americans in Detroit suggests that there is a clear preference on how residents choose to communicate -- whether it's by researchers asking questions for a health study or community advocates gauging resource needs. They want you to talk to them through their phones.

"Our study shows great potential to connect with a population that's traditionally difficult to reach. Texting is a simple technology that is already being used for everyday communication- it is something people from all backgrounds are very comfortable with," says lead author Tammy Chang, M.D., MPH, MS, an assistant professor in the department of family medicine at the University of Michigan Medical School and member of the Institute for Healthcare Policy and Innovation.

"This is a group whose attitudes and perceptions are incredibly important to understand, but who may not necessarily be taking online surveys or attending community meetings. We found that texting is not only acceptable and feasible but is the preferred method of collecting real time information from low-income community members. Most importantly, texting may offer an efficient, inexpensive way to give a voice to people who aren't often heard and whose needs aren't always met."

The study, which appears online in BioMed Central Public Health, was a collaboration among researchers at the U-M Health System, VA Ann Arbor Healthcare System, the Detroit Community-Academic Urban Research Center and non-profit Detroit organization, Friends of Parkside.

The 20 participants were asked hypothetical questions related to their health to evaluate how they would respond to leading reasons for urgent outpatient medical visits and also common primary care concerns. Examples included what they'd do if they needed a flu shot for a new job, had a four-day-old rash on their leg, or fell down the stairs and thought they'd broken a leg. On average, the response rate was 72 percent.

The answers gave researchers a glimpse into possible health needs in the community. One question, for example, asked people how they would respond if they couldn't move their right arm or leg and suddenly couldn't speak. Several participants didn't realize those were signs of a stroke, answering that they would wait it out. The findings prompted local initiatives to better educate the community on telltale stroke warnings.

Researchers point out that text messaging has become ubiquitous in the country and its brevity and simple wording may potentially reduce the literacy barrier of other types of surveys. Cell phone ownership is also rapidly growing among minorities and low-income Americans. More black Americans own cell phones than white Americans (93 percent compared to 90 percent) and they are also more likely to use text messaging (79 percent compared to 68 percent), according to the Pew Research Center.

"What's great about text messaging is that it's not a new technology that anyone has to create or learn how to use," says Chang, an alumnus of the Robert Wood Johnson Foundation Clinical Scholars Program. "Typically we have used cellular phone technology to push out information, not as much to collect opinions from people.

"However, this everyday technology may not only help researchers better understand under-represented perspectives, it can also help organizations quickly tap into their stakeholders thoughts and opinions to get to the heart of significant issues."

The median income of residents surveyed was between $16,000 and $26,000, with 32-47 percent living below poverty. Most participants said they could answer two text message survey questions a day while others said they could answer up to five. Seven Fraunhofer Institutes will be presenting their latest developments in seafaring and navigation at the SMM Convention in Hamburg from September 9 to 12. Among the featured novelties at Booth 411, Hall B6 is a 3D configurator that makes it possible for owners to experience cruise ships and yachts in real time, down to the last detail -- even before the shipbuilding begins. Researchers will additionally display a new software program for crew management, and a ship and logistics system so that inland water routes become more feasibly useable.

When building a cruise ship or a yacht, the minor details often play a major role -- ultimately, customer is king and should also feel that way, too. Which surfaces suit the interior furnishings the best? Which materials are just right for the seats, which wood for the furniture? Up to this point, designers had to decide on the interior configuration of the ship based on material or fabric samples, or arrange for costly mock-ups to be assembled. With its "Virtual Maritime Interior Configurator," the Fraunhofer Institute for Computer Graphics Research IGD in Rostock developed a tool that enables shipbuilders and customers to plan the entire interior -- prior to construction start and in a realistic manner and in real time. Above all, this prevents the additional costs generated by poor planning. For this purpose, the project team configured its 3D software with a materials scanner. This way, random materials can be scanned in, stored within a database and transferred to the virtual elements and spaces/rooms that are depicted with the software. Then in real time, a realistic tracking of the finished space is calculated that -- depending on the perspective -- even displaces the actual lighting conditions and surface reflections. Interested visitors at the SMM can try out the capabilities of this configurator for themselves, at a multi-touch monitor, and make their way through a yacht by means of a 3D flight.

Crew management made easy

It is no easy feat to produce a crew's service plans. There must be compliance with the international laws of the sea and the ocean carrier's own internal regulations, flexible responses to changes to the schedule, and seamless documentation of work shifts and rest periods. The "Crew Compliance Organizer" software tool engineered by the Fraunhofer Center for Maritime Logistics and Services CML is intended to assist planners in this regard to structure individual plans efficiently and in compliance with regulations. This means shipping companies can prevent having their ships held up at the port due to regulatory violations -- ultimately disrupting sensitive business processes. The solution consists of three modules. The first one calculates the personnel needs in the shipping operation. On this basis, the second one automatically compiles the security and service plans. It takes into account working hours and employee rest periods. The third module handles responsibility for reporting, and compiles the compliance reports. At the Fraunhofer booth at SMM, the researchers will exhibit the functions of the software using a multi-touch planning table.

Using the waterways economically

Europe's rivers still have major potential when it comes to the transport of goods. Under the EU Project known as "NEWS -- Development of a Next Generation European Inland Waterway Shipping and Logistics System" -- an international research team is developing a holistic concept on how the water routs can be used with greater logistic efficiency. At the SMM, Fraunhofer Austria -- together with the Vienna University of Technology will be showing, for example, how a new ship's hull may appear that has markedly greater transport capacity than the conventional container ships. Furthermore, an innovative usage concept will be displayed for the Danube and adjacent rivers and canals. For this project, Fraunhofer experts conducted a socio-economic analysis as to the extent it benefits automotive manufacturers and suppliers to shift product shipments to Europe's waterways.

Fewer Vibrations

Distributed engines, drive trains, pumps, and secondary aggregates are frequently the cause on board of often undesirable oscillations and noise. Particularly aggravating is travelling as passenger in an inside cabin next to the engine room. The agent of rotational vibration from the Fraunhofer Institute for Structural Durability and System Reliability LBF produces and eases the rotary oscillation in rotating systems, and thus reduces uncomfortable vibrations. Shipbuilders can use this innovation for test benches of ship drives or for the drive trains in the ship.

Ship radar without rotating antenna can see more

One challenge in the shipbuilding industry is the selection and placement of a variety of electronic systems for navigation. Because traffic on the world seas is growing, and the demands placed on the ships radar increasing, in order to prevent collisions and to stay on course. While previous navigation systems work with old fashioned high frequency technology, at this year's SMM the Fraunhofer Institute for High-Frequency Physics and Radar Techniques FHR presents an antenna with electronic beam scanning. An agile modularly structured radar system equipped this way delivers higher resolution and can even detect smaller objects -- like fishing boats or hazardous flotsam. This is possible through coherent signal processing. That means the form of the transmission pulse is nearly identical in every signal / receiver cycle. This way, disruptions can be suppressed and other target information gained. In comparison to the previous rigidly rotating systems, the antennas can furthermore pivot quickly and without inertia -- thus allowing for any arbitrary directional change. Even when it comes to costs, the radar is clearly superior to past technology in the long term: Previously, the magnetron tubes of conventional systems, prone to wear, had to be replaced every year. These maintenance costs were dropped with the new radar. Since the system works in the S-band using a lowered transmission power in the 2.9 to 3.1 GHz frequency range, it became possible to use more cost-effective semiconductor components and technologies. Researchers at the Autonomous University of the State of Mexico (UAEM) were able to simulate human facial expressions in virtual characters and use them in order to create better environments within a virtual communication.

So far, the so-called virtual agents also mimic human behavior through programmed commands or scripts, but this results in a very "robotic" reaction, which is not interesting for the user, said Marco Antonio Ramos Corchado, engineer at the Department of Computational Science at UAEM.

The main objective of the research is to generate expressions and emotions based on real people, taking as reference the 43 muscles involved in facial behavior depending on the psychological environment.

To achieve this in human models, tactile sensors were placed that release tiny electrical pulses to provoke different gestures with which a 3D camera captures the personality traits.

With the data collected, multiple virtual characters were included in project called "serious game" which, unlike video games consoles or computers, does not seek to entertain, but to run different educational, scientific or civil strategies, detailed the UAEM engineer.

Human behavior is strongly influenced by emotions, intentions, attitudes and moods that vary depending on the social context. Once these factors are captured by the 3D camera, they are translated into numerical data and then entered into the kinesic model designed by the UAEM, to sort and generate the animation of the expressions and gestures of the virtual characters in situations of happiness, sadness, surprise, fear, anger and disgust.

In order to achieve better results, the UAEM teamed with CINVESTAV GDL and the University of Guadalajara (UdeG) where students served as models to obtain the physical and psychological characteristics, which the application requires for a psychological profile and temperament. From the numerical measurement of emotions and sensations different facial expressions are generated.

The aim of the project is to foster attitudes of self-improvement, use dynamics of context to enhance the learning process, promote collaborative environments and communication to solve problems and riddles.

In this case the "serious game" created by the UAEM, Cinvestav Unit Guadalajara and the University of Guadalajara, is intended to predict the behavior of people in different risk situations such as natural disasters. For this project an earthquake will be simulated in Guadalajara, Mexico explained Ramos Corchado. Collaborative three-dimensional sketching is now possible thanks to a system known as Hyve-3D that University of Montreal researchers are presenting today at the SIGGRAPH 2014 conference in Vancouver. "Hyve-3D is a new interface for 3D content creation via embodied and collaborative 3D sketching," explained lead researcher Professor Tomás Dorta, of the university's School of Design. "The system is a full scale immersive 3D environment. Users create drawings on hand-held tables. They can then use the tablets to manipulate the sketches to create a 3D design within the space." For example, as the designers are immersed in their work, this could mean designing the outside of a car, and then actually getting into it to work on the interior detailing. Hyve-3D stands for "Hybrid Virtual Environment 3D." Univalor, the university's technology commercialization unit, is supporting the market launch of the system.

The 3D images are the result of an optical illusion created by a widescreen high-resolution projector, a specially designed 5m-diameter spherically concave fabric screen and a 16-inch dome mirror projecting the image onto the screen. The system is driven by a MacBook Pro laptop, a tracking system with two 3D sensors, and two iPad mini tablets. Each iPad is attached to a tracker. "The software takes care of all the networking, scene management, 3D graphics and projection, and also couples the sensors input and iPad devices," Dorta explains. "The iPads run a Satellite application, which serves as the user interaction front-end of the system. Specialized techniques render the 3D scene onto a spherical projection in real-time." The Hyve-3D software also works on conventional 2D displays.

Up until now, 3D design required complicated or expensive equipment. "Our system is innovative, non-intrusive and simple," Dorta said. "Beyond its obvious artistic applications, Hyve-3D clearly has industrial applications in a wide range of fields, from industrial or architectural design to engineering, medical 3D applications, game design animation and movie making. My team is looking forward to taking the product to market and discovering what people do with it." The mystery behind the movements of flocking starlings could be explained by the areas of light and dark created as they fly, new research suggests.

The research, conducted by the University of Warwick and published in the journal PNAS, found that flocking starlings aim to maintain an optimum density at which they can gather data on their surroundings. This occurs when they can see light through the flock at many angles, a state known as marginal opacity. The subsequent pattern of light and dark, formed as the birds attempt to achieve the necessary density, is what provides vital information to individual birds within the flock.

The dynamic pattern of light and dark is created by birds within the flock altering the positions and angles at which they fly, causing a change in the amount of light let into the flock. The researchers observed that it was always possible to see areas of light coming through the flock, providing the initial insight that the changing patterns of light and dark had a role to play in the flock's movement.

This insight led to the development of a computer model in which individual birds with simulated intelligence were attracted to the areas in the flock that could provide the most information on the rest of the flock.* When each simulated bird was attracted to the areas in the virtual flock that can provide the most information the result was a cohesive swarm.

The Warwick team then applied the model's findings to flocks in the wild and established that there was a strong correlation between movements of the virtual and natural birds.

"An individual starling within a flock can see in front of them areas of light and dark created by other birds, forming a dynamic and changing silhouette," says lead researcher Daniel Pearce from the University's Department of Physics. "Our research ascertained that the silhouettes we external observers witness were a result of large flocks self-organising to achieve a marginally opaque state at which a bird can still see some of the light sky through gaps in the flock and gather information from other birds.

"When we observe a flock of starlings we are actually seeing a 2D projection of a dynamic, changing 3D environment. By developing this model we have been able to simulate this environment and see that when each bird is attracted to the areas in the flock that can provide the most information the result is a cohesive swarm that resembles a large flock of starlings in the wild," comments Mr Pearce.

It had previously been thought that co-ordination of a flock was achieved through birds interacting only with neighbouring members but, argues Professor Emeritus and paper co-author George Rowlands, the new research marks "a paradigm shift in our understanding of how birds organise within a flock as it shows that the local interactions between birds are alone insufficient to explain large-scale flock organisation."

* "We use a technique called agent based modelling of self-propelled particles, made famous by Vicsek et al (1995). Each bird is represented by a particle which each have an identical set of rules to follow (and likelihood of making a mistake). In this case the rules are a) follow your nearest neighbour and b) move towards the areas of the projection containing the most information. When lots of these particles are introduced, the result is a collective motion much like that of a real flock of birds," says Mr Pearce. Football success in Brazil may not be over yet for England. Although England's national team failed to make the knockout stages of the FIFA World Cup, the University of Hertfordshire's robot football team, the 'Bold Hearts', is set to fly out to Brazil next week to compete in the 2014 RoboCup robotics world championship -- taking place in João Pessoa, Brazil, 19 -- 24 July 2014.

Robot teams from around the world, including the USA, Japan, China, Germany as well as the UK, take part in the RoboCup competition which hosts a series of tournaments in various robotic disciplines, the most prominent of them being football. The concept behind these competitions is to provide a uniform and comparable test-bed for progress in robotic research. Teams use state-of-the-art artificial intelligence with the aim that by 2050 a team of robots will be able to take and beat the best human players -- to be the world champions!

Dr Daniel Polani, Researcher and Reader in Artificial Life and Team Leader of Bold Hearts at the University of Hertfordshire, said: "We've been taking part in the RoboCup competition since 2003. And this year we have been making significant advances with our robots. To succeed at RoboCup competitions, it is not sufficient for robots to operate well in laboratories they have to prove themselves in a novel environment and in adversarial circumstances against opponent teams. We have built on our success in the simulation league, where artificial intelligence software controls virtual reality robots, aiming to become one of the best humanoid robot football teams now that we are in the humanoid hardware league."

In the RoboCup humanoid competition, the robots are fully autonomous -- humans are not allowed to control the robots remotely during a game. Only at the beginning of the game, the robots receive a "start" command and begin playing. They can communicate with each other, but are not allowed to use external computers or commands. Every decision must be computed on board by the robot itself. This is the extremely difficult challenge that the RoboCup community has set itself as a target.

Dr Polani continued: "In 2013, Bold Hearts made the transition from a pure simulation team to a team of hardware humanoid robots. Despite the transition, we made a great start in the competition series. This year, we have focused on expanding our abilities -- and we're really proud and excited that our team took 2nd place at the RoboCup German Open (April 2014) and 3rd place at the RoboCup Iran Open (April 2014), ahead of some very well-established and strong teams. So that's set us up really well for the RoboCup World Championship in Brazil at the end of July."

Just like the national teams taking part in the FIFA World Cup, the humanoid robot teams need to be able to score goals and communicate with each other. The Bold Hearts team has been very successful in scoring goals -- a good attribute to have, you may think. But what if they are own goals? For example, in the first game at the German Open 2014, Bold Hearts won 4:2, but Bold Hearts scored all six of the goals! So Bold Hearts' goal scoring ability was clearly indisputable! However, this result showed a clear need to put a 'direction-detecting' module urgently into action.

Dr Polani added: "Implementing the 'direction-detecting' module definitely had the desired result -- with no further own goals. But the Bold hearts robots became what could be described as more 'cautious' and 'hesitating'. "

"While this would be sufficient for a good placing at the Open competitions in Germany and Iran, such hesitation would not be good enough at the World Championship in Brazil. So we have had to put our efforts into making the Bold Hearts team more forceful and decisive, improve situational awareness, and we incorporated past experience from the simulation competitions to achieve better team play -- so robots on the team would pay more attention to each other."

The RoboCup competition highlights the challenges and promises of the future of robotics. William Warren's research group is advancing virtual reality technology in the service of studying the science of the swarm: how patterns of crowd movement emerge from individual behaviors. He described his work June 29 in a keynote address to a conference in Vancouver.The cognitive scientists in the Virtual Environment Navigation lab at Brown University are not only advancing a frontier of behavioral research but also of technology.

Led by Professor William Warren, the group developed a wireless virtual reality system to study a phenomenon that scientists don't yet understand: how pedestrians interact with each other and how those individual behaviors, in turn, generate patterns of crowd movement. It's an everyday experience for all kinds of animals including ants, birds, fish and people.

"When you walk across campus during class change you are, consciously or not, coordinating your movements with the people around you," Warren said. "In some situations coherent 'swarms' form, somewhat like a flock of birds or a school of fish. We want to understand that process."

A well-tuned computer model of such swarming behavior could have many specific applications in human life.

"There are applications to urban planning, architectural design, and evacuation planning," Warren said. "Crowds seem to behave in predictable ways, but the environment is not always built to accommodate people's behavior."

It could also lead to technology to help visually impaired pedestrians. The research is gaining attention. Warren flew to Vancouver last week to give a keynote address June 29 at the annual meeting of the International Society for Gait and Posture Research.

Of swarms real and virtual

His talk was not just about how what we see guides how we walk. It also delved into the kind of technology people normally discuss at video game conferences.

One needs a crowd to study crowds, so Warren and his students recruited volunteers for one called the Sayles Swarm and recorded them using motion capture technology (Sayles Swarm Link). But the most versatile way to assemble a crowd is in a virtual world, where any configuration is possible. In the VENLab, Warren's team including graduate students Kevin Rio and Greg Dachner can immerse up to four real people in a carefully controlled, realistic virtual crowd.

"We're trying to use VR to figure out the rules that govern pedestrian behavior," Warren said. "In experiments, we can manipulate a virtual crowd to see how real subjects respond to the behavior of their virtual neighbors. This allows us to model the visual coupling between an individual and their neighbors. We then use that model to predict what will happen when a group of people walk together in a real crowd."

Walking wireless

To make this possible his team rigged up the 168-square meter VENLab to make their off-the-shelf Oculus Rift VR headsets wireless. The room also has video cameras and a grid of beacons on the ceiling that emit ultrasound pulses denoting their location. The headset "listens" for those beacons with microphones and combines them with signals from accelerometers, much like the ones in a smartphone, to determine where the subject is and what they are looking at in the virtual world.

Brown has filed for patents on the wireless system.

Going wireless has vastly improved the research, because it allows subjects to move unencumbered.

"That's the breakthrough: the freedom of movement," Warren said. "Before we had a bunch of heavy cables running to the head-mounted display. At one point the subject had to wear a big control box in a backpack. But now we've managed to make it quite light and mobile so people can move about freely."

In the future they hope to integrate many people into the same virtual environment, even if they aren't in the same real room. That will require making headsets that can track their location without any surrounding hardware. They are working with engineering Associate Professor Gabriel Taubin to do that.

All along the group has been generating insights. In Vancouver, Warren discussed several "swarm scenarios" his group has studied both in crowds of real people and in virtual reality. In "Grand Central" individuals crisscross the room, as if heading across a train station concourse. In "counterflow" two groups pass through each other.

"We're finding that when people walk together they tend to match the speed and direction of their neighbors. That intrinsically leads to the emergence of collective motion," Warren said. "What we're showing with the virtual crowd is that as you add more neighbors, their influence simply sums up. Now we are mapping out what we call the 'coupling field' that connects you to neighbors at different distances and different positions. This enables us to simulate the collective behavior of a large crowd."

In this high-tech way, Warren and his team are revealing the science of the swarm. Final year student in the University of Alicante's Multimedia Engineering degree Roberto Gómez has designed a soccer game adapted for people with cerebral palsy which is operated with a foot switch, a push rod head switch and a hand switch. A new tool used for the first time by the Cerebral Palsy Association of Alicante, APCA, and which allows any player to have equal access with different physical conditions.

So far, this is a prototype on which student Roberto Gómez as well as lecturers and co-directors of the project, Rafael Molina and Carlos Villagra, have worked since last November, and in a few months it will become a free software created to suit people with cerebral palsy.

The APCA team stated that the game empowers strategic planning, perceptual ability, enhances spatiotemporal organisation and speeds up physical response," so it can be used by any disabled person. Furthermore, it meets the recreational component of any game, enhances physical, emotional, and self-improvement motivation of this group.

According to the UA student's experience, this final year project has been warmly welcomed by the Association since the demo was designed based on their needs and preferences. In fact, APCA boys and girls themselves have been the ones who chose the interface theme and have given their opinion on how to improve the tool.

This is a game with well thought-out software designed for people with cerebral palsy, with a simple interface, free from distractions, allowing all participants to focus on the essential information. Technically, the greatest difficulty has been to filter out the unintended clicks of players and replace the traditional menu to choose a new game, player etc., by automatic options scanning, Roberto Gomez said.

Until its release, only a small group of users had interacted before with a commercial game with the access limitations that this entails, so it is the first time that the Centre is involved in the use of such resources.

The features of this game allow the group to have the same opportunities regardless of their physical conditions, so they only need to have access to any type of switch (foot, joystick, rod, ...). "All these factors contribute to the learning opportunities, interaction and entertainment of this game," APCA professionals emphasized.

During the various experiments and tests, users have played both individually and in groups and it has been very welcomed amongst them, who stated their intention to play it more often. In this sense, the team of professionals has proven that users are more enthusiastic when playing in a group, since it allows them to enjoy leisure by interacting with peers and competing among them.

The project started from the initiative of lecturers Rafael Molina and Carlos Villagra in order to engage their students in professional jobs that respond to real needs of society. "It is the first time a student has accepted the challenge and it has been a very rewarding experience so we will prepare new proposals for our students next year." The human brain is the most complex computer in existence. Understanding how it works has been a scientific endeavor for centuries. However, technology has only recently advanced to the point where we can really understand brain function down to the molecular level. By combining the newest advances in computer programming with biochemistry, Okinawa Institute of Science and Technology Graduate University's Computational Neuroscience Unit is helping to create the tools that will advance our understanding of the brain. Members of Professor Erik De Schutter's unit have developed a computational simulator called STEPS to help researchers understand neuronal signaling pathways at a molecular level.

Since its first release in 2009, this simulator has been significantly enhanced. In a recent paper published in Frontiers in Neuroinformatics, researcher Dr. Weiliang Chen introduced two new toolkits he has developed to further extend STEPS functionality.

Computer modeling allows researchers to collect data and anticipate what may happen in a live brain, helping them test and develop hypotheses that can be used in conjunction with experiments in the lab. Computational neuroscience simulators, such as STEPS, rely on detailed reconstructions of the complex morphology, or shape, of the neurons. The reconstruction of neuronal morphology for simulation is difficult and time consuming, although recent advances in scientific software has somewhat simplified the process. The geometry preparation toolkit introduced in the paper simplifies recreation of neuronal morphology for the researcher. The second toolkit, the visualization toolkit, allows researchers to directly visualize STEPS simulations of those reconstructed morphologies in real time, both graphically and numerically, a capability that was not available in the STEPS program before.

In the paper, Chen demonstrates how the toolkits work in two different simulations. One example, using the geometry preparation toolkit, illustrates how the complex morphology of a neuron impacts the function of the cell. In this simulation of anomalous diffusion, you can see that the molecules get caught in the protrusions of the cell, the 'spiny' parts of what are called spiny dendrites, and diffuse slower than the ones in a smooth dendrite that does not have the protrusions. The other example, demonstrating the visualization toolkit, simulates what happens when an ion channel opens to release stored calcium within a neuron. Researchers are able to see the effects of the channels opening and the calcium being released both graphically and numerically. Both of these examples have been previously reported in laboratory experiments and simulated using computational modeling. However, the ability to easily recreate the shape of the cell, watch these events as they occur and manipulate conditions in real time is due to the new toolkits.

The entire STEPS project is part of a worldwide research initiative by the European Commission called the Human Brain Project. Ultimately, the goal is to integrate STEPS, which models neurons at a molecular level, with other computational models that look at the cellular and neural network levels that others are developing, to create a massive virtual human brain. Researchers will use this virtual brain to study anything from traumatic brain injury to the effect of a drug or a disease. The Computational Neuroscience Unit is one of only two laboratories in Japan that are part of the Human Brain Project.

As for Chen, he says, "I just want to see what's going on in the brain, and I want others to see it easily as well!" Asked whether or not the Human Brain Project will create that understanding, he says, "eventually we will be there, definitely. But you need to get the tools ready before that can happen." With ingenuity from researchers like him, it seems we are well on our way. Deliberately grounded on a tiny reef in the South China Sea, part of an island chain claimed by the two Asian countries, the Sierra Madre is now the unlikely base for a detachment of Filipino marines who stand guard over the atoll, scanning the turquoise waters for Chinese ships.

Just reaching this unusual landmark, located 105 nautical miles (194 kilometers) from Palawan province in the western Philippines, is a nerve-jangling experience.

We approached the submerged reef in our aging wooden fishing boat at top speed — 11 knots per hour. From the north, a modern Chinese coast guard ship was closing in at least twice as fast with the intention of blocking our path. A second Chinese vessel quickly approached from the south with the same idea.

"We are prepared here just in case China attacks us. The school was assigned as an evacuation center. I am nervous because it might happen. What will happen to us?"

— Jacqueline Morales, Pag-asa resident

But they didn't make it. After several tension-filled minutes, we entered the shoal, which was too shallow for the larger Chinese boats to follow. Some of the fishermen on our boat prayed in relief – it doesn't always happen this way.

It's probably fair to say not many people have heard of the Second Thomas Shoal, which is known as Ayungin in the Philippines and Ren'ai Jiao in China. This teardrop-shaped reef is part of the Spratly Islands, a mostly uninhabited archipelago midway between the Philippines and Vietnam, claimed entirely by China and in its various parts by the Philippines, Brunei, Malaysia, Taiwan and Vietnam.

To complicate matters, the conflict has sucked in the United States, which has a mutual defense agreement with the Philippines and is urging a peaceful resolution. It has also prompted closer security ties between the Philippines and Japan, which is itself at loggerheads with China over islands in the East China Sea.

While some of these disputes focus on little more than clusters of uninhabited rock, the outcome of this territorial wrangling has the potential to sway the balance of power across the region. The value of some of these territories actually lies under the seabed with pockets of natural gas and oil — as we've seen recently with the deployment of oil exploration rigs by China off the Paracel Islands — another disputed chain in the South China Sea.
The start of a difficult journey

Getting to the Second Thomas Shoal took months of negotiations with Philippine authorities – because of logistical and security concerns – and then seven days traveling by boat.

I started the odyssey in April this year in Puerto Princesa, the capital of Palawan province. I was traveling with Eugenio Bito-Onon Jr., the mayor of Kalayaan, the smallest and one of the poorest municipalities in the Philippines. It consists of 10 tiny islets and reefs situated at the northern tip of the Spratlys.

We were headed for Pag-asa first, the only island in the area with a civilian population. It was also the staging point for reaching the Sierra Madre. Usually based in Puerto Princesa, Bito-Onon only manages to travel to Pag-asa once a year. We stopped at a few smaller islands on our journey, each home to small detachments of marines from the Philippines — the last line of defense against foreign encroachment.

The journey: Journalist Tomas Etzler makes the long journey to Pag-asa in the Spratly Islands territory claimed by the Philippines and China.

Some of those marines admitted it was a lonely mission but said they were proud to serve in such a remote outpost in defense of Philippines territory. Others claimed it was "fun and exciting" to monitor foreign ships, mostly Chinese, moving into their waters.

At the end of the third day of our journey, we finally arrived in Pag-asa, the second-largest island in the Spratlys. Previously a military base, the Philippines government encouraged civilians to move here in 2002. More than 12 years on, and 120 people now live alongside the small units of Philippines air force, navy and marines still stationed here.

Jacqueline Morales, 28, moved to the island from Palawan with her husband and two children. She wanted to serve her country and heard that Pag-asa was in desperate need of teachers. While residents' living costs are partially subsidized by the central government, she admitted the "China factor" is a real worry.
 Some of those marines admitted it was a lonely mission but said they were proud to serve in such a remote outpost in defense of Philippines territory. Others claimed it was "fun and exciting" to monitor foreign ships, mostly Chinese, moving into their waters.

At the end of the third day of our journey, we finally arrived in Pag-asa, the second-largest island in the Spratlys. Previously a military base, the Philippines government encouraged civilians to move here in 2002. More than 12 years on, and 120 people now live alongside the small units of Philippines air force, navy and marines still stationed here.

Jacqueline Morales, 28, moved to the island from Palawan with her husband and two children. She wanted to serve her country and heard that Pag-asa was in desperate need of teachers. While residents' living costs are partially subsidized by the central government, she admitted the "China factor" is a real worry. As we planned the final part of our journey — to the Sierra Madre itself — we discussed the possibility of encountering Chinese coast guard ships with the crew of the modest fishing boat we hired for this leg of the trip. We were told the Chinese have stopped or tried to stop boats entering the shoal.

We agreed the strategy would be that unless the captain felt his boat was in danger of being rammed — not uncommon in many of the territorial disputes in this region — he would try to outmaneuver the Chinese to reach his destination. Most boats approaching the Second Thomas Shoal come from Palawan, from the southeast. We came from the northwest, which the Chinese patrol boats would not anticipate, our skipper reasoned. That's why our boat surprised and eluded them.

From a distance, the Sierra Madre looks like any other big ship. It's only when you get closer you realize something isn't right.

The sun-scorched hulk towering above the shimmering blue waters of the shoal looks like it could be part of a movie set for the sequel to a postapocalyptic epic such as "Mad Max" or "Waterworld." The bridge tower is so rusty it looks like it could collapse at any moment, while the hull is pockmarked with large, rusting holes. After climbing on deck via a worryingly makeshift ladder, we were greeted by Lt. Earl Pama, the commanding officer. The 29-year-old marine's unit had been here since March 30. As with other islands in the area, the marines are rotated in and out every three months. It's not an easy deployment; Pama's unit got to Sierra Madre only on the second attempt. Their first approach was blocked by Chinese coast guard ships.
Chinese ships like predatory sharks

By late afternoon on our first day there, three more Chinese ships arrived in the vicinity. The Sierra Madre was now surrounded by five vessels, which were slowly circling the shoal like predatory sharks. As I peered through my binoculars, I saw some of the Chinese sailors were looking right back at the ship taking pictures using cameras with long lenses. They face a merciless sun and searing temperatures. During rain showers or typhoons, the radio room, their only contact with superiors in Palawan, is the only one on the ship that doesn't leak. The soldiers are cut off from the outside world most of the time.

"I estimate there are five to six hundred rats and a million cockroaches."

— Hilbert Bigania, Philippines marine

"Our life here is hard sometimes because we are far from our families," Hilbert Bigania, a 30-year-old sergeant, said. "We can't communicate with them, and we're in the middle of the ocean. That's our everyday life here. We can't do anything."

It can also be a struggle just to survive.

The marines claim that in 2012, the Chinese ships became more aggressive and started to harass Philippine navy vessels bringing in troops for rotation and supplies. "What they do is they block the provisions that would be delivered to us, so that we don't have food to eat and we don't have supplies or even water," said Pama.

Fearing an open conflict with the Chinese, the Philippine navy began to use airdrops or civilian fishing vessels to bring in supplies. On my second day on the Sierra Madre, two small navy planes dropped two loads of supplies. One landed on the ship, the second in the water. The Philippine planes appeared to be shadowed by other aircraft — Chinese spy planes, the marines claimed. The small containers held basic food supplies, soft drinks, flip-flops and towels. But what cheered the marines most were letters of support from schoolchildren as well as boxes from a fast-food chain filled with fried chicken, rice and French fries. It was a rare feast, as there are only one or two drops like this during each deployment.

The bulk of their diet consists of fish they've had to catch. Using handmade spear guns or makeshift rods, they fish twice a day. The waters surrounding the boat are as shallow as 5 feet (1.5 meters) and full of marine life. The soldiers move around the shoal on an improvised rubber raft and use strips of wood with rubber straps as flippers to propel themselves around the water. The catch is then dried and grilled on the deck of the ship.

Fishing also helps them to kill time; there's not much to do on the ship. Even walking on the deck is dangerous. The Sierra Madre is severely weathered and riddled with holes. When not fishing, the marines monitor their Chinese shadows, clean their weapons, exercise using broken off metal parts from the ship as weights or simply relax in their hammocks, listening to Filipino pop music.  Earlier this year, the Philippines filed a case with the United Nations over China's conduct in the South China Sea, including the encirclement of the Second Thomas Shoal. China says it will not accept international arbitration, saying the only way to resolve the dispute is through bilateral negotiations.

"Regardless of how the Philippines packages its lawsuit, the direct cause of the dispute between China and the Philippines is the Philippines' illegal occupation of part of the islands in the South China Sea," Foreign Ministry spokesman Hong Lei said in a statement in March.

The government officials I spoke with in Manila said that even if the ruling, which is expected early next year, goes against China, it won't change much. There are no mechanisms to enforce the ruling. The standoff between the two countries will likely continue for years. During my second and last evening on the boat, I joined Pama as he sat alone on the deck sipping a Gatorade — courtesy of the earlier airdrop — staring at a beautiful sunset. A Chinese ship sailed by just a few hundred yards away. I asked him if he thought the Chinese would ever move on their position.

"If the Chinese try to enter here, we'll defend it," he replied without hesitation. "We will use our training to defend the ship. We will lay down our lives to defend this ship."

We left the Sierra Madre shortly after 5 a.m. the next day. The surrounding Chinese ships did not even move. Finding commonality among classes makes for effective object-oriented programming. Often, programmers express that commonality using an inheritance hierarchy, since that is one of the first concepts taught in object-oriented programming.

We're going to go to the other extreme in this chapter to explore the difference between using inheritance and using interfaces. An emphasis on interfaces guides you in determining what is the real essence of a class; once you have determined the essence, then you can look for commonalities between classes.

Creating an inheritance hierarchy prematurely can cause extra work when you then need to untangle it. If you start with interfaces and discover an appropriate hierarchy, you can easily refactor into that hierarchy. Refactoring into an inheritance hierarchy is far easier than refactoring out of an existing hierarchy.

We will look at examples of alternative designs that emphasize either inheritance or interfaces, so you can compare the two approaches. An interface-oriented alternative of a real-world Java inheritance hierarchy demonstrates the differences in code. 


5.1 Inheritance and Interfaces

You probably learned inheritance as one of the initial features of object oriented programming. With inheritance, a derived class receives the attributes and methods of a base class. The relationship between derived and base class is referred to as "is-a" or more specifically as "isa-kind-of." For example, a mammal "is-a-kind-of" animal. Inheritance creates a class hierarchy.

You may hear the term inherits applied to interfaces. For example, a PizzaShop that implements the PizzaOrdering interface is often said to inherit the interface.[1] However, it is a stretch to say that a PizzaShop "is-a" PizzaOrdering. Instead, a more applicable relationship is that a PizzaShop "provides-a" PizzaOrdering interface.[2] Often modules that implement PizzaOrdering interfaces are not even object-oriented. So in this book, we use the term inherits only when a derived class inherits from a base class, as with the extends keyword in Java. A class "implements" an interface if it has an implementation of every method in the interface. Java uses the implements keyword precisely for this concept.[3]

Inheritance is an important facet of object-oriented programming, but it can be misused.[4] Concentrating on the interfaces that classes provide, rather than on their hierarchies, can help prevent inheritance misuse, as well as yield a more fluid solution to a design. Let's look at some alternate ways to view example designs using both an inheritance-style approach and an interface-style approach. Both inheritance and interfaces provide polymorphism, a key feature of object-oriented design, so let's start there.
5.2 Polymorphism

A common form of polymorphism consists of multiple classes that all implement the same set of methods. Polymorphism of this type can be organized in two ways. With inheritance, a base class contains a set of methods, and derived classes have the same set of methods. The derived classes may inherit implementations of some methods and contain their own implementations of other methods. With interfaces, multiple classes each implement all the methods in the interface. 5.3 Hierarchies

The animal kingdom is a frequently used hierarchy example. The hierarchy starts with Animal on top. Animal breaks down into Mammals, Fishes, Birds, Reptiles, Amphibians, etc. The relationships parallel those of an object-oriented hierarchy: a cow "is-a" Mammal. The subclasses (derived classes) have attributes in common with the superclasses (base classes). This zoological classification is based on characteristics used to identify animals; Figure 5.3 shows a portion of the standard hierarchy.

How do people become professional programmers? Many people go the "traditional" path through a computer science or software engineering education and from there into professional programming work.

Others become professional programmers by accident. A person writes a small program to help at work, and their workmates say, "Oh great, you can write programs! You're our programmer now!"

Other people start out as hobbyists and follow a less traditional path, not always getting a degree, but clearly wanting to be programmers from the start and working actively towards that goal.

I've been a hobbyist programmer since I was 6. I wasn't writing anything amazing back then but I had started writing and soon found it was absorbing most of my time. Since I never really stopped, that gives me 24 years "programming experience" and counting.

At first I was into writing computer games. Later people asked me to write programs for them, and sometimes I even got paid. From this I learned that software is always for something. Programs are not self contained worlds of their own. People expect things out of a program that have more to do with Japanese or Geophysics or Engineering (or whatever they've got in mind) than with how a computer works. I had to learn something about all those domains in order to write programs for them.

At university it didn't take long before I was a tutor, and that's where I found I enjoy teaching, and especially enjoy teaching programming.

While I was at university I got my first "real" job, writing Visual C++ code for a financial database company. In terms of design and theory it was lightweight stuff. But in terms of working with others on a large project I was being thrown in the deep end! They had gigabytes of source code, growing cancerously through the efforts of a dozen developers of wildly differing skill levels.

In spite of my programming skills being well above average there, I learned to settle for being a junior programmer, a little fish in a large pond.

Skipping along a few more jobs and a lot more years, today I am a senior developer in a small research group—a big fish in a little pond. I've had to teach my co-workers a lot about professional programming, because most of them haven't been in industry to get that taste of what large code bases and diverse skill levels do to programs if you aren't using those "professional" skills to keep everyone pointed in the same direction.

There's quite a gap between "being able to program" and being a "professional programmer." It took me 15 years to go from beginner to hotshot programmer, then another 10 years to go from hotshot to professional—and I'm still learning.

Whatever the path we follow, most professional programmers have in common the fact that they learned to code first and how to be a professional later.
The Meaning of "Professional"

So what does it mean to be a professional programmer? What does it mean to be a professional anything? Some definitions simply say to be a professional is "to make money from a skill," but true professionals also have a set of qualities often described as "professionalism." In my opinion, these qualities are: trustworthiness, teamwork, leadership, communication, constant updating of skills, an interest in minimizing risks and accountability. Each of these effect the professional programmer in certain ways.

Trustworthiness The concept of trustworthiness applies in several different ways for programmers. Can you be trusted with a job? To perform a task without someone checking up on you? Can you be trusted to ask for help when you need it?

If you're given clients' data or have signed a non-disclosure agreement, then you are being trusted to respect privacy. You are trusted to check license agreements on third party tools or libraries and to get licenses or permission as required. And like any professional you are trusted to simply do a good job.

Teamwork Will you genuinely cooperate with your team mates? Will you work to mutual advantage and not just your own? Can you trust your team to work with you? Can you do your share of the work and trust your team to do the rest? And can you accept your management (and sometimes even clients) as part of the team, everyone trying to get the same job done?

Leadership Showing leadership means both earning respect from others and knowing what to do with it. Recognize the skills of your team members, and make sure you can offer each person challenges and development without exceeding what they can cope with at a given time.

Leadership involves not always getting to do the "fun" parts of a project yourself (that scary "delegation" word). It also involves not asking anyone to do a task that you wouldn't be willing to do yourself. It's not just the managers and lead programmers who need to show leadership, it's any professional programmer. The best programmers to work with are the ones that know what's going on, not just their little tasks.

Communication Respecting the people you work with, and your clients, enough to really listen to them is a critical part of communication. Teamwork can't happen without good communication, nor can accountability.

Communication is critical for helping clients to produce usable specifications and feedback. Will you question whether the specs you are given really will serve the purpose that the client has in mind?

Communication skills help with making meetings timely and effective. A professional's communication is effective and to the point, whether in person, in email, on the phone or in written documents.

Documentation at first seems like a programmer-specific concern until you consider how many people require documentation in a serious project: other programmers need high level, API level and in-code documentation; managers need planning, progress, and bug documentation; lawyers need proof of what was done and when; and users need documentation on how to use the software.

Updating Skills Keeping your skills up to date involves staying aware of what's going on in your industry. What are the current ideas about methodologies like eXtreme Programming? What libraries and tools are out there that might support your project? What are the current refactoring tools? How about standards, file formats and protocols? Are you up to date with Unicode, XML, SQL, and all the other acronyms? Perhaps you're missing out on something if you're not. What platforms are your potential clients using? Should you be learning about cross platform development?

Basically you need to possess a genuine interest in your field, and to read broadly so you know what's out there and which areas to then read deeply about. You also need to accept that even (or should I say "especially") the very best programmers are still learning.

Minimizing Risks Familiarity with best practices, combined with a healthy dose of common sense, will take you a long way towards managing risks. Professional programmers keep track of known bugs or any other change they intend to make. Bugs are risks, and a simple database can prevent you having a product ship with bugs you'd simply forgotten.

Another risk that's often not properly considered is any and all changes to the source code. Source is your livelihood and any change can be a mistake. There's good software out there that will keep track of every revision of your source code and even help merge code that multiple people have changed.

Professional programmers are careful to do enough testing. A software company will generally have testers but the developers need to know how to get the most out of testers and also how to write their own unit and regression tests to make sure every change in behavior is noticed and checked by a human.

Keeping your code simple and well styled is another commonly overlooked way to manage risks. If anyone can look at the code and see right away what it does, you are far less likely to find bugs in it later, and you are less likely to have a junior programmer attempt to change something without understanding it first.

Another risk is the client changing their mind, or more often changing their specifications because they've realized it wasn't what they had in mind. Write your code to be modular and reusable and you won't have any trouble adapting it to changing needs.

Accountability Writing code for others is a responsibility. You need to make sure your software is reliable. You need to make sure you and the client truly understand the requirements and specifications. You need to have documentation of your work, all current and past bugs, your progress, any problems, signed-off milestones, and more. You are also required to know about some basic legal issues, like software licensing, the terms of your employment contract, and intellectual property law.


As you can see, there is a huge gap between "coding" and "professional programming." Most programming courses focus on the coding side of things, and the professional skills tend to be glossed over or not covered at all. I have found myself regularly teaching these skills to new co-workers, which highlighted the need for "professionalism skills training." Teaching my co-workers reminded me how much I enjoy teaching. I decided to teach more people by trying my hand at professional writing for a change.

I set up a web site, which is completely independent from my day job. The site is called Developing Programmers.com. It is devoted to teaching people how to develop into professional programmers. Since founding the site, I've been presenting the tools and ideas that I think professionals should know about.

Some of my articles simply refer to other sites of benefit to would-be professionals. I research other articles from scratch: tutorials, guides, and discussions of things professionals should be thinking about, like revision control, documentation, keeping your group pointed in the same direction—and of course, each of the aspects of professionalism that I listed earlier.

These days I consider myself to be a professional programmer, though I am still discovering the depth and breadth of what exactly that means. Perhaps that ongoing exploration of programming and of professionalism is what makes this for me a career and not just a job. There is evidence that testing is still vitally important to software development, and that it probably always will be. Reviews may be more cost effective, according to recent studies, and proof of correctness (if it ever scales up to larger problems) may be more rigorous, but neither can take the place of taking the software into a near-real environment and trying it out.

Once we realize that we are committed to a future full of testing, it is worth exploring what testing really means. I would assert that there are several flavors of testing, and that all too often when we speak of testing we consider far too few of those flavors.

Here are the flavors I see:

1. First of all, there is goal-driven testing. It is in goal-driven testing that the reason for testing drives the tests to be run. There are roughly four goals for testing:

a. Requirements-driven testing. Here, enough test cases are constructed to demonstrate that all of the requirements for the product have been tested at least once. Typically, a requirements-test case matrix is constructed to ensure that there is at least one test for every requirement. Tools are now available to support this process; 100 percent requirements-driven testing is essential for all software products.

b. Structure-driven testing. Here, test cases are constructed to exercise as much of the logical structure of the software as makes sense. Structure-driven testing must supplement (but never replace) requirements-driven testing because all-requirements testing is simply too coarse a level to insure that sufficient tests have been run. "Good" testing usually tests about 60-70 percent of the logic structure of a program; for critical software, closer to 95 percent should be tested. Testedness may be measured by a tool called a test coverage analyzer. Such tools are now available in the marketplace.

c. Statistics-driven testing. Here, enough tests are run to convince a customer or user that adequate testing has been done. The test cases are constructed from a typical usage profile, so that following testing, a statement of the form "the program can be expected to run successfully 96 percent of the time based on normal usage" can be made. Statistics-driven testing should supplement (not replace) requirements-driven and structure-driven testing when customers or users want assurance in terms they can understand that the software is ready for reliable use.

d. Risk-driven testing. Here, enough tests are run to give confidence that the software can pass all worst-case failure scenarios. An analysis of high-risk occurrences is made; the software is then examined to determine where it might contribute to those risks. Extra thorough testing of those portions is then conducted. Risk-driven testing is typically used only for critical software; once again, it should supplement, but not replace, requirements-driven and structure-driven tests.

2. In addition to goal-driven testing, there is phase-driven testing. Phase-driven testing changes in nature as software development proceeds. Typically software must be tested in small component form as well as total system form. In so-called bottom-up testing, we see the three kinds of phase testing discussed later. In top-down testing the software is gradually integrated into a growing whole, so that unit testing is bypassed in favor of continual and expanding integration testing.

a. Unit testing is the process of testing the smallest components in the total system before they are put together to form a software whole.

b. Integration testing is the process of testing the joined units to see if the software plays together as a whole.

c. System testing is the process of testing the integrated software in the context of the total system that it supports.

It is the manner of intersecting goal-driven testing and phase-driven testing that begins to tax the tester's knowledge and common sense.

For example, do we perform structure-driven testing during unit test, or integration test, or system test? What I would like to present here are thoughts on how to begin to merge these many flavors. Let us take a goal-driven approach first, and work that into the various phases.

Requirements-driven testing means different things in different phases. During unit testing, it means testing those requirements that pertain to the unit under test. During integration testing, it means testing all software requirements at the requirements specification level. During system testing, it means repeating the integration test but in a new setting.

Structure-driven testing also means different things in different phases. During unit testing, it means testing each of the lowest-level structural elements of the software, usually logic branches (for reasons that we do not go into here, testing all branches is more rigorous than testing all statements). During integration testing, it means testing all units. During system testing, it means testing all components of the system, with the integrated software simply being one or more components.

Statistics-driven testing is only meaningful at the integrated-product or the system-test level. The choice of which is application dependent; normally, the system level will be more meaningful to the customer or user.

Risk-driven testing may be conducted at any of the levels, depending on the degree of criticality of the system, but it is probably most meaningful at the system level.

There is one other consideration in testing: Who does the testing? Usually, all unit-level testing is done by the software developer; integration testing is done by a mix of developers and independent testers; and system testing is done by independent testers and perhaps system engineers. Notice, however, that whereas requirements-driven and statistics-driven testing require little knowledge of the internal workings of the software or system under test, structure-driven testing and risk-driven testing require software-intimate knowledge. Therefore, developer involvement in testing may need to be pervasive.

It is popular in some circles to declare testing to be nonrigorous, a concept on its way out. My view of testing is entirely the opposite. Properly done, testing can be rigorous and thorough. It is a matter of knowing how, and doing it. I hope this short discussion may set some testing concepts into proper perspective. Since Code Complete maintains its perennial position on my computer bookshelf, I was curious as to whether Steve McConnell’s After the Gold Rush would make a similarly strong impression.Code Complete is a book that I have referred to again and again for its wisdom about the most basic aspects of quality coding. Naming conventions, cohesion and coupling, layout, commenting, control structures—the book is loaded with discussions of elements both aesthetic and functional which any developer worth his mouse should be aware of. I’ve recommended it to colleagues and loaned out my copy to many junior developers. I think it’s one that everyone in the profession could benefit from reading.

Admittedly, After the Gold Rush is not the same type of book. But it does have similarly powerful advice to dispense. There is plenty of wisdom distributed throughout this book of essays, enough to cause any seasoned software professional to consider the general state of the software development profession. McConnell’s writing style and solid arguments make for enjoyable reading, as well.

McConnell puts the industry at large through the debugger and finds that overuse of acknowledged worst practices (i.e. "code-and-fix development", individuals tackling Herculean programming feats, "Gold Rush" attempts to hurry a product along for market advantage) are largely at fault for the majority of failed projects and bad software churned out yearly. He reasons that, given the accelerated integration of technology into society and the consequent need for effective and reliable software, programming must mature and mold itself into a much more organized and cohesive profession.

McConnell posits Engineering as the ideal model for that professional maturation process, and he makes many solid arguments in its favor. Most of these points are based on the central idea of reusability. This is not just recycling the data access library you wrote for your last project, or getting the most out of that expensive GUI charting widget. McConnell instead refers to reusability in the more general sense of utilizing predictable best practices that can be relied on to produce successful results. Design patterns, configuration management tools and techniques, requirements gathering processes, code reviews, and test cases are all part of the reusable knowledge base available to the software industry. McConnell argues, sometimes seemingly in disbelief, that the majority of the industry is simply not using these well-known and well-defined practices.

His solutions for these ills involve better and more standardized education and a more stringent certification or licensing program for coders entering the profession. He feels that, as in engineering or any number of other professional vocations, software developers should meet certain minimum educational and experiential standards to be fully accredited. Other elements of governance missing from the software domain that exist within other professions, such as quasi-mandatory professional societies (like the American Bar Association for lawyers) and codes of ethics (like the medical profession’s Hippocratic Oath), provide a foundation on which to base a self-imposed set of operating standards.

The reasoning here is very sound at the societal level, if not from the perspective of every single developer currently working. Ask yourself: why is it that programmers, who build technology used in trust by potentially millions of people everyday, should be subject to less licensing and certification than a hairdresser? For me, this point is the crux of the book and the real seed of thought to gain from it. Software is too critical in almost every area of our lives, from balancing checkbooks to maintaining coolant levels in cars to controlling air traffic, to not require some degree of baseline mandated level of expertise.

But here also, in my opinion, is the book’s major weakness. Is engineering really the best model across the board, for all programming efforts? Does every single project require zero fault tolerance and the expense and time that come with attaining that goal? A very solid argument could be made by business software developers against the universal application of certain "best practices," an argument that takes into account the realities of the actual software profession.

For example, in development contexts where requirements and schedules change on a dime, the penalty of an extensive documentation effort outweighs the benefit. To my thinking, the sorts of environments where agility and adaptability are key lend themselves logically to a "craftsmanship" model. There is an academic-style removal from reality in some of McConnell’s analysis—not a lack of realism, but a dismissal of certain business facts that intrude on the desired orderliness of methodical software engineering. Further, many of those "reality vs. ideality" issues may not ring true for entire subsets of the development profession, especially those who program in the more scientific arenas where high-levels of process standardization already exist.

To be sure, some programming is, in fact, rocket science–especially if one happens to work for NASA. Projects in that vein—like modeling the behavior of rockets, or tracking jumbo jets, or precisely welding steel support beams for skyscrapers–mandate the rigorous and heavyweight processes implied by the term software engineering. When things go wrong in projects like these, people may die. But I’m not sure about the necessity of the engineering model in the realm of the vast majority of software development: commerce.

Business software is, as the name implies, for the benefit of business, and as such is frequently initiated, funded, supervised, and monitored by business. But the process of constructing business software is also finagled, interfered with, undercut, unreasonably accelerated, and generally completely misunderstood by the very entities that rely on it. This is because many companies are driven completely (and often unreasonably) by short-term monetary concerns, and consequently are totally at odds with the kind of methodical and repeatable processes which McConnell proposes to take the software profession into its next phase of growth.

It seems to me that mandating some basic engineering education for our non-technical business professionals (who, by being immersed in technology daily are becoming more and more technical) might be at least as important as tightening up programmer education. Software developers are often fully aware of compromises in quality, and they also know where the implicit and explicit bad software decisions are chiefly made: in the business domain.

Certainly, there has to be a middle ground between the absolute chaos of many business software projects and the tight control of a NASA software effort. But this avoidance of the "in-the-trenches" reality of programming is a weakness of the book. McConnell’s reasoning is not flawed, nor is it too impractical, and many of the suggestions he makes are very valid and would improve the success rate of the entire industry.

But in the majority of software development environments, true professional programmers are not empowered to make the decisions required to bring those suggestions to fruition. If the non-technical business professionals in charge are to understand and promote maturation of the software development field, they need to be schooled a little in the subject themselves, with the focus on quality rather than cost-cutting. What’s more, progress seems to be marching backwards as the commoditization of development skills spreads, reducing short-term cost at the expense of long-term quality. Until that trend changes, I predict roadblocks towards the kind of professional growth McConnell promotes here.

Perhaps I also felt a little badgered by the refrain, that the bulk of us really should be licensed and official to be programmers. That seems like a good idea on the surface, but intuitively I have my doubts. After all, you can still get a bad haircut from a licensed cosmetologist, and with the dearth of qualified developers still projected to extend far into the next decade, why winnow the field so drastically?

In any case, After the Gold Rush has a place on my shelf, but I’m not sure between which two other tomes. I benefited from reading it, but not quite to the degree that I had hoped. If you have been in the software game awhile, it’s worth reading for some perspective about what you’ve seen and done—and for some ideas about how things might get a little better. Are software developers simply bad writers?

The prevalent belief that software developers cannot write is a sweeping generalization that is both offensive and untrue. That’s not to say, however, that software developers haven’t produced some shoddy work and slapped the label documentation on it.
Why is documentation so bad?

Why does the industry have such a horrendous track record where documentation is concerned? The problem is a vicious cycle that goes something like this: (1) Documentation is a long-winded, poorly organized, outdated, erroneous pile of worthless, tree-killing drivel that no one actually takes the time to read. (2) Believing that any effort expended in writing documentation that will not be read is an exercise in futility, the unfortunate soul assigned to write it expends as little effort as possible, opting to impress management (who won’t read it anyway) with quantity (since quality might require more work), resulting in worthless, tree-killing drivel. And the beat goes on.

This pattern is compounded by the reality that in many small shops, the software developer is a one-man band, pumping out a rhythmic stream of code while simultaneously wheezing in his testing juice harp, and finally, banging together an after-thought of documentation, like symbols clanging between his knees. Individually, he could play each role with elegance and grace. Ask him to do it all, and documentation will almost certainly rank below working code as a concern. In fact, it barely generates a blip on the "I care" meter.

In larger, well-funded organizations, the response is to employ professional technical writers, who produce slick-looking documents. However (and I think I am qualified to say this, having a master’s degree in technical communication as well as being a software developer), the bigger the gap between the knowledge source and the writer, the higher the probability of producing useless documentation. (This is less true for end-user manuals that benefit from the fresh, unspoiled perspective that a technical writer can bring.)

Thus, in the software development food chain documentation is the murky green fungus that grows on inside of the development tank, sucking the oxygen from unwitting developers who are forced to write it or, worse yet, read it.

Rüping puts it a tad more elegantly, quoting Jerry Weinberg from The Psychology of Computer Programming, "Documentation is the castor oil of programming. Managers think it is good for programmers and programmers hate it!"
The Agile Answer: Minimum Necessary

In Agile Documentation, Rüping gets to the heart of the documentation dilemma, offering a two-word solution (with a few extra words of helpful explanation): minimum necessary.

Useful documentation has its place, but it should be succinct, worded simply, and presented well. That’s where Rüping’s book comes in, laying the foundation with elegant simplicity. The book itself is a testament to the layout the author endorses, with strategic use of white space, helpful margin sub-headings and a few useful examples drawn from credible project experience. Issues are laid out in readable fashion, broken down into sections for Problem, Forces, Solution, and Discussion, making the book suitable for flip-through scanning. It’s All About Quality

For many software developers, Rüping’s guidelines will seem surprisingly familiar, reinforcing the notion that for the most part, we recognize quality documentation when we see it. We simply need to pay attention to the details and apply the discipline necessary to produce the same high quality we demand from our software products. Rüping again quotes Gerald Weinberg, saying, "The value of documentation is only to be realized if the documentation is well done. If it is poorly done, it will be worse than no documentation at all."

Finally, Rüping includes a healthy-sized chapter on Management and Quality Assurance, highlighting two critical steps that are often dismissed in the glee of typing the final word on the page: review and marketing.

Stakeholders must be given the opportunity to review and provide helpful feedback—an activity that is more fruitful when a "positive review culture" has been fostered, with both author and reviewer understanding and being open to their respective roles.

Still feel like that technical masterpiece is going to languish in unread obscurity like the half-finished novel in your bureau drawer? Rüping’s got it covered in his discussion of the "information marketplace." Again, even though "give it to the target audience" or "post it on a bulletin board" seem like "Well, duh" suggestions, Rüping knows (like we do, if we stop and think about it), these obvious measures are often not done.
Is Documentation All Bad?

Documentation isn’t bad. But bad documentation is terrible. Follow Rüping’s advice in Agile Documentation and cut the fat from your documentation diet. Automating repetitive procedures can provide real value to software development projects. In this article, we will explore the value of and barriers to automation and provide some guidance for automating aspects of the development process.

Although few experienced developers and project managers would argue the merits of automating development and testing procedures, when push comes to shove many teams place a low priority on implementing automated processes. The result is usually that, if automation is considered at all, it is given lip service early in the project life cycle, but falls quickly by the wayside.

Experience teaches us over and over again that trying to run a "simple" project by implementing a series of "simple" manual protocols, backed by "simple" written (sometimes, even just verbal) instructions, just doesn’t work well. Even so, many of us still tend to allow ourselves to start the next project with the thought that the manual, protocol-based method will "do just fine."

After all, aren’t we all professionals? Can’t we read a set of simple instructions and just be disciplined enough to follow those instructions when the time is right? Isn’t it a waste of time and money to invest in automating procedures for such a small project? The development team is only a half-dozen people after all—and the argument against automation goes on and on.

If you’re a developer or tester who enjoys spending your time actually adding value to your project, rather than repeating the same routine tasks over and over, you’ll want to consider advocating the concept of automation to your team (especially, to your project manager). If you’re a project manager who’s committed to maximizing the talents and time of the members of your technical team, as well as minimizing the risk of your project failing to deliver on time and on quality, you will want to encourage your team to invest the necessary time and effort required to automate the types of tasks that will be identified in this article.
Why Should I Automate?

You may already be familiar with many of the benefits of automating development processes. Some of the more commonly cited ones are:

Repeatability. Scripts can be repeated, and, unless your computer is having a particularly bad day, you can be reasonably certain that the same instructions will be executed in the same order each time the same script is run.

Reliability. Scripts reduce chances for human error.

Efficiency. Automated tasks will often be faster than the same task performed manually. (Some people might question whether gains in efficiency are typical, noting that they have worked on projects where, in their view, trying to automate tasks actually cost the project more time than it saved. Depending on the situation, this may be a real concern. In addition, automation might have been implemented poorly or carried too far on some projects—but keep reading for more on what to automate, and when.)

Testing. Scripted processes undergo testing throughout the development cycle, in much the same way the system code does. This greatly improves chances for successful process execution as the project progresses. Automated scripts eventually represent a mature, proven set of repeatable processes.

Versioning. Scripts are artifacts that can be placed under version control. With manual processes, the only artifacts that can be versioned and tracked are procedure documents. Versioning of human beings—the other factor in the manual process equation—is unfortunately not supported by your typical source control system.

Leverage. Another big benefit to automating is that developers and testers can focus on the areas where they add real value to a project—developing and testing new code and features—instead of worrying about the underlying development infrastructure issues.

For example, instead of requiring everyone to become intimately familiar with all the little nuances of the build procedure, you can have one person focus on automating the build and have that person provide the team with a greatly simplified method of building, hopefully as simple as running a command or two. Less time spent on builds leaves more time for the tasks that add the most value to the project.
What Should I Automate?

If you’re convinced that automating your development processes is a good idea, the next logical question is which processes should be automated. While the answer, to some extent, is different for every project, there are some obvious ones, as well as some general guidelines that I’d like to offer. Some of the typical targets for automation are:

    Build and deployment of the system under design.
    Unit test execution and report generation.
    Code coverage report generation.
    Functional test execution and report generation.
    Load test execution and report generation.
    Code quality metrics report generation.
    Coding conventions report generation.

The above list is obviously not exhaustive, and every project has its own unique characteristics. Here’s a general, and perhaps obvious, rule of thumb to help identify any process that should be considered for automation: Consider automating processes that you expect to be repeated frequently throughout a system’s life cycle. The more often the procedures will be repeated, the higher the value of automating them.

Once a process has been identified, spend a little time investigating how you might be able to automate the process, including researching tools that could assist with automation, and estimating the level of effort required to implement the automation versus the total cost and risk of requiring team members to manually perform the procedures. As with any other business decision, it really should come down to a cost versus benefit analysis.

You probably noticed that the term "report generation" appears in the above list of automation candidates. The repetition points out another important aspect of automating development processes: the end result of every automated process execution should be a report that is easily interpreted by the team. Ideally, such reports will focus on making anomalous conditions (for example, test failures) obvious at a glance. Also, these reports should be readily accessible to the appropriate team members.

In many situations, it’s even a good idea to "push" reports to the team (perhaps via email or RSS), instead of requiring people to remember to go out and search for them. The basic idea is that the development team should be notified as soon as possible when problems are introduced to the system or environment. A side benefit is that management is provided a more tangible real-time view into the progress and health of the project than is possible with team status reports alone.
When Should I Automate?

Consider automation as soon as you realize that team members are starting to execute a process that meets the criteria discussed in the previous section. For example, automating the build process when the project is nearly over provides very little benefit. It can, however, save dozens, if not hundreds of hours when automated as soon as development begins.

However, before you go off and start automating everything, one caution: be reasonably certain that the procedure will be required for your project, that it will be repeated more than two or three times during the project’s life cycle, and that you understand the steps that need to be executed for the procedure. There are some things you just don’t do very often, and in these cases the costs may outweigh the benefits.

Even for processes where automation is warranted, if you’re truly guessing at the steps involved, there’s a high likelihood that you’ll end up re-writing the whole thing. Re-writing is much different than fine-tuning or refactoring the automated scripts. Refactoring is expected, but scrapping and starting over again means you tried to automate before you understood the process you were trying to automate.

Another danger is allowing your project to become the "proving ground" for automation for the entire organization. If your organization doesn’t already have the infrastructure in place to support development automation, you should provide what makes sense for your project, but try not to allow your team to lose sight of your project’s goals. A project team whose goal is to deliver working software is in no position to develop enterprise-wide strategies and tools. Trying to do so is asking for trouble.

If outside entities begin to get involved in these efforts, I’d suggest that you recommend that a separate project be undertaken to work out the automation infrastructure for the enterprise. Members of this "automation project" team are free to review your project’s implementation for ideas or for use as a launching point. Once this infrastructure is in place, the question of When to automate? is easier to answer for future projects, and less costly.
Obstacles to Automation

If there are so many benefits to automation, why don’t we see more of it on software projects? All software development teams struggle to balance the need to show immediate results with the long-term goals of the project. There are many obstacles to implementation of development process automation. Here are a few of the more common ones:

    Stakeholder pressure. Faced with the desire to show early and rapid progress, teams often overlook or choose to ignore practices that do not seem, at first glance, to directly contribute to getting working code up and running as quickly as possible.
    Focus on core requirements. The team has an overwhelming desire to begin producing working code and tests as soon as possible. Writing code that can be demonstrated is much more satisfying to developers than writing automation scripts.
    Lack of management support. Management may not have a good understanding of automation—how it works and/or the costs and benefits.
    Lack of organizational support. There is no enterprise-wide policy or infrastructure for development automation (standards, tools, expertise, etc.)
    Lack of follow through. Even with the best of intentions, teams can quickly lose their commitment to plans for implementing automated processes.

As with any fundamental change, there must be someone who’s both committed to the concept and who has the authority to make sure that teams follow through with the plan. Not following through is much worse than never having taken the time to investigate and discuss automation. First of all, there are probably other problems that you could have successfully solved using the time and resources and, second, if you fail to implement any automation, when the idea surfaces again people in the organization will point out that it didn’t work the "last time we tried it".
Selling Automation

As implied in the preceding section, the primary obstacle to automation is short-term thinking. Therefore, your primary task is to get your team and management to pause to think about overall project costs and schedule, instead of focusing solely on meeting next week’s promised deliverables. The message should be clear. Meeting milestones is critical, but it’s very possible that you could meet that next milestone to the detriment of the project. But before you try to convince people who are under a great deal of pressure that it sometimes makes sense to slow down momentarily in order to improve the chances for success, you’d better have more than a set of maxims at hand.

This is where Return On Investment (ROI) can help. Most people in the business world, regardless of field, have at least a basic understanding of ROI and agree that, when based on valid assumptions, it can be one of the best mechanisms for evaluating whether or not to take action or adopt a particular solution. The process of calculating ROI is well beyond the scope of this series of articles. However, the Suggested Reading section at the end of the article provides a link to an outstanding article from Edward Adams that describes how to do this, as well as some additional advice on selling the case for test automation, in a very straightforward and easy to understand way.

Although many managers really like to see quantifiable benefits, such as ROI, before making major decisions about adding tasks to the project schedule, others have sufficient experience with software development and are just as comfortable with a common-sense explanation of automation’s benefits. With that in mind, rather than going into a full-blown ROI calculation, let’s just take a look at one scenario that most projects will face at some point to see, from a common sense perspective, the likely outcome of a purely manual process versus a process that had been automated early on in the project.

Manual testing approach. Your team is in the final month of a twelve-month project, and the system test team is starting to perform regression testing by manually re-running the entire set of test cases. Some test cases haven’t been executed in several months, because, frankly, the testers have just been spread too thin, and, due to perceived pressure to deliver, management chose to move testers onto other tasks as soon as they were finished wrapping up a set of test cases.

Regular regression testing was put into the schedule early on, but repeating the same tests over and over throughout the project was (whether or not anyone wants to admit it) eventually considered a "nice to have" and definitely much less critical than other, more pressing tasks. Now some of the regression tests are failing and the test team is logging defect reports.

The development team decides to manually re-run the entire unit test suite (which also hasn’t been executed in a few months, because it wasn’t automated) to hopefully zero in on the source of the problem. The developers find two problems as a result of the unit test run: first, apparently there were never any unit tests written for the parts of the system in question; and second, there are other, seemingly non-related unit tests that are failing. To make things worse, the components that are failing were developed by someone who’s no longer with the team. Even if he were available, he’d probably require significant spin-up time to re-acquaint himself with the failing code, since he hasn’t even looked at it for months.

Automated testing approach: Early on in the project, the decision was made to automate at least the following: unit testing, code coverage, and system testing. The same bug that caused the situation above was introduced in month three of the project. The code coverage report that’s automatically generated as part of the daily build clearly indicated to the development team that there were no unit tests for the related components as soon as those components were placed under source control. Therefore, the developer went back and immediately implemented the unit tests.

After implementing the unit tests and working out the bugs that were initially revealed as a result, the developer checks the unit tests into source control. The daily build automatically picks up the new unit tests, the coverage report reflects that the components are now being tested (as well as to what extent they’re being tested), and the unit test report indicates test success or failure each day. As soon as any change is made to the system and placed under source control, the team will become aware of the impact that change has on the overall system, while the changes are still fresh in the minds of the implementer.

The system tester who’s responsible for testing that same set of features now designs his test case, implements the tests, reports any defects he discovers, etc. until he feels that the code and test case are operating as expected. He then uses the test automation tools to record the execution steps for the test case and checks the resulting scripts and files into source control. The next day, the automated test runner picks up his new test case for execution and the results become available to the test team as part of the daily test run. The development and test teams are leveraging the automated tools and processes for proactive detection as problems are introduced.

Without performing a detailed ROI analysis, which of the two above scenarios is intuitively more desirable? Assuming that the exact same bug is introduced into the system at the same time—month three of the project—in each case, which approach do you think will result in the least expense and least risk? Which will result in the least stress to the team and to management?
Start Small

Unless you’re lucky enough to have a team with multiple members who’ve had a good deal of experience automating development processes, you won’t want to try to take the idea of automation to the extreme. Trying to do so with an inexperienced team or a team that’s not completely sold on the idea of automation will likely result in failure, as both expectations and barriers are set high from the start. Pick one or two areas where you believe there is both high potential value for automation and a low risk that the team will fail implementing automation. You might ask yourself the following questions:

    Which processes will likely be exercised the most frequently?
    Does someone on the team have the experience and/or skill set to implement the automated process?

It’s better to succeed in a small way than to fail in a big way. Your successes just might build on themselves, and when you suggest additional areas for automation, either later in the project or on your next project, you may face little or no resistance at all. To reach the ideal takes a long-term plan, management commitment, adequate tools and training, and of course time. If it were easy, everyone would already be doing it.
Conclusion

I’m well aware that there may be other factors that affect the decision of whether and what to automate than those we’ve addressed in this article. For instance, you may find that the only automation tools your organization will approve for your team’s use are beyond your budget or that, due to your project’s special needs, there are more important factors. Maybe you’re a vendor and your marketing department has determined that time-to-market for your first release is more critical than the quality or the long-term success of the current product.

I’m a big believer in the "it’s all about trade offs" principle. To gain one advantage, you typically accept other disadvantages, and in the end you should try to make decisions that tip the scales most in favor of the "advantages." I would say, however, that if your project is truly one of those projects that is better off not automating, you’re in the minority category—the exception, not the rule. If the primary obstacle to adoption of automation for your organization is lack or expense of tools, there are several free open source tools that can go a long way towards solving that problem. The time you spend investigating automation strategies and tools could very well make the difference in whether your projects successfully meet schedule and quality goals. Over the last decade there has been a lot of effort put into object/relational mapping, which refers to techniques for resolving the mismatches between the object-oriented world, with its encapsulation of data and behavior, and the relational world, with its tables and columns. There’s not only a difference in terms of data types (and complexity of these data types) but also in terms of relationship types. The object world has a variety of relationships (aggregation, composition, association, inheritance) which cannot be mapped directly to the database world.

The general topic of object/relational (O/R) mapping can be divided into two areas of concern: the mapping itself and the persistence API. The persistence API not only acts as an indirection layer for the database, but also hides the mechanics of mapping the objects to the database tables. A good persistence API should do this while not constraining the object modeling in terms of data types and relationships.

In this article I will begin with a discussion of home-grown vs. off-the-shelf persistence solutions, including areas to consider when deciding between the two, and advice for choosing the best off-the-shelf solution to meet your needs. I will also share suggestions and advice from my own experiences with O/R mapping and persistence APIs. It is not my intention to explain all of the background details of these topics, but to focus on "best practices."
Home-Grown Mapping

I have been directly involved with both home-grown (custom built) and commercial-off-the-shelf (COTS) mappers, and have also observed several other home-grown mapper implementations, each approaching the subject in a specific way. In my experience, the primary drawback of "rolling your own" persistence mapping implementation is that limited resources do not allow for enough time to think everything through, to improve the framework over time, and to backtrack if you realize that design changes are needed.

Because an O/R mapper is a generic piece of software, it is typically hard to explicitly list which aspects are of the most importance to you (and if you build your own, you will not be able to focus on all of them). I do not mean to say that you could not envision a good design, but that it would take a lot of time and effort to fully implement a solution that meets all of your needs.

I observed the following limitations in just one home-grown O/R mapper:

    It provided no support for association relationships; only containment relationships were supported. This was a serious constraint when defining the object model.
    It offered no support for transactions.
    It only supported one RDBMS.
    The API was not type safe, which caused a lot of errors that could only be detected at run time.
    Testing of the O/R mapper was underestimated. Not only were there a lot of possible paths through the code, there were also stability and performance issues to be considered.
    The designer had to create and maintain the object model by editing a plain text file without any special editor. I know the saying "a fool with a tool is still a fool," but not being able to visualize one’s own object model is like walking around in the dark—you don’t see where you are heading and you have difficulties in pointing the others in the correct direction. In this case, the object model, which was like a red ribbon winding through the whole architecture, could not be clearly communicated to the team.

I hope these examples are enough to help you to avoid stepping into a home-grown solution. Of course, a home grown O/R mapping project would be fun to do from a development point of view, but a COTS (commercial-off-the-shelf) O/R mapper will be cheaper—unless you consider O/R mapping as one of the core competencies of your business.
Selecting a COTS O/R Mapper

Below I list some criteria you might want to consider when selecting an O/R mapper.

    Consider whether the tool will restrict your modeling freedom too much. For example, many tools don’t support relationships on abstract classes. A workaround for this is to duplicate relationships on concrete classes, which is less ‘OO’, but works for these tools.
    Consider whether the O/R mapper that allows you to model visually (preferably using UML).
    If UML is important to you, ensure that you can either import UML into the O/R mapper, or that you can export UML from the O/R mapper.
    Take a close look at the programming model the O/R mapper imposes and see whether it is compatible with the things you want to get out of it.
    Look at the range of mapping possibilities to ensure that the kinds of relationships you envision between your objects and tables will be supported. Typically, most O/R mappers support a large range of features, but not every mapper supports every type of relationship.
    Assess the performance, even if you think you do not have a lot of performance demands. Testing the performance can also give you a chance to learn and assess the API.
    If you prefer or need to start with an existing database schema and then map objects onto it, assess whether the O/R mapper supports the database system you want to use.

Selecting a Persistence API

The O/R mapping features are only part of the story. The other part of the story is the selection of a good API for persisting objects, and this part has a lot more visibility to your development team than the O/R mapping part. While the O/R mapping functionality will only be exposed to a few team members who are dedicated to maintaining the persistence layer, the persistence API defines the interface that the whole development team will use.

Persistence APIs can be divided into two categories: transparent and non-transparent.
Transparent Persistence APIs

A transparent persistence API hides the persistence completely. A transparent API does not need to have a lot of methods; a load and a save method is sufficient most of the time. Typically, a lot is defined declaratively instead of procedurally. Hibernate and JDO are examples of transparent persistence APIs. Let me clarify with an example:

An Insurance object can contain 0-n Warranty objects. The client application updates an attribute of an Insurance object. Semantically a Warranty is contained by an Insurance, so when you update an Insurance it is possible that you implicitly update its Warranties. According to the requirements, this might be a correct design, but it could have a negative impact on performance, especially if I am not aware that the implicit update of the Warranty objects is happening. When I only have modified an attribute of the Insurance object, I should be able to limit the persistence manager to this functionality.

The beauty of this is the "magic" way in which the persistence manager knows what to do. The negative side is that the persistence manager is thinking in your place.
Non-Transparent Persistence APIs

A non-transparent persistence API has a lot less "magic" inside of it. When compared to a transparent persistence API, it has a rich API, offering a lot of control to the user of the API.

Consider a transparent persistence API with a single method Persist(). This persistence method does all the magic behind the scenes, like checking whether there are associations that potentially need to be persisted as well. Although this might sound attractive, when selecting a persistence API, ensure that you can optimize. When touching associations, it is possible that the associated objects haven’t been persisted yet. What should the Persist() method do? Persist those objects first? I say that it is better to put the client in control.

To illustrate the power of a non-transparent persistence API, I’ll use the SimpleORM API as an example. Consider an insurance class with two methods. The first method will explicitly load all of the children linked to this object:

insurance.getAllChildren(insurance.Warranties)

The second method will only list those items that were added to the object in memory and the ones that are already retrieved from the database:

insurance.getRetrievedChildren(insurance.Warranties)

The advantage here is that the user of the API can "see" what he is doing, and can make better decisions regarding performance costs. (Also during code reviews this visibility can make life a lot easier.) In contrast, due to the fact that a transparent persistence API has a very generic interface (e.g. Save() and Load()), it also creates the illusion that database actions are cheap.

The cost for the non-transparent API is that the interface is more complex than a transparent persistence API. However, if you are planning to write your own persistence API, I recommend a non-transparent persistence API.

It is not my intention to classify all transparent persistence APIs as "do not use." But if you are considering a transparent persistence API, I would advise you to assess its performance carefully.

In case you are not satisfied with the persistence API that is provided to you, you can also decide to wrap it such that it maps onto your needs. In the next section, I’ll elaborate on some good reasons to wrap a COTS persistence API.
Wrapping a COTS O/R Mapper’s Persistence API

After you have selected a COTS O/R mapper, you should also decide whether to use its persistence API directly or to wrap it. Good reasons for wrapping the O/R mapper’s persistence API are:

    You want to add some extra logic (for example, field validation is not part of the O/R mapper, but you want it to be an inherent part of the persistent objects).
    Some O/R mappers generate code for the persistent objects, others expose a generic API. Typically, the generated persistent objects are type safe, while the generic API is not. Type safety is a good reason for wrapping the O/R mapper’s persistence API.
    Apply the subsystem principle: you want to avoid a tight coupling with a specific O/R mapper. Therefore you treat it as a subsystem and work interface-based.
    You want to limit the features that your clients can use. For example, your mapper might support the use of direct SQL while you don’t want to expose such a feature.
    You might want to expose certain services in a different way. For example, you might want to introduce a query object rather than to expose an OQL query interface.

Of course you need to place everything in perspective: if you are creating a throw away application, you don’t need to worry about aspects such as maintainability, extensibility, and resilience. For strategic applications however (commercial software products, product families, core business applications, etc.) you really should spend some time evaluating the pros and cons of different approaches.

A bad reason to wrap a persistence API is that you think that you will be able to boost performance. In general, you won’t be able to do this because the performance is inherent to the internal design of the COTS component. Only in rare occasions you will be able to turn the performance to your advantage by wrapping the COTS component.

A better way to approach performance is to invest some time in seeking a usage pattern that is optimal for your situation.
Wrapping and Object Management

Another way to discriminate between persistence APIs is the way objects are managed.

An advantage of this approach is that the manager always knows the current state of the object (new, retrieved, already saved). The manager can use this information to its advantage, resulting in good performance. Several years ago, when I was just getting started as a software developer, I read Steve McConnell's Code Complete all the way through, cover to cover. Then I went back and read several sections again. There is no question in my mind that as a result I immediately became a far better developer than I was at that time-not only that, my entire mindset was changed. Since then, I've returned to the book countless times, reading many parts of it again and again. I've recommended it to all the aspiring or less experienced developers with whom I've had the pleasure to work. Every time I interview a candidate for hire, I want to know whether he or she has read this book.

Why do I consider this such an important book for developers? Here are a few reasons:

1) Code Complete focuses on the absolute fundamentals of programming-in other words, the little details. By this I don't mean that the book is a programming instruction manual. It does assume that you know the absolute basics of variables, data types, branching, looping, etc. Beyond that basic assumption, though, McConnell dives into the how and why of fundamental concepts such as data and routine naming, when and how to decide to break code into routines and modules, indenting, declaring variables, appropriate and tactical use of data types, properly writing If statements and loops, named constants, commenting, layout and style, etc, etc. The book also goes into detail on higher level, yet still fundamental, topics such as unit testing, debugging, tuning, and delivery. Sadly, these are fundamental topics in which too many developers never get any real guidance or instruction.

2) Almost as important as this instruction in the critical techniques and best practices of writing code is the message that McConnell weaves throughout the book: quality matters, and as the person writing the code, you have a personal responsibility to everyone affected by your code to do good, high quality work. This is the single most important characteristic I look for when hiring developers: I want to know if a developer cares about quality, about doing things "the right way." The message of quality goes hand-in-hand with McConnell's focus on "the little things." My only complaint in this area: Chapter 31, titled "Personal Character," should have been Chapter 1. (Then again, maybe McConnell knew what he was doing by not hitting people over the head with that one right away.)

3) Many developers, when they are first starting out, work by themselves. If the only audience for your code is yourself, then it might not occur to you to be concerned with exactly how to indent a nested loop or how to write good comments. McConnell does a great service for developers in this situation by shining a light on what it is like to work with a team of programmers who share code and responsibilities such as testing and documentation.

4) Looking back, one of the most important benefits that I received from Code Complete is a glimpse into the history of software development, which inspired me to read further and become a student of the discipline and craft of building software. McConnell does not simply dispense with a lot of advice and expect you to take his word for it. Code Complete is a distillation of thousands of pages of books, academic papers, and studies, a fact which McConnell makes quite explicit. He repeatedly refers directly to these primary sources. This not only reinforces his points, but also introduces the reader into a wider world. The book's last chapter, called "Where to Go for More Information" is full of great ideas for further reading.

It's interesting to go to Amazon.com and read the 100+ reviews of this book. The vast majority of them are glowing five star reviews. Many of the less favorable reviews criticize the fact that, since Code Complete was published in 1993, it does not discuss the issues of software construction in terms of the object oriented languages and n-tier architectures that are popular today. If one wanted to look for them, it would be easy to find ideas and conclusions in the book that seem dated. It would be easy to get hung up on the fact that many of the examples are written in languages like Pascal, C, C++, Ada, Modula-2, Fortran, and Basic.

As you might guess, I don't buy the argument that Code Complete is any more than a little out of date. While I agree that it would be nice if McConnell would write an updated version of the book, my guess is that the reason he has not done so is that it would take hours of work, and probably only increase the value of the book by 5% or so. Yeah, it would be cool if the examples were in Java, C#, or Ruby, but the languages are totally beside the point. And yes, the book would be improved if the discussions of modules and routines also discussed objects and methods, but the exact language-specific implementation details of the principles of modularization are also beside the point.

Let's look at my argument in a different way. I see three audiences for Code Complete:

    Beginners
    People who have written some programs but who are really just hacking at it, with no sense of stylistic purpose or conscious technique
    Seasoned programmers who like to sharpen their skills and refine their style, or who are looking for a book to recommend to those in the first two audiences

The first two groups are the primary audience of Code Complete, and a book aimed at these two groups would totally fail if it turned on the fire hose and said to the reader, "Open wide!" The people in these groups are primarily building their first ASP web sites, simple desktop utilities, scripts, and basic database applications. With a firm grounding in the fundamentals, and with more experience and learning behind them, newer developers can eventually get to the point where they are ready to balance cohesion, coupling, protected variations, separation of concerns, performance, public vs. private interfaces, exception handling, transactions, workflow, persistence, object/relational mapping, and security, not to mention domain-specific concerns like cross-process marshalling, threads, memory management, etc. McConnell does the right thing by keeping the focus of Code Complete on the basic building blocks.

If you are a person in those first two groups, you need to buy a copy of this book right now. That's a strong statement, but I stand by it. If you are in that third group, and have not yet read this great book, I recommend it to you for the sheer pleasure it will bring. I’ve been thinking a lot this week about code and documentation—more precisely, I’ve been thinking about code as documentation. The idea of "self-documenting code" is certainly not a new one. When, as a relatively inexperienced developer, I first read about self-documenting code in Steve McConnell’s Code Complete, it was a revelation to me. My first real-world experience with programming was maintaining and debugging someone else’s horrible code, and I knew intuitively from that trial by fire how important it is to make one’s code clear and understandable. From that experience, I started concentrating on code-level comments as the key to making sure that code is properly documented and can be easily understood by others.

But McConnell’s Code Complete really opened my eyes. Self-documenting code, he writes, is "the Holy Grail of legibility...Such code relies on good programming style to carry the greater part of the documentation burden. In well-written code, comments are the icing on the readability cake." (page 456) While reading the 450 pages leading up to this statement, I began to see how writing good quality code is really a matter of making thousands of tiny decisions along the way. Quality code does not happen by accident. It really takes true desire in the heart of the developer, because at each of those thousands of tiny decision points, there are so many opportunities for shortcuts, and so many opportunities to do something that reduces the quality of the code.

The key to achieving self-documenting code is what McConnell calls "good programming style." To achieve this "Holy Grail of legibility," it really takes constant attention, and an ongoing dialog with myself as I work. I don’t know that I’ll ever achieve it, but I’m always trying. I try to approach the writing of code as if I were making art. In my mind, the metaphor of the musician is most apt, since the musician must work within the mathematical confines of his medium, but at the same time can lend so much personal style to the end product.

Architecture is another useful analogy. The architect designing a building must obviously work within the confines of physics, engineering principles, and municipal building codes, but people pay huge premiums for the best architects, who infuse the building design with their unique stylistic statements.

But is self-documenting code enough? Clearly not, because the code itself can never capture the intent of the developer writing the code. The code documents the solution, but knowledge of the problem can only be inferred. This is where comments come in. Here is a quote from my description of The Principle of Comments:

    The code is the solution to the problem being solved. Ideally, the code should speak for itself. A person can read the code, and if it's good code, should be able to readily see what the code is doing and how it's doing it. However, what is lost over time is what was in the mind of the developer who wrote the code. In general, this is what needs to commented on. What was the intent of the developer? How is this code intended to be used? How does this code intend to solve the problem at hand? What was the idea behind the code? How does this code explicitly interrelate to other parts of the code? One of the greatest sins a developer can commit is to leave his code without his intent being clear to subsequent readers of the code.

What I try to capture most in my comments is to explain, at a high level, what my code is doing. I try to illustrate what my "scheme" is—how it all interconnects and works. I try to picture myself in the position of the maintenance developer, coming to my code for the first time, trying to figure out how it all works. What would he want to know about this code? I try to picture myself looking over the shoulder of that maintenance developer. How would I explain this code to him? What’s not immediately obvious? What would his questions be? How would I explain to him why I did it this way? The answers to these questions are what I put in my "at the level of intent" comments.

There is a lot of controversy today over which methodology or "family" of methodologies is the best one (the truth is that different methodologies are better or worse in different circumstances), but the larger issues of process and methodology fall away when a developer is by herself, at the computer, writing code. The writing of code is a very personal act. It’s just the coder and the code, alone at the computer. The opportunity exists for the developer (or pair of developers) to stamp his or her own unique signature and style onto the code, just as the musician or architect does.

However, while the desire to develop a truly advanced coding style is essential, it is not enough. The other essential part of the equation is knowledge. Can the musician produce beautiful music without knowledge of the rules of music? Can the architect design a beautiful building without knowledge of the laws of physics and the principles of structural engineering? Can we as developers produce beautiful code without the knowledge of what it takes to accomplish that?

In order to develop a style that leads to beautiful, self-documenting code (as if the two could be separated), in order to make the best choice in those thousands of tiny decisions—indeed, in order to even recognize the opportunity to make those decisions—we must have the knowledge of where the potential trouble spots are. We must know the techniques and conventions and standards. How can we choose the best data and routine names or use ideal layout techniques if we’ve never learned what the programmers of the last forty years have learned by trial and error? How can we properly modularize a system and make it tolerant to change if we are not intimate with the concepts of cohesion and coupling?

If one has that true desire to make one’s code the best it can be, one must acquire the knowledge of what works, and what doesn’t. One must learn the situational subtleties that go into making the best choice in those thousands of tiny decisions that a developer makes when writing a system. Choice A might be the best choice in Situation 1, but in Situation 2, Choice B might be preferable. The rules and conventions are not necessarily absolutes. But if one does not know the rules and conventions, one cannot know when to adjust them to the situation. This is another way of phrasing the old adage, "First master the rules, then you can discern when to bend and break them."

(One must also master the peculiarities of the language being used, even if that language is not the developer’s favorite. Each language is different, and has its own conventions for naming, layout, exception handling, etc. We should try to respect these differences, and not force the conventions of another language onto the one we are writing in today. Too many developers learn a style (good or bad) with their first language, and then impose that style on every other language they use in the future.)

If one desires to develop his or her own unique style, reading lots of great books, articles, and essays is an excellent first step. Make reading a continuing activity.

The next step would be practice, since the code editor and IDE is where theory gets put to the test. Strive to make each program better than the last one you wrote, and then move on to the next one.

Third, read and analyze other people’s programs. If your team is not performing peer-based code reviews, suggest that you start. If you are learning on your own, download code from programming web sites, or look through the code that comes on the CD-ROM with a programming book. You will find plenty of bad code, and, if you’re lucky, some exceptionally good code too.

Finally, if you are able, find a mentor. Identify someone around you that you look up to, whose opinion you respect, and who has the experience you would like to acquire. Ask that person if she will read your code and help you make it better. This can provide amazing leaps forward that might otherwise take months or years—or that might not ever happen at all. The concept of "correctness," as it relates to software, is usually discussed in relation to the known requirements for building or using a particular piece of software. When building software we ask questions like Does this algorithm produce the correct result? or Does the workflow of this application correctly conform to the needs of the user? This is one of the ways we measure the success of our efforts. We also judge correctness when using software tools and platforms, whether to build software or to accomplish other types of tasks. If we don’t use a tool correctly, then we likely won’t get the result we want. For example, if the syntax of the code is not correct, then the compiler will not compile it.

However, in this essay, I will be using the term correctness in a different way. I would like to discuss correctness as it relates to convention and personal preference. For example, how do you prefer to name variables? What size should a button be on a GUI form? Where should buttons be placed on a GUI form? How should you design compound keys in a database schema? What error handling technique should you use? Should implicit meaning be attached to the null value in database columns? For these types of questions, each of us over time develops our own concepts of correctness.

Sometimes these concepts of correctness come from conventions we have encountered in books or on previous projects. For example, for data and function naming we can choose from the Hungarian convention, camelCase, and PascalCase, to name just a few. These are examples of conventions, which are presumably agreed upon by some group of people, and are often formalized in some way. Concepts of correctness also arise out of pure personal preference. For example, maybe you really hate typing underscore ( _ ) characters, so you don’t use them when naming database tables or named constants. Often, the distinction between convention and preference is unclear, since preferences often mutate from established conventions, and vice versa.

Correctness in this sense, then, is quite relative. Different people in different situations at different times will have different ideas about what is "correct." As individuals, when we create software, or when we look at other people’s software, we are constantly making judgments as to this relative correctness. We judge things like technique, look and feel, format, and style. We compare what we see against what we perceive to be the prevailing convention and against our own preferences—or against our preferred convention, prevailing or not.
Making Changes

Judging someone else’s code according to our own relative correctness measurements is one thing, but what happens when we need to modify that code? At that point, our (often unconscious) concepts of relative correctness can work for or against us. This is the point where we must weigh correctness against consistency. When it comes to convention and personal preference, consistency has a higher value than correctness. At a certain point, our desire for what we believe to be correct becomes a liability. If we selfishly cling to our conventions and preferences, we can taint the things we change by creating inconsistency and incongruence. We might make a change that is correct in terms of the requirements, and correct in terms of our own conventions and preferences, but which creates disharmony because of a lack of consistency with what’s already there.

Let’s look at a quick example to illustrate what I’m talking about.

Bill is a developer of business software who has always worked with Oracle brand databases running on Unix systems. He recently joined a new team that is using Microsoft’s SQL Server database. As a first task, Bill’s boss has assigned him to add a new feature to an application that is already in production. This change will require the addition of two columns to an existing table. After receiving the requirements, he adds the two new columns. He names the first column DT_PURCHASE_DATE and the second column IND_IS_FIRST_ORDER. He updates all the appropriate places in the system code to work with these new columns. He tests his new feature, and it all works great. Everything he has done matches the customer’s requirements exactly. When it goes to production, the users are thrilled with the new feature.

There is one problem, however: all of the other columns and tables throughout this database use a totally different naming convention than the one Bill used for these two new columns. In this particular database, all object names use PascalCase with no underscores. Furthermore, no prefixes like DT_ (which Bill uses for date columns) or IND_ (which he uses for Boolean "indicator" columns) are used anywhere in the database structure. Bill should have used names such as PurchaseDate and IsFirstOrder (assuming, that is, that these two names are consistent with the way that date and Boolean columns are named in this database). Now these new columns stick out on the E/R diagram like a cockroach on a wedding cake.

So what happened here? Why did Bill do this? If we want to give Bill the benefit of the doubt, we could say he was simply not paying attention. We could assume that consistency is not a priority to him. He did things the way he is accustomed to doing them—the way he considers to be "correct."

If we do not want to give Bill the benefit of the doubt, we could say either that he is lazy or that he does not care. If we want to say he is lazy, then we’d have to acknowledge that he knows better, but did not want to go to the trouble of changing his normal way of doing things. But if you follow this logic through, you have to say that he is lazy and that he does not care. In order for him to decide to be lazy, he’d have to notice that he was creating inconsistency. This does not speak well of Bill, for if he knowingly used an inconsistent naming convention, then it appears that he puts his own preferences above the good of the database, application, team, and company. We expect more from a professional.

The trouble for Bill is that people on Bill’s team who encounter Bill’s work on this new feature are going to think less of him. Even if they give him the benefit of the doubt and conclude that he was simply not paying attention, that still is going to lower their estimation of Bill. If Bill continues this behavior in more serious matters, he may find himself out the door, or relegated to meaningless tasks so that he can do less harm. Here’s another thought: maybe this first simple feature that Bill was assigned was a test to see how he would do with it. When the team leader or review team comes behind him to see how he did, this behavior will raise a red flag.
The Cost of Inconsistency

Now, you might be saying at this point, "Come on, Dan, lighten up. All Bill did was use a different naming convention for a couple columns. What’s the big deal?"

To this I would say that the principle illustrated in this seemingly trivial example is critical to our measure of ourselves as professionals. A professional goes out of his or her way to maintain consistency when making changes. However harmless a couple of inconsistently named columns may seem, it represents a real problem. If these are the first two inconsistently named columns in the database, and no one catches this in a review process, and the changes go to production, they may stay there forever.

These inconsistently named columns are a problem for two reasons. The first reason is simple aesthetics. When we create software, the aesthetic value of what we create is important not only because it is pleasurable to look upon, but because it adds real value to what we are building (see The Principle of Aesthetics at developerdotstar.com). Which leads to the second reason: now that the E/R diagram is polluted, it’s a whole lot easier for other developers to add more of their own pollution, and those people probably won’t be consistent with Bill’s inconsistency, so now we’ve got multi-faceted inconsistency. Before long we’ve got a total mess.

I live in a condominium building. We have a common trash dumpster in the back of our building so that residents of the building can throw their trash away. We keep the dumpster locked because if we don’t, the other buildings around ours will fill the dumpster with trash, taking away all the space in the dumpster for the people in our building, who are paying for the dumpster. Whenever I see that one of our considerate neighbors has thrown their bags of trash and boxes and junk on the ground around the dumpster, I pick it up and put it in the dumpster. Why?

Because if I leave it there, then it sends a message to all the people in the neighboring buildings that it’s okay for them to throw their trash on the ground around our dumpster. I have seen this happen over and over, which is why I started picking up the trash as soon as I see it. If the trash stays there, then other people will start piling their trash all around it. If I pick it up right away, then weeks will pass between incidents of people throwing their trash around our dumpster. Trash attracts trash. Inconsistency attracts more inconsistency. (Authors Andy Hunt and Dave Thomas write about a similar principle they call "Fix Broken Windows" in their book The Pragmatic Programmer.)

Going back to the database example, as the inconsistency piles up, the understandability of the database degrades. The barrier of entry for someone new coming to the database goes higher and higher. The likelihood of mistakes goes up, because the inconsistencies make it harder to make assumptions.

For the project on which I am currently working, just about zero attention was paid to consistency for the code and user interface for the first year and a half of the project (this was before I joined the project). What came out as a result is a damn mess. Some user interface forms have buttons on the bottom right, some in the bottom center. Some of the buttons are big, some are small. Some forms have an OK button, others have a Save button, while still others have an Apply button. Within the code, naming conventions are all over the place, as are data access techniques, class designs, etc., etc. Without conscious and conscientious attention to consistency by everyone involved, inconsistency is the inevitable result.
Areas of Concern

So where is the line? When do you decide to make a stand and say "Consistency be damned, this is just the wrong way to do this."? What do you do when you encounter a project like the one I just talked about, one that is already a mess of inconsistency? How do you try to achieve consistency when such a goal seems hopeless? These are tough questions to answer, and I don’t know that we can formulate absolute guidelines. Using our best judgment, these kinds of dilemmas need to be handled differently based on the circumstances. There are probably several factors influencing the outcome, including at the least:

    What the is the future viability of the project/product? If it is definite that the product will be killed or retired in the near future, then maybe consistency is not all that important.
    How much time and resources are available to put effort into clean-up and refactoring activities so that some consistency baseline can be reached?
    What politics are at play in and around the project?
    How much latitude does the management give to the members of the team to implement quality initiatives, which are often viewed by management as non-value-adding?
    What emotional investment do the existing team members have in what they’ve already created, even if what they’ve created is a mess?

When the way is unclear, the key is this: by paying attention and asking explicit questions about consistency, and by giving real thought to the answers, we are more likely to create quality and add value, even our actions result in a little inconsistency. However, if we don’t pay attention, and just barrel in and do it our own way, inconsistency is the likely result, and worse, inconsistency for no good reason at all.

Before wrapping up this part of the discussion, I’d like to briefly list a few areas where consistency often gets neglected:

Adding a new form/window to an existing GUI application. Inconsistency creeps into user interfaces all the time. When multiple developers are working on a GUI application, if there is not a published GUI standard, and if those developers are not consciously making an attempt to preserve consistency, then different parts of the GUI will end up looking and acting different. Even when there is not a published standard, a developer has a responsibility to take a look at what is already there and do her best to match the look, feel, and behavior.

Adding new code to an existing application. Consistency here applies to things such as data and function naming, style, layout, technique, etc. Even when you hate the naming conventions being used in the app (It’s no secret that developers get very attached to their naming conventions.), you have a responsibility to use the existing naming conventions. The same goes for module and routine comment blocks, indenting styles, error handling conventions, data access techniques, etc.

Coming to a new language, or switching between different languages. This is a pet peeve of mine, and could probably be the subject of a whole essay. Too many developers try to impose the conventions, styles, and techniques of one language onto another language. Often the developer will carry the conventions and techniques from his first language for years, trying to impose what he learned about his first language onto every other language he encounters subsequently.

Every language has its own "ways of doing things." Even the same language from platform to platform, context to context, will have unique conventions. In my opinion, when you learn a new language, you have a responsibility to not only learn the syntax of the language, but to also learn the common conventions that people use when they write code in that language. You can do this by reading books, magazines, web sites, and of course, code. For mature languages, you might find different camps of people with ideas and conventions that clash. This is normal. Apply your best judgment, exercise your own personal preferences, and code away.

Writing documentation. If you are creating documentation for a project/product, then do your best to make that documentation look like the rest of the documentation already produced for that project. Are there document templates or published standards? Try to find out. If you find that there are none, maybe you can be the one to create some templates and standards.

Process consistency. If some process is clearly already being followed, make an effort to follow it, even if you have not been fully schooled in it. This is especially important when it comes to change management processes. If you have questions, ask them. Don’t just roll on and do it your own way, because all you are going to do is create work for someone else.

Joining a new team. This is an all-encompassing item. When you join a new team, make an effort to learn what they are doing and try to do things in the same way. The members of the team will welcome you more easily if they see you are paying them the courtesy of trying to learn their customs, standards, and conventions. Granted, an attentive team leader will explicitly introduce you to this information, but that often does not happen when teams are working hard to ship software. So even if you are a little annoyed that no one took the time to give you the information, seek it out for yourself. If you have concerns that something the team is doing is wrong or ineffective, then tactfully bring that up in a constructive way. However, don’t be surprised when the team leaders decide to not change what they are doing, even if they agree that your advice is sound. Inertia is a powerful force.

Starting a brand new project. If you are starting a whole new project from scratch, it makes sense that you would have free reign to create a whole new set of standards and conventions. You should absolutely do this, since, as I already mentioned, if you don’t make an explicit effort towards consistency, inconsistency will inevitably result. In addition to published standards, it’s a great idea to have review processes in place so that inconsistency will not slip through the cracks. However, there might be concerns about consistency between projects at the organizational level. If you are building software for an organization that has multiple systems already in place or in progress, it’s a good idea to check for standards and conventions already in use within the organization and adopt those for your new project.
The Craftsmanship Connection

There are a few keys to preserving consistency: first, you must decide that you care about consistency and that it is your responsibility to do your best to maintain it; second, you must pay attention to "the little details" in all aspects of your work (David Pye calls this "the habit of taking care"(1)); and third, you must possess the humility to subjugate your own concepts of correctness to the higher aim of consistency. Responsibility, pride, and attentiveness (and a little humility) is what software craftsmanship is all about. Quite often, when corporations consider the possibility of doing some or all of their software development offshore (typically in Eastern Europe or India), the decision is made without considering all of the factors necessary to arrive at the best decision.

With companies that already have internal development capability, careful consideration needs to be given to the strategic importance that this capability provides.  Unfortunately, in many companies executive management often misunderstands the process of software development.  They frequently view the software engineering community and its professionals as a management challenge and the product that they produce as a commodity without regard to the value proposition that this capability can deliver.

The promise of lower hourly rates with offshore development is typically the "public" justification given for considering the possibility of going offshore, but often the true financial comparison is not given much more consideration than this simple comparison.  This promise, combined with the desire to unburden the organization of internal development groups, forms the basis for sending the work offshore.

However, deeper consideration can often show that the perceived advantage quite often is not realized and that the organization could suffer in the long haul.  The strategic importance of the software product being developed internally can often dwarf the perceived short term savings of a lower hourly rate.  In addition, frequently the lower offshore rate does not result in lower overall development costs.

I recently did an analysis for a company that had multiple in-house development groups and one group that used offshore development exclusively.  The organization claimed that the cost savings realized going offshore was substantial, based on the fact that the offshore company charged about $18 per hour, while the in-house groups had a "rate" calculated at over $60 per hour.

In reality, a comparison of total development cost as a percentage of the sales generated in each group revealed a different conclusion.  The group doing development offshore had a development expense as a percent of sales that was over two times the groups using internal development resource.  So not only was the company not saving money with the offshore resource, but also they also truly did not understand the total picture that made up their development expenses.

So in this scenario, what were the factors that were overlooked?

    Less overhead for internal developers—typically, the relationship with offshore developers requires extremely detailed specifications in order to communicate the software product being developed.  While specs are necessary with internal development teams, the team can afford less rigor in this area, as issues are more easily resolved locally.
    The proximity of the development team to the "internal" customer (i.e. Sales, Marketing, Product Management) makes an iterative development process during the design/development portion of the project efficient and effective.
    Typically, an internal development team has knowledge of the industry, the channel, and the customer that the product is being developed for.  This knowledge improves efficiency, accuracy, and ultimately provides value that is difficult or impossible for an offshore developer to deliver.
    Qualifying the software product with the customer (assuming the customer is in the U.S.) can also be lead to more logistics difficulty, additional communication between the client and offshore developer, and overall less efficiency.

Costs aside, the organization looking for an offshore solution must also consider a host of other factors regarding intellectual property security, potential legal issues with export control policies, and potential difficulties with project oversight.

Rather than focus on cost, the organization dependent on software products may receive a larger payback from more focus on the benefits their internal team can deliver in terms of innovation and the strategic value that their current organization can deliver.  Software development is by nature an experimental and iterative process—organizations should not only accept this fact, but they should embrace it as a competitive advantage in an ever-increasing world of "me too" thinking. With the deluge of software development methodologies and mindsets touted in recent years, the choice hardly seems crystal clear. In fact,  Alistair Cockburn’s book, Crystal Clear: A Human-Powered Methodology for Small Teams may seem to muddy the water further. That is, until you read it, and discover Cockburn’s common-sense approach, borrowing the best-in-class from different camps.
What’s In a Name?

Crystal Clear is just one among many in the Crystal "family" of methodologies, which Cockburn describes as having a "common genetic code." The methodologies are distinguished by size and criticality, analogous to the mineral properties of color and hardness. Hence, Crystal borrows its name from quartz, ranging from Clear, for "collocated teams of 8 or fewer people," Yellow, for teams of 10-20, Orange, for 20-50, Red, for 50-100, and Maroon, Blue, and Violet covering greater numbers. Is the quartz/methodology analogy clever and useful, or bewildering and contrived? That probably depends on who you ask and their propensity to cynicism.
Essential Properties

Cockburn’s tolerance for extensibility and flexibility offers practitioners a foot-hold for transitioning toward agile practices. He acknowledges that in the trenches of development, some of the more extreme agile initiatives cannot be rigidly adopted in every environment. Through interviews and case studies, he highlights common denominators of successful project teams, which he breaks down into seven essential properties. (Note that the adjective "essential" is not happenstance. It replaces the formerly popular term, "best," with aspirations of sufficiency, not grandeur.)
1. Frequent Delivery

Cockburn describes frequent delivery as, "the single most important property of any project, large or small, agile or not." It’s hard to argue with the advantages of frequent delivery:

    it provides an opportunity for feedback from clients;
    it gives sponsors a warm-and-fuzzy affirmation that work is actually progressing;
    it provides the project team an opportunity to debug and work out deployment processes along the way, and perhaps most importantly; and
    it gives developers a sense of accomplishment.

Delivery can (and many times must) take different forms, depending on the nature of the project, client, and environment. When a client simply cannot accept delivery of incremental software updates, Cockburn suggests deploying to a test workstation, ideally with a friendly user trying it out. When even that is not possible, he recommends incorporating user viewings so the users have an ample opportunity to see work-in-progress.

How frequent is frequent? Anywhere from an hour to three months, with the average falling somewhere between two weeks and two months.
2. Reflective Improvement

A buzz-phrase from the quality movement was "continuous improvement." Reflective improvement is along the same lines, but as the name implies, involves the team getting together periodically throughout the project to think about and discuss what is and is not working.

The idea is to not wait until the project is over to conduct a project retrospective, hoping to learn something helpful for the next go-round, but to adjust and regroup as necessary throughout the project in nimble fashion. Cockburn recommends devoting an hour every few weeks or month for this purpose.
3. Osmotic Communication

"Osmotic communication" is not only the coolest new catch-phrase, but a way of referring to the sort of indirect information flow that occurs when you overhear background discussion. (Didn’t that used to be called "eavesdropping?") The real point of this property is to emphasize the importance of team members being co-located, either in the same room or in adjacent offices.

Cockburn acknowledges the possible pitfalls—namely the occasional need for private space for personal communication, as well as the need for silence during concentration, and offers some office space configurations that make accommodations for these factors.
4. Personal Safety

The personal safety property refers to trust and fostering good relationships between team members so that genuine communication is not hindered by fears of revealing inadequacy or reprisals for making a mistake. Cockburn emphasizes the importance of not confusing the sort of "amicability" he refers to here with stilted politeness, which can obscure disagreements and cause more damage.
5. Focus

Do you know what your top two priorities are at any given point in time? You should, says Cockburn, if you are focused. Yet, knowing what to do is not the only key. You must also have time and peace of mind to work on the priorities. Therein lies the rub. Many developers are assigned multiples projects simultaneously, and the loss of focus due to transition wreaks havoc on productivity. According to the interviews Cockburn conducted, one and one-half projects is the most a person can handle and remain effective.
6. Easy Access to Expert Users

In an ideal world, an expert user would be co-located and available full-time for direct involvement throughout the design and build. In real life, that’s rare, but if an expert user is not available at least one hour a week, the likelihood of a successful project will diminish. Cockburn recommends weekly or semi-weekly user meetings with phone calls as needed. If a user isn’t available to be directly involved with the project team regularly, he suggests sending a developer to shadow a user and become a trainee for period of time.
7. Technical Environment

Successful project teams operate in a technical environment with a core set of tools and practices: automated tests, configuration management, and frequent integration. While Cockburn is reluctant to say that automated tests are absolutely necessary for Crystal Clear (since adequate manual tests can be used) he makes the astonishing claim that every programmer he has interviewed who has moved to automated tests declared he would never work without them again.

A configuration management system for controlling versions of code should be considered "the most critical noncompiler tool," according to Cockburn’s research. Since integration of disparately developed code components is traditionally a trouble area, Cockburn cites the importance of integrating frequently, while the code is still fresh in the minds of developers, and to prevent too many errors from accumulating.
Summary

Crystal Clear, the methodology (or "methodology generator", as Cockburn prefers to characterize it) is simple and makes sense. Crystal Clear, the book, is written accessibly, first summarizing the seven properties of successful projects, then detailing specific useful techniques gleaned from actual projects, followed by an extensive question/answer chapter. Alistair Cockburn anticipates and responds to the inevitable critics, providing the sort of feedback one might anticipate from a panel discussion.

Whether one embraces Crystal as the Next Big Thing, or discounts it as a re-branding of common knowledge, Alistair Cockburn deserves credit for his thoughtful, practical dissertation, which brings transparency and clarity to a subject that is becoming increasingly complex. It doesn't help that "SUCCESS" really stands for Subsidized Child Care Eligibility System (which isn't nearly as cool as the sweet sound of SUCCESS—chalk another one up to the importance of the acronym). If I'm going to be stuck with an acronym, following me ever-after, at least I'm fortunate enough to have a positive-sounding one, like Eastwood's great Dirty Harry line and Stallone's Rocky theme.

Will there be life for me after SUCCESS? Will there be life after your current project? Many a weary developer has heaved a deep sigh while asking that same question.

As Gilligan's Island's Bob Denver and I Dream of Jeanie's Barbara Eden know, even a positive association can become tired after a while. Sure they were happy that their characters were loved, but wearing the same red shirt and white hat or transparent genie outfit is bound to get old after a while. Amazingly, Bob Denver kept his boyish figure, though not his hair color, almost indefinitely, and Barbara Eden continues to wink and work her magic, looking remarkably fit in that belly-baring costume. Would that applications could enjoy such longevity. (Even if they dared, the platforms on which they resided would spontaneously combust like a one-dimensional Hollywood set in flames of unsupported kindling.)

Things are even worse if the project was a major disappointment at the corporate office, engendering two-thumbs-down ratings. In that case an image makeover is particularly important for a developer. What could be worse than to have your name automatically associated with an epic flop, even if you weren't responsible for its failure?

We may not receive a lot of public recognition, but developers are a lot like actors or directors, our careers punctuated by each software smash. Most of us can't get away with being a "one-hit wonder" and manage to stay gainfully employed from project to project, but it's amazingly easy to fall into the typecasting trap.

Trap? Maybe "trap" is something of a misnomer. What about trademark? Was the Arnold Drummond character on Different Strokes a typeset trap for Gary Coleman, or the highlight of his career? Was Barney Fife on Andy Griffith an inescapable shadow for Don Knotts, or the stuff that beloved legends are made of?

Since programmers aren't as likely to rake in residuals or sell memorabilia at fan-supported conventions (what's the street value on an original project Gantt chart?), it behooves us to make sure that our "specialized niches" don't yield a flash of fame and a retirement plan standing in the unemployment line.

The thing is, actors rarely take a part knowing that it will forever label them—they just need the work.

We're a lot like that too, and if we aren't the free-agent-contracting sort, we typically don't get to pick and choose our projects. Yet, like actors, we can make a conscious choice to stretch ourselves, trying our hands at comedy (web development), even if we've always done drama (traditional client/server development). If you take the initiative, demonstrate ability, and make your interests known, most managers will accommodate eventually. After all, it's cheaper to keep you happy than replace you.

Does anyone even remember that Tom Hanks started his career in the cheesy, cross-dressing television sitcom Bosom Buddies? If you mention Tom Hanks's name today, you'll conjure up images of that Wilson volleyball from Castaway, or the box of chocolates from Forest Gump, or the tragedy of war depicted in Saving Private Ryan. Hanks reinvents himself.

What can you do to broaden your spectrum, silver screen or otherwise? You can learn when to bring closure and move on. Imagine making a million dollars per object you code, like a TV actor making a million per episode. Would you walk away from that? Apparently the cast of Friends has reached the conclusion that's in the best interest of their careers, ultimately, to do just that.

Often clients become so attached to a developer during the intensity of an active project that they have trouble letting go post implementation. The next thing you know, you're their "personal banker," receiving direct phone calls relating to everything from how to do a mail merge in Word, to something's wrong with my home computer. Enhancement requests trickle in over time, and before long, you're on the road to the gold plating you were so careful to avoid during the development phase.

Sometimes you just need to enlist your manager's support in establishing a "settling in" or "cooling off" period, when no changes (other than necessary fixes) are made. Another strategy is to purposefully turn over support and enhancements to another developer, making the organization less dependent on you. The notion of willfully making oneself dispensable is so scary that many developers are reluctant to do it. Some shops adopt an organizational structure (separate development and support groups) to facilitate these very migrations, to ease the transition from one "starring role" to another.

But specialization is necessary—it's a good thing! Sure it is...

We can't all be experts at everything. But consider actors who weren't willing to change with the times: the silent film stars who claimed that "talkies" were just a passing whim or later that color just wasn't as artistic as black and white.

Take a long look at your list of credits. Does it show diversity and depth, or is one application a cookie-cutter replica of the other? If you have a solid base of experience, look for new and creative ways to channel or repackage your talents. Take risks. Accept unusual challenges.

Consider John Wayne's transition from the tough cowboy to the hard-hitting military man. Same type of macho character, but it breathed new life into his career. What about Jimmy Stewart's versatility, successfully appearing in westerns, romantic comedies, and mystery/thrillers? If you're a developer who can only wear red shirts and white hats (even if you wear them as well as Bob Denver), the opportunities are going to be limited.

Even though people still occasionally call me the SUCCESS lady, I just smile inwardly and have the last laugh because deep inside I know I'm not a typecast coder. I've worked on several failed projects since then! In response to a previous essay called "The Human Impact of Software," a reader named Scott wrote to me and had this to say:

    "In my opinion it is usually NOT the coder, but the rest of the software company who is at fault. The code and the coder are not the lynchpins of software development, but instead it is usually the managers and designers. You can't always blame the coder, and so your column is sort of biased against the coder.

    "The managers of that company should have monitored the coder's progress, done code reviews, had good design docs, and made sure the code for THEIR business was good. It is THEIR business and therefore THEIR fault. If the coder sucks, then kick his @$$ out of the company and get another one. As for all the bugs that were found after the release, again this is management’s fault for NOT having a proper testing process before release.

    "You can't expect people to really care if their code is good or not. They are just hired to do a job and they will do it however they can to secure their paycheck. This is human nature. 10% of people do care. The rest don't and NEVER will. It is the same with all crafts in this world. Most people will do a poor job which will cause others stress and cost them lots of money and time. This is a terrible thing, but it is the job of the manager to make sure quality people are hired and well compensated."

Scott’s comments deserve a response, so I thought I would use the excellent opportunity as a basis for a follow-up essay.

I hear Scott making two major points, and I will hit them one at a time.

First, Scott makes the point that I placed all of the blame for the disastrous release described in the "The Human Impact of Software" on the developer who wrote the code. This is an understandable reaction, given that I did not make any mention of what mistakes management might have made, or what inadequacies might have existed in the overall development process. This is a failing of mine, and I should have balanced the column by making some mention of other factors that might have also influenced the failed release I described.

There were of course other factors at work, and yes, the management of the company has to accept ultimate responsibility for all of the problems caused by that release. It was, as Scott point’s out, management’s decision to release the software. Furthermore, there were probably deficiencies in the design and requirements process (though I was not at the company during that time), and given the number of bugs that reached our customers, the testing process should have been more rigorous. Regular code reviews would also have been a stellar idea. So yes, other people share responsibility.

However, none of that absolves the developer of that horrible code. Horrible code is horrible code, and the only one who can write it is a developer. The designer did not write that bad code, nor did the managers, testers, or the person who gathered the requirements. The problems in that code were not related to process and management deficiencies, nor were they related to incomplete requirements, bad design, or an unreasonable schedule. When a developer writes bad code, any attempt to shift the blame for its failures to designers and managers is nothing less than a cop-out.

The point of the essay was not to focus on how managers and development processes can have an effect on the quality of software (and therefore the lives of real people)—of course they can. Rather, my point was to draw attention to the fact that there is a direct line between the abstraction embodied in our code and the reality of the people who will come into contact with that code. Methodologies and managers are beside the point—a distraction from the real issue.

Too often when we write code, we get caught up only in the exercise of the code itself and forget about the people who will have to use the software and maintain the code after we are long gone. We should write code as if we are going to compile it, hand it to the user, and that’s it. We should strive to make it that good.

When I am working, I take this idea one step further: I pretend that I am going to have to sit there right next to the user every day and watch him use my application. Every time he has a problem understanding how it works, every time it crashes, every time a calculation is wrong, every time it takes too long, every time a cryptic error message or prompt comes up on the screen, he’s going to turn to me, with a pained look on his face, and say "What are you trying to do to me?" In this way, I use my own ego and vanity to make sure that the user instead would turn to me and say "This is awesome!"

Another thing I do is picture another developer coming to my code with the job of maintaining it. I picture myself looking over that developer’s shoulder, watching him trying to understand it and make some modifications. When I write code, I want to make sure that this developer would turn to me and say "Damn! This code is amazing. You have made my life as a maintenance developer so much easier. Thanks." These imagined scenarios may be silly, but they work for me.

Will we as developers make mistakes? Will we cause bugs? Will we take a shortcut here and there? Will we be forced by business realities to make compromises? Of course. I am not calling for absolute perfection. I live in the real world with everyone else. But in too many cases, the balance is shifted so far the other way that it is (and probably always has been) a huge problem for our profession. The number of bad developers out there is so high that it is an embarrassment to all of us.

Which brings us to Scott’s second point: that 90% of all developers will always suck and won’t ever care one bit about all this quality and craftsmanship jibber jabber. Perhaps Scott is right. Perhaps it is futile, naïve, or even arrogant to think that something like this essay is going to convert a bunch of people from poor developers to good ones. Perhaps as an industry we should just resign ourselves to the fact that 90% of developers will always suck and do everything we can to compensate for this fact—or just make sure that we only hire from that pool of top 10% people.

While I am a long-time fan of Sturgeon’s Law, I am not quite this cynical when it comes to our burgeoning profession. I believe that we can—and must—raise the general level of quality and craftsmanship. The best way to do this in my opinion is to examine ourselves, do our best to improve, and set a good example for our fellow developers. Has anyone reading this not encountered code so bad it hurts the brain just to look at it? I’ve only been in this business eight years, but I have encountered code like this at every single job I’ve worked at.

I see the problem as two-fold: one, too many developers have not adopted the proven techniques and practices that lead to good code; and two, the "profession" of software development is so young and informal that most managers and business people do not respect it or even have a clue about it. In other words, the profession itself does not have yet the posture to "push back" at business to insist that code be written to a high standard of quality, and to insist that certain minimum processes be followed. Until some of us in that top 10% start standing up (in a nice way, of course) and saying, "No, I just won’t do it that way," business will never get the message, and neither will our fellow developers.

Think of the "mature professions": accounting, law, medicine, electrical engineering, civil engineering, mechanical engineering. These professions have established and codified standards, practices, and ethics. These principles are taught in schools and enforced through peer pressure and even official sanction. Even though Sturgeon’s Law still applies within any one of these professions (that is, 90% of accountants will suck when compared to the top 10% of accountants), the general level of quality and craftsmanship one finds in any one of these professions is going to be higher than what we find generally in the profession of software engineering.

I was inspired by Steve McConnell’s 1999 book After the Gold Rush: Creating a True Profession of Software Engineering (Microsoft Press). This book argues that the there is already underway an inevitable evolution of software development from a "craft," into what McConnell calls a "true profession" based on the principles of engineering. I will close this installment with a quote from After the Gold Rush:

    "Arthur C. Clarke said that any sufficiently advanced technology is indistinguishable from magic. Software technology is sufficiently advanced, and the general public is mystified by it. The public does not understand the safety risks or financial risks posed by software products. As high priests of powerful magic, software developers need to use the technology wisely.

    Engineering may be regarded as boring in some quarters but, boring or not, engineering is a better model for software than magic is. The engineering approach to design and construction has a track record of all but eliminating some of the most serious risks to public safety and of supporting some of the most elevating expressions of the human spirit. Concerned software developers have a responsibility to ensure that software rises to the level of engineering. In the short term, software engineering practice needs to be patched up enough to prevent further software disasters. In the long term, software engineering practice needs to be elevated to a level at which it can support the next generation of technological miracles." (Page 63) According to Software Productivity Research, the average productivity in Java is only 20% better than in BASIC. C++ fares no better than Java [SPR, 2005]. In fact, with the exception of Smalltalk, not a single programming language in general use today can show a substantial increase in productivity over BASIC.

So after all of the language wars, objects, components, and frameworks, we are still scarcely more effective than 20 years ago. However, go back a couple of decades more and there is a radical change: a leap in productivity of 400% from Assembler to BASIC. Why was that leap so big, and how can we achieve it again today?

The 400% increase was because of a step up to the next level of abstraction. Each statement in C++, BASIC or Java corresponds to several statements in Assembler. Most importantly, these languages can be automatically translated into Assembler. In terms of productivity, this means you effectively get five lines of code for the price of one.
Did UML Increase Productivity?

Traditional modeling languages like UML have not increased productivity, since the core models are on the same level of abstraction as the programming languages supported: When designing in UML we still work with objects, their attributes and return values. One day of modeling with a UML modeling tool produces a day’s worth of code.

However, after using the code generation functionality of these types of tools you still need to go in by hand and add or edit the majority of the code. You’re thus trying to maintain the same information in two places, code and models, and that’s always a recipe for trouble. Ever seen anybody hand edit Assembler and try to keep their C++ code in synch with it?

Of course, UML has its benefits: the visual presentation can be read much faster to get an overview. But one look at the ongoing development of UML speaks volumes. Half of the developer world finds that UML cannot represent what they need in their models, and so want to add something to it. The other half says UML is too complex, and want to reduce it down to its core elements. UML tries to be all things to all men, and thus cannot raise the level of abstraction above the lowest common denominator.
Raising Abstraction With Domain-Specific Modeling (DSM)

How then can we raise the level of abstraction beyond today’s 3rd generation programming languages? One way is to move away from trying to specify all kinds of applications using only one generic set of program language concepts. Instead, each company could use the concepts of the products it makes, giving each its own visual representation.

Symbols in such a domain-specific modeling language (DSM language), along with the rules about how they can be connected and used, would thus come directly from the problem domain—the world in which the application is to run. This is a whole level of abstraction higher than UML. This results in a very expressive, yet bounded, design language that can only specify applications in the problem domain that it was designed to cover.

For instance, a DSM language for developing mobile phone applications could make use of concepts like "soft button", "menu", "send SMS" and "notification", which are descriptive of the mobile phone domain but can hardly be used for developing web applications, ERP software, or a business intelligence application. This narrow focus makes defining a DSM language a lot easier than a more generic language like UML or Java. The improved expressiveness on the other hand makes it possible to define an application completely with a relatively small modeling effort.

Because of this completeness, a good DSM language is well suited to be the basis for automatic generation of full code. This leads to a situation where you can have graphical models that form the core of your software development effort, and automatically generate all necessary code and documentation from them. The models are then both the design documentation—a view of the product or system at a high abstraction level—as well as its implementation in code. Documentation and implementation remain synchronized throughout the whole lifecycle of the application, and software development becomes truly model-driven.

Full code generation from a DSM language requires a code generator. The mapping from model to code is defined in the generator. DSM languages and the way we write code differ from domain to domain. A DSM language therefore requires a matching code generator that also meets the requirements of the problem domain. In other words, to make sure that you are able to generate full code—written in the way you want it—from your DSM language, you need complete freedom in defining how your language maps to code.

Vendor-proprietary or fixed code generators are quite useless for a domain-specific modeling language. This becomes even more valid with the knowledge that both problem and solution domains evolve over time. Therefore, you need to be able to evolve your modeling language and code generator and don’t want to be at the mercy of a tool vendor doing this for you.
When and How to Implement DSM?

Domain-Specific Modeling requires a DSM language and a matching code generator. Although defining these is much easier than defining a generic modeling language, it is still not a task for every developer. It requires a good knowledge of the problem domain and the code that is written for it. This domain expertise is usually found in situations where we deal with product family development, continuous development of the same system or configuration of a system to customer-specific requirements (phone routing systems, CRM, Workflow, payment systems etc.).

Experienced developers who work in these types of domains know what the concepts of their domain are, are familiar with the rules that constrain them and know how code or configuration files should be written best. Sometimes it is one expert who possesses this knowledge, sometimes it is shared among a small group of people with different skills. These experts are therefore well qualified to define the automation mechanism that will make the rest of the developers much more productive.

Situations where we deal with a repetitive development effort are thus well suited for DSM adoption. A facet of repetitive development is the use of, or move toward a common product platform upon which product variants are developed. The product platform based approach to software development has its roots in the drive to better manage the complexity of offering greater product variety.

Key in this approach is the sharing and reuse of components, modules and other assets across a product family in order to develop new variants faster by assembling them from standard components. These standard components in a standard environment form the product platform, which raises the level of abstraction and at the same time limits the design space.

In most situations however, development teams use generic programming languages to develop software on top of this product platform. Why use a programming language that has been designed to develop any type of application when we are already working inside the boundaries set by the product platform? It is this product platform that is supposed to make things easier, so why not make optimal use of this by using a DSM language on top of the product platform? The code generator then maps the models to code that interfaces with the product platform.

But what if the product platform itself is subject to a lot of change, or several platforms or platform variants must be targeted? In these cases it makes sense to create a framework layer between the generator and the product platform(s). A change in the platform then only requires the expert to make a change in the framework layer. The code generator and DSM language that all other developers use can remain unchanged—and of course there is no need to change the models the developers have made.

This is in stark contrast to cases where applications are coded directly on a platform, when platform changes generally mean wholesale changes throughout most of the written code. Building a framework between the generator and platform can be a good idea even for a single platform. It can make creating the code generator a lot easier as many of the variation issues can be handled inside this framework layer, instead of having the code generator deal with all the details.

To summarize: DSM implementation requires the definition of a DSM language and code generator by the expert. It makes most sense in cases of repetitive development where it allows making optimal use of the development platform. DSM thus requires an investment of development resources and time to set up the DSM environment. Although this often clashes with the urgency of current development projects the improved productivity is often worth it.

Industrial experiences of DSM consistently report productivity being between 5 and 10 times higher than with current development approaches. Additionally, one has to bear in mind that it is better that the expert formalizes development practices once, and other developers’ generated code automatically follow them, rather than having all developers try to manually follow best practices all the time.
Tools for DSM Implementation

A software development method is of little value if it has no tool support. Designs in a DSM language on a whiteboard do not generate any code, and thus while they help in defining the functionality of the application or system, they do not improve productivity much. DSM has been around for quite some time but companies that chose to adopt it often had to resort to building their own environments to support their DSM language and generators. The huge investment in man years required to do this often led to the decision to look for other ways to improve productivity.

Today, increased competition requires companies to reducing development cost and time to market. Improving the productivity of software development teams has become an important factor in achieving this. After realizing that UML in its standard form does not offer the possibility to significantly improve productivity, companies like my employer, MetaCase (MetaEdit+), along with Microsoft (Software Factories and DSL Tools), Eclipse (EMF & GEF), and Xactium (XMF-Mosaic), have taken steps toward allowing users to build domain-specific model-based code generation environments.

Some do this by allowing the user to make UML more domain-specific by adding profiles to it, while others offer complete freedom in designing the DSM language and code generator. The effort that is required to do this differs from tool to tool. The ideal of course is a tool that gives plenty of freedom to truly raise the abstraction level of the language beyond code and make it as domain-specific as is needed, where more is usually better. Furthermore, the code generator should be fully open to customization, allowing the generation of code that looks like the best previous hand-written code.

In both of these areas, the tool should allow the expert to produce the desired results with as little effort as possible. In particular, the initial definition process should be as efficient as possible, allowing the expert to try out various language approaches quickly. Later on, when the language has been taken into use and real models exist, the tool should handle the evolution of the modeling language and automatically update models as much as possible.
How Does DSM Differ from MDA?

Currently, the Object Management Group (OMG) is intensively promoting its Model-Driven Architecture (MDA), a model-driven development method that comes down to transforming UML models on a higher level of abstraction into UML models on a lower level of abstraction.

Normally there are two levels, platform-independent models (PIMs) and platform-specific models (PSMs). These PIMs and PSMs are plain UML and thus offer no raise in abstraction. It is important here to distinguish between software platforms (what the OMG means with the word platform) like .NET, J2EE or CORBA, and the "product platforms" that I described earlier, which would be at least one level of abstraction higher.

In MDA, at each stage you edit the models in more detail, reverse and round-trip engineer this and in the end you generate substantial code from the final model. The aim the OMG has with MDA is to achieve the ability to use the same PIM on different software platforms and to standardize all translations and model formats so that models become portable between tools from different vendors. Achieving this is very ambitious but also still many years away. This focus however clearly defines the difference between DSM and MDA, and answers the question of when each should be applied.

DSM requires domain expertise, a capability a company can achieve only when continuously working in the same problem domain. These are typically product or system development houses more than project houses. Here platform independence is not an urgent need, although it can be easily achieved with DSM by having different code generators for different software and/or product platforms. Instead, the main focus of DSM is to significantly improve developer productivity.

With MDA, the OMG does not focus on using DSM languages but on generic UML, their own standard modeling language. It is not looking to encapsulate the domain expertise that may exist in a company but assumes this expertise is not present or not relevant. It seems therefore that MDA, if and when the OMG finally achieves the goals it has set for it, would be suitable for systems or application integration projects.

MDA requires a profound knowledge of its methodology, something which is external to the company and has to be gained by experience. Thus whereas the domain expertise needed for DSM is already available and applied in an organization, MDA expertise has to be gained or purchased from outside. In these situations the choice between MDA and DSM is often clear.
Conclusion

Throughout the history of software development developers have always sought to improve productivity by improving abstraction, automation, and visualization. Domain-Specific Modeling combines these methods and copies the fundamental idea that has made compilers so successful: When you generate code, the process has to be complete.

These benefits present a strong case for the use of DSM but at the same time form an important obstacle: DSM implementation requires companies to define and maintain their own domain-specific modeling language and generators. A range of new tools are already helping to make this easier, allowing expert developers to encapsulate their expertise and make work easier, faster and more fun for the rest. Software developers play professional Hacky Sack with terms like "standards" and "best practices," batting them around with effortless familiarity. But under the microscope of personal scrutiny, the game might very well change to dodge ball.

Seven years ago our CIO sponsored a departmental logo contest. I won—though lack of interest and entries probably explains it. The winning entry wasn’t great: I simply dotted the "i" of MIS with the county seal. Underneath, in small, italicized letters, I added three words that came to mind: "Integrity, Innovation, Initiative." I’ve always been a sucker for alliteration. The phrase sounded like something you’d hear in a deep, TV announcer voice.

Clearly we had innovation covered—as an information technology organization we kept in tune with the latest microprocessors and software. We upgraded, enhanced, re-engineered, developed, and replaced like our futures depended on it. (They did.)

The other two words—integrity and initiative—seemed appropriate for a service organization, especially one in government. After all, we’re paid with the people’s tax dollars...and we’re some of the same folk who want to see that hard-earned levy well spent.

Even if lethargy and procrastination would rob us of initiative, between the CIO’s published departmental goals, performance plans, and our clients’ never-ending service requests, we managed to get a regular workout.

The third word—integrity—is something altogether different, I now realize.

At the time I was probably thinking of perceived departmental integrity. The department implemented a service request system that held us accountable for every password reset, keyboard coffee spill, or report change—and thousands of other critical and trivial calls for IT assistance. We wrote hundreds of procedures and best practices, following a rigorous review process. Despite the tremendous effort and discipline it took to maintain, we did the right things to build trust and confidence in our organization. We wanted to be viewed as a department with integrity.

Yet, despite all the coaching and well-planned strategy, the game is won or lost by individual players.

When each developer comes to work in the morning, he or she makes a conscious choice whether to exercise personal integrity. What do I mean? Take this quick-and-dirty self assessment.
True or False?

    Since I’m in the technology profession, surfing the Internet throughout the day increases my awareness of the latest tools and trends. The only thing that would really be considered inappropriate at work is accessing pornography.
    I play Solitaire or other computer games occasionally throughout the day to relax my mind so I can regroup and focus on work better.
    When going for coffee or a soft drink, sometimes I get into lengthy conversations with coworkers about a ball game or TV show, but I’m still entitled to my usual break. Talking with coworkers is really teambuilding, and therefore work-related.
    I send and receive emails from friends or relatives quite a bit throughout the day, but it’s no big deal. Everybody does.
    If I arrive at work on time and am available at my desk in case a real emergency comes up, I’m doing my job.
    I play DVD’s or have the TV on with a game or the soaps in my office, but I’m still working.
    I keep my door closed a lot of the time, but I need my privacy in order to focus. It’s nobody’s business what’s on my screen and I don’t like people sneaking up on me.
    Everybody checks their eBay auctions at work.
    Supervisors should not monitor Internet or email usage. We’re adults and professionals and should be treated accordingly.
    I have changed the date and/or time on my computer to beat the system (to close a request on time, for example), but only when I felt completely justified.
    I have hard-coded a program or "zapped" data off the record for a quick fix when I was in a hurry and couldn’t figure out the real solution.
    I don’t feel compelled to share everything I’ve learned through sweat and hard work with my coworkers, even if that means they have trouble supporting my code.
    Coding standards are really for newbies and incompetents. At a certain point, a professional should be able to exercise judgment and use his/her own personal style.
    I’ve used my system administrator rights to query a database for information I didn’t need for my job, but I just wanted to know for personal reasons.
    I’ve told a client a feature they requested was not technically possible (when it was) because it would have taken too much effort to implement.

Most of us, if honest, would have some "True" responses to these statements. Is that in itself an indication of a lack of integrity? Are all of these issues equally serious? Are the ethics as simple as that? I don't pretend to have clear answers.

Considering that most of us work so hard that we scarcely take time out to visit the...um...facilities, the hint of censure is infuriating. We work through lunch, stay late, and take work home. Probably 90% or more of us do just that. While we might check stock prices or our Yahoo! email account, we give back much more than we ever take.

Before I became a supervisor, the very notion that someone might be monitoring my Internet usage was not only horrifying, but demoralizing. I was also extremely upset that developers in our organization were not given network administrative rights. Didn’t they trust me? Didn’t they consider me professional enough to be responsible? Then with the implementation of our new fangled IP phone, I realized Big Brother was possible from every angle.

Ah—but then I became responsible for ten other developers—good people, who by the nature of their work are fairly stapled to their desks all day. During my entire first year as a supervisor, it never occurred to me to check Internet usage logs. When I did take a look late one day, I was astounded by the quantity of research some of my staff had been engaged in. Surely they were close to solving the problem of world hunger.

A sinking feeling in the pit of my stomach followed me for the rest of the evening. What if the county manager, on a lark, asked for the log? Would I have trouble explaining why one of my staff had more hits than anyone else in the county? How credible would our request for an additional head-count and greater bandwidth seem then?

It’s really not about Internet use or superfluous external email. As a supervisor, I know if I even mention the Internet usage log, the least guilty staff will be afraid to touch it for a few weeks. I’m afraid the most guilty will be quick to justify the numbers, and maybe they can — I hope they can.

The real issue is that third important word on our department’s logo: integrity—who you are when no one’s looking, when no one is there to be impressed by hard work or disappointed by lost productivity. The reality is that Internet usage monitoring and integrity surveys aren’t really all that necessary. Work speaks for itself. Supervisors know who is faking it. Coworkers know whose extra burdens they are carrying.

The tragedy is that the organization pays the price. Maybe it’s time for software professionals to do a little less Googling and a little more soul searching. As with most things in life, moderation is a wise governor. For many software companies, intellectual property just happens. Programmers write code. Sales staff find customers. Executives book revenues. The company may have a sales strategy and a long-range view of technology, but the legal protection of its intellectual property becomes an after-thought—a strategic orphan.

Big mistake. Companies that do not identify risks early, if not immediately, pay dearly when the day comes when a potential buyer or investor tries valuing the core asset—software. The strategy need not be poetry. Nor must it harmonize every syllable of the business plan. However, the software protection regime must be more than a three-line checklist of patent filing deadlines, or the occasional piece of hate mail threatening to sue over pilfered computer code.
Developer Versus Consultant

Most software firms are part developer and part consultant. Each company must recognize which trait dominates. Developers need strong stand-alone licenses and sometimes a patent; consulting-oriented firms require solid engagement agreements and cross licensing deals. Along with all the other competing demands, the executives must juggle the mindset of lawyer, developer, and business executive.

A pure development shop can afford to delegate the "legalese," and focus on technology. The easy-to-forget contractual terms and conditions weigh less on the daily demands of these firms. However, the firm that tilts toward advising, not selling code, needs an elegant agreement that untangles the various deliverables and identifies who owns what. Legal prose—no matter how unreadable—reigns supreme.
Total Cost of Ownership

Beware the hidden costs of patent. Although an initial cost of a patent may be as little as $10,0000 to $15,0000, the "total cost of ownership" of the patent is much, much higher. First, the patent owner must invest in monitoring the marketplace for possible infringers. Courts have held that patent holders who sleep on their rights and permit free sales of infringing products, have relinquished their rights to enforce the patent. Second, once a developer catches the infringer, the patent holder must invest the time and money to file a lawsuit (or present a credible case) to stop the infringement. The costs of this phase can dwarf the price of the initial filing.

In the right situation, the nature of the product of the business strategy justifies feeding the patent monster. For example, some firms aim for applications with long shelf lives. Perhaps the software they sell depends less on the ever-changing chip market or communications equipment makers. If the software publisher can establish standards, then the market will change less frequently. If the software depends on the installation of thousands of units, such as the Microsoft operating system, then users can less easily replace the system with that of a competitor.

Patents protect big-ticket items that companies can sell year after year. Unlike patented drugs or heavy equipment, software buyers can instantly upgrade and sell their programs. The three- to four-year average patent prosecution period is simply too slow for software sold in a competitive and volatile market. Also, without a protection during the period between the filing of the patent and the issuance of the patent, others can market the idea with impunity. Note that, in many cases, after only 18 months from the filing, the Patent and Trademark Office will post the application on the internet, available to every Napster-trained software pirate.
Avoiding Patent Overkill for One-Hit Wonders

Some programs must constantly be updated – not only to accommodate behind-the-scenes changes in software and embedded code, but also to adapt to user demands. The more complex, large, expensive, and customized the software, then the more developers should opt for trade secret protection over the patent. A software program providing a fundamental function, such as an operating system or basic utility, will likely enjoy a longer commercial life than a niche application product.

In such a situation, the long-term patent poorly fits the short-term product. The trade secret is simply the general right of a company to profit from a non-public idea or program. Confidentiality agreements and strict security practices are the instruments that protect the trade secret. With patents, the government grants a monopoly over the innovation in exchange for disclosure by the inventor. However, for trade secrets, disclosure equals disaster. Companies must scrupulously require everyone - employees, directors and contractors—to sign confidentiality agreements. Firms should prohibit employees from competing with the company. In addition, anything the employee invents should belong to the company. Limited variations of these themes may be appropriate for independent contractors. With trade secret law, the emphasis is always on the secret. If established companies have an arsenal of patents, lax adherence to these practices can be tolerated; with no strong patent program, however, the company must absolutely keep these agreements, establish rules limiting access, and mark copies of items as confidential.
Where to Protect the Expensive Stuff

The more expensive the software development, the more formal the intellectual property protection. If the software required massive amounts of development by highly trained engineers, then the costs and delay of a patent may well be worthwhile. Thus, companies should patent the expensive stuff that took years to develop; on the other hand, new code assembled over a cold pizza after a frantic weekend warrants only trade secret and copyright protection.
Local Versus International

The more international the product, the more formal the copyright and patent protection. For products distributed beyond the United States, developers often lose control over secrecy. Domestic licenses depend on the enforceability of obligations to limit copying, avoid reverse engineering, and maintain confidentiality. Software is at the mercy of users in distant lands with exotic laws and a disregarded and disrespected legal system. Intellectual property is considered no more proprietary than drinking water. Patents, however, with a more uniform set of international standards, have a better chance than trade secrets of being honored by local officials. In contrast, international law defers to local customs and rules regarding trade secrets, thus increasing the inconsistency and unpredictability of this body of law between two legal systems.
Have a Plan: Stay Small, Be Acquired, or Go Public

The grander the vision of the future, the more likely the intellectual property will warrant protection as a patent or registered copyright. Large acquirers of smaller innovative software developers will demand a more formal protection program. Just as companies must behave like publicly-held firms three years before an initial public offering, the intellectual property strategy should reflect the longer-term objectives of the company. If the owners of the firm intend to remain independent or forsake outside investors, then an expensive and inefficient patent program makes little sense. The risks of an informal intellectual property protection program are similar to that of a farmer owning the fields he plows: If no sale or financing of the property is expected in the near term, then an imperfection in title will make little difference. Try selling the farm, or getting a mortgage, then even a minor issue can become a major problem.

In the end, coordinating the legal strategy need not be a perfectly harmonized plan. However, companies with better organized programs and a general concept of the types of protections needed will more likely maximize the financial reward they deserve as the software matures into a popular product. Those of you that have been around long enough might remember the glory period of the traditional CASE tools and structured programming. These were the days where a lot of people were experimenting with CASE tools and even went so far that they started to predict work estimations using function point analysis. The main drive was that companies wanted to get their IT projects under control again. In the late 80s, the traditional CASE tools were victorious in a lot of large companies because of this reason. These tools had a decennium to learn from their mistakes and from the mistakes of others.

In the early nineties, with the rise of Object Orientation, the traditional CASE tools were perceived as a failure by many people. The root cause of this bad perception was that the tools a) failed to embrace OO, and b) were used in combination with traditional software development processes. This caused traditional CASE tools to be strongly associated with the waterfall methodology, at a time when iterative development and rapid application development were increasingly gaining momentum. With the rise of the OO CASE tools (Popkin Architect, Rational Rose, Cool:Jexx, etc.) in the mid-nineties, everything was reinvented.

As with most software products, the CASE tool market is highly feature-driven. However, CASE tools have been around for more than 20 years, and one might expect also some mature features. The irony of it all is—as I will illustrate further in this article—that a lot of features with high user value can be implemented; it's not that there are rational reasons to not do it.

This article outlines some important aspects that today's "best of breed" OO CASE tools could learn from the traditional CASE tools. A generally applicable rule—which we forget frequently in the software industry—is that we should not neglect the experiences in a very similar domain just because we have labeled it "old-fashioned."
An Object Oriented CASE History

For the purposes of this discussion, I will identify three "generations" of OO CASE tools:
First Generation OO CASE Tools: Diagramming Tools

Diagramming tools only allow the modeling of ideas—things like concepts, analysis artifacts, etc. There's no link to code. Everything is maintained manually.

The first OO CASE tools still had the "diagramming dialect" battle to fight, which made comparing the tools quite difficult because people were more comparing the diagramming dialects than the tool's features themselves. (You may remember the "OO notation wars" before Rumbaugh, Booch, and Jacobson got together on UML and ended the controversy.) After the introduction of UML in the late nineties this was resolved.
Second Generation OO CASE Tools: Code Visualization Tools

A "code visualization" tool is a form of diagramming tools that knows how to "harvest" code and map it onto a model of classes and diagrams (this process is also known as "reverse engineering").

These tools also allow you to start from a model and to transform that into code. However, in these tools this implies that you are modeling at the lowest level—the level where you have a one-to-one mapping with code.

Typically this is not that productive, as you need quite some training and patience before you are able to get the result that you want in one go. An agile way to approach this is using the tool that suits the purpose: If you want to get an interface down on paper and you know the programming language, then just type it using a code editor. If you want a visual representation of it to be able to highlight the relationship with other classes (e.g. which classes implement this interface), then use the reverse engineering feature.

To recall an anecdote, many years ago in the Microsoft COM era I wanted to model an interface that could be generated in IDL (Interface Definition Language). It took me quite some iterations before I could get it right. Effectively, this was a waste of time because I was IDL literate: I really knew what I wanted it to look like and how to write it.
Third Generation OO CASE Tools: Computer Aided Software Modeling Tools

CASM tools assist the user in his/her choices regarding software architecture, data modeling, business modeling, etc. These tools don't solve all the problems, but rather help in coming to a solution. For example, the tool could provide a computer-aided review of your design; or it could include a data normalization/demoralization tool; or it could suggest or highlight potential "deviations," such as the use of a functional database key (e.g. customer number) instead of a technical one. These are the tools of the future. In the rest of the article, I'll outline what features you should expect of such a tool.
Lost Opportunities and Future Directions

In the remainder of the article I will explore a variety of features shared or not shared by traditional/structured CASE tools and today's "third generation" OO CASE tools, including discussion of opportunities for today's OO CASE tools to incorporate some of these features.

Before beginning this discussion, I'd like to make the distinction between a CASE tool as "tool" and as a "framework." The viewpoint of a "tool" encompasses everything I can do without any custom development. The functionality is directly accessible using the user-interface of the CASE tool. The viewpoint of a "framework" encompasses everything that can be done with the tool using custom development (that is, by using the CASE tool's API).

As a brief illustration of the difference, consider a standard feature of a CASE tool user interface: finding all artifacts with a certain name. This is a common feature. In contrast, extracting the dependencies between all classes to a spreadsheet is typically not a feature of a CASE tool. To do this, custom development—using a scripting language or a regular software development language—is required using the tool's API. I'll discuss the "framework" viewpoint in a future article, because the concerns are different.

The table below summarizes the list of features we will explore, including an indication of whether and how the features are implemented by the traditional/structured CASE generation of and the more contemporary Object Oriented generation of CASE tools. The Repository

The repository in which all metadata was stored in traditional CASE tools was a central database. Extracting metadata could be done easily by querying the central database using SQL language and its aggregation functions. SQL is universal, easy to understand and an obvious "standard." In this section I will discuss the ideas of storage format and central repository.

Typically and especially with the introduction of XML into the OO CASE tool repository format, storing goes very slow. An example of the 'overhead' of the storage format is the IBM Rational XDE XML format: an empty model already takes about 800 KB!

Other tools use XMI (the XML Model Interchange standard) as an internal storage format. From an architectural point of view, this is a strange decision. XMI is typically something you would expect at the boundaries of the tool, not on the inside. What if the XMI standard evolves into a new version with maybe a new structure (note that XMI 1.1 broke severely with XMI 1.0)? Are they going to change their storage format as well? I agree that semantically you need a good mapping from your internal model to an external model, but personally this external format would not be my first choice. Apart from the 'Select' CASE tool, none of the OO CASE tools I mentioned in the introduction use a database as repository.

And what do they miss out on: fast storage, fast querying capabilities, transactions, etc.

Typically, OO CASE tools have an object API to query models. I'm not saying they should throw these capabilities overboard, but they could merit from relational database technology in the following way: by implementing the object query capabilities internally as database queries and by exposing a „public" database structure (or part of it) to create reports on. The latter would be an alternative to expensive tools such as IBM Rational Soda and the like (this is the best case, in the worst case there's even no custom reporting at all unless you resort to writing code).

For relational databases, a lot of standard tools are available that offer data mining capabilities. If OO CASE tools would adopt relational databases and part of the database schema would be opened-up ('public interface'), standard data mining tools could be used (business objects and other analytical tools) to perform the most complex analytical processing in a high performing way. As a lot of companies are storing strategic data in their business models, this definitely would come in very handy. The problem we are facing now is that big investments are necessary in new technology (or at least new implementations) to get this kind of analytical functionality in the OO CASE tools, while this kind of functionality based on relational databases is already a commodity.

Additionally, the concept implemented in traditional CASE tools was that you had a central store (single repository). OO CASE tools completely abandoned this concept and drive users into the direction of creating multiple model repositories. However, the reason behind this is that many OO CASE tools have problems with big repositories. This means that they mainly deviated from the "central repository" approach because they weren't able to create big repositories built on structured files.

Advantages of a central repository are that you can reuse artifact definitions between projects and/or that you have the possibility to easily check whether certain terminology is already used (and whether the semantics are not overloaded). Think about it as the ability for you to create an entity and link it to an already-defined entity. Good browsing and reporting capabilities would bring the documentation associated to the referenced entity in an easy way. The central repository concept has the promise of bringing modeling to the next level.
Multi-User Support

Modern OO CASE tools typically use the file system as the repository. For multi-user support, these systems use a source control system. Many of them use the XML Metadata Interchange format (XMI) to store the UML models.

In order to modify an artifact, the appropriate file needs to be checked out. After check-in, the artifact is accessible to the other users.

The multi-user aspect is of course less easy to solve with databases, but object versioning can also be accomplished using databases (there have been enough articles published which cover this subject). However, the storage capacity and speed of a database would allow more easily to support a larger amount of users, models and versions.
Modeling Instance Diagrams Easily

A lot of OO CASE tools don't have any good support for modeling instance diagrams (a.k.a. object diagrams, as opposed to class diagrams). An instance diagram shows "live" objects, a view you typically need to help explain a class diagram to business people for example. UML doesn't have a separate diagram for this. An object diagram can be completely modeled using a UML class diagram. Tools typically don't have an instance diagram view, where you can put classes on and just fill in the values of the object together with its identity.

Traditional CASE tools offered the feature to fill in data for a certain structure that you defined within the tool. Typically, this was a table editor where you could fill in column values per row. For OO CASE tools, the visualization would have to be done in the form of an instance diagram. None of today's tools have features to enable the creation of instance diagrams easily. The author of an instance diagram needs to explicitly know how object diagrams need to be created. Consider this list of rules that the author of the diagram needs to enforce without any help of the OO tool:

    Objects don't have operations, just attributes with their values.
    By convention, doubles are represented with a decimal point; strings are put between quotes, etc. There's no tool that enforces these conventions.
    Objects have an identity before the colon, and a class name after the colon.
    The only type of relationship that is drawn between objects is an association relationship.
    When you create an instance of a class that you already have defined, you have to check yourself that the attributes you add on the object indeed exist on the class. In general, instance diagrams can facilitate communication with customers a lot, either to help in the explanation of a concept or just to double-check whether you have modeled the customer's business correctly.
Data-to-Entity Class Conversion

XML authoring tools have the capability to generate an XML schema from an XML document, but CASE tools typically don't have this capability. Integrating such a capability in an OO CASE tool would allow for the visualization (in UML) of the structure of the XML document without requiring extra tools. A UML representation of an XML schema can also be very helpful.

People have worked on a UML profile for XML schema modeling, but this has not really been standardized. During the XML hype, some OO CASE tools have added some XML features, but far too limited.
Data Normalization Tools

Traditional CASE tools had support to help you perform data analysis. Although an object is not limited to data but also includes behavior, this does not mean that you don't need any help in defining the correct structure of the data. Some of the traditional CASE tools guided you in going from data to structure using general rules to go up to the fifth normal form.

In an object oriented world this could be translated into something like starting from an object diagram, or from a non-structured table, and the system assists you to normalize the data into a structure.

Another feature is to take a database structure or object diagram and to ask the tool to validate whether it is normalized or not.
Design Critics

Some traditional CASE tools had normalization "critics" built in: These pieces of software informed the user of what was considered as bad design decisions (e.g. a table without a primary key, etc.). Unfortunately, some tools instead of "criticizing" were disallowing the offending option altogether.

As with all tools, the tool should never think it's smarter than the user. Maybe I just want to keep the model this way because I want to store my efforts and go home, or maybe I want to model in an agile way: analyze a bit, data-model a bit, etc. Another reason might be that you know that it is not perfect, but you are constrained to design errors that were made in the past.

Of course, in the traditional CASE tool era the new buzzword 'agile' did not exist yet. However, many OO CASE tools followed the same bad example: Many people have had the experience that when modeling sequence diagrams in Rational Rose, they had to fight against the "supreme modeling" knowledge (!?) of the tool. For example, when you tried to delete a signal which triggers other signals for example, the tool decides for you that you want a "cascading" delete.

Some OO CASE tools, like Argo-UML have experimented with so-called "design critics." These are rules that are defined and checked on different artifacts (for example a design model, an analysis model, etc). A positive evolution that can be seen in the OO CASE tool scene is that a lot of tools are moving away from enforcing to advising. Typically, you can validate your modeling efforts as an action you need to trigger manually instead of letting the tool performing the validation automatically.
Impact Analysis

Traditional CASE tools allowed you to visualize the effects of certain changes. If we had this feature in an OO CASE tool, you could envision a scenario such as "If I were to change Component X, what would the consequences be? What impact does this have on my deployment architecture? How many components are affected?"

This together with reporting and data mining capabilities would make the modeling tool a key instrument.
Code Generation

In traditional CASE tools, code generation was not limited to skeleton code. Many of the CASE tools supplied a language to express business rules which were translated to the target programming language upon code generation. Some believe that in future, everything will be expressed visually in a UML 2 modeling tool. Others don't believe in visual modeling at all. The truth will probably lie somewhere in between.

A picture tells more than a thousand words, but one should only model those things that have an added value when modeled visually. For some things, such as business rules, I have the feeling that a business rule language is more readable than a cluttered diagram or a scientific language such as OCL. Having said this, don't get me wrong: expressing a business rule in a programming language such as Java, C#, or Ruby is equally good for me.

I have the feeling that in CASE tools today we have reached the productivity equivalent to what could be done with a 4 GL language. The next step forward will be either massive code generation or the introduction of a functional language.
State Modeling

Every UML tool supports state diagrams, which is a first-class citizen in the UML diagram family. However, only few tools allow simulating the states and transitions (e.g. Rational Rose Real-time). Also, almost none of the tools support the ability to validate a finite state machine: Is what I've modeled finite or infinite, is it representing exactly what I had in mind? Etc.

In real-life, finite state machines are used to model object state, technical workflows, business collaboration workflows, etc. I can assure you that a complex state diagram with sub-state machines etc. is very hard to validate manually. A tool should be there to help you in these efforts. It could check whether it is indeed finite, it could allow you to simulate the different states and transitions between states, etc.
Visualizing the Architecture

Starting from a set of binaries, a CASE tool could calculate the cumulative dependency of each component. (The cumulative dependency of a component is a figure that gives an indication of the complexity.) The CASE tool could also check for layer violations, depending on the strategy you want to see applied (strict layering versus "loose layering"), and check whether the layering has been violated.

Many tools only allow you to start from the code, while an architect (and also a designer and software developer) would like to have an insight in the big picture. Where does this component fit into the general picture? Is it allowed to access component Y and Z, or would this violate the layering or even introduce circular dependencies?

How do we currently work around these issues? Circular dependencies are detected using a daily build cycle. However, the compiler cannot detect when the layering strategy is violated.

In smaller projects it is possible to manually visualize the actual architecture, but for bigger projects this takes quite some time and is destined become out of sync.

The ability to visualize the component architecture would help tremendously in steering and following-up on the architecture being built (actual architecture) and the planned architecture. When you notice that the actual architecture starts deviating from the planned architecture, you can revise the architecture in an early stage, either by approaching the developers to get the components back in line with the envisioned architecture and/or by revising the envisioned architecture.
Conclusion

I hope that I made you realize—whether you believe in OO CASE tools or not—that there are quite some features that can be implemented today. Instead, all CASE tools are currently focusing on design pattern injection, Model-Driven Architecture, etc. The first OO CASE tool vendor that figures out how to break out of this cage and start addressing what is really required to get us to the next level at a decent price will be victorious. Analysts tout 2005 as the year of open source. Its use has gained increased media attention, and many software consulting companies are starting to support various open source projects. As developers, we know there is a definite benefit gained with using open source software in some aspect of our project, whether it is used as a tool (e.g. Eclipse IDE, gcc, etc.) or integrated deeply into the project (e.g. Tomcat, Spring Framework, Apache HTTP, etc.).

In fact, most of us use this type of software without much thought to the issues surrounding its use. We hear about a great new open source software project, download the latest release, deploy the "Pet store" example, develop the "Hello World", and on we go using this software in every other project we work on. Open source software gives the developer more options, helps to increase the knowledge base among developers, and improves the overall programming prowess of the community.

With all the benefits associated with open source, developers might wonder what the issues are surrounding the use of open source software. What are the risks? Is this software really free? What if I need support? These, and several others discussed below, are all valid questions a developer should ask when considering whether an open source product should be incorporated into the developer's project.
Open Source: A Definition

Let's take a step back and define what open source software means. Some people use the term loosely to describe any software that is freely distributed with modifiable source code. An organization called the Open Source Initiative (OSI) maintains a more formal and strict definition on its web site). The OSI uses ten criteria in its definition, addressing issues such as the ability to freely distribute software, accessibility to source code, ability to derive works, and more.

The OSI also reviews and approves open source licenses to determine whether they meet the organization's standards for open source licensing. At the time of this writing, there are 58 different licenses approved by the OSI, including the GNU General Public License (GPL), the Apache License 2.0, and the Mozilla Public License 1.1 (MPL). Not all "open source" licenses, however, meet or exceed the OSI's criteria.
Communicating with Management

Typically, the developer finds management to be of two different mindsets when it comes to open source. They are either supportive because they believe it is an inexpensive solution, or they are not supportive because of the perceived risks associated with its use (this may stem from not understanding the paradigm, or not having the ability to purchase support).

In the first case, the developer may also find management to be overly enthusiastic in the use of open source software. This usually stems from the initial cost-savings aspect of using open source software. Often times, management hears how open source will ultimately save a project a lot of money. This may be the main reason why management will opt for the use of open source software. An example of this may be the migration of a software development project that has been implemented using a non-open source database such as Oracle 10g to an open source database (e.g. MySQL, PostgreSQL, or Firebird).

However, management may not understand the risks associated with migrating from one database to another. They may also be unaware of the process, time, and resources necessary to complete this project successfully. The developer may need to explain to management the risks and any potential roadblocks that could be encountered during the migration.

In the second case, the notion of open source software may seem enigmatic to management. The fact that source code is available may be thought of as a security risk for the company. Management may treat the lack of up-front cost for some open source projects as adding to overall risk. They may even think the product is not serious enough to be used in a mainstream production environment or software product.

In this scenario, the developer must take on the responsibility to educate management as to the capabilities of the open source product, and how the use of it can both help the "bottom-line" as well as improve the quality of the project.

As discussed in the next section, management should also know about the licensing ramifications associated with distributing any type of software. Often times, the developer is the only one who fully understands the license. In fact, many times the license is only available in the compressed source download. (Interestingly enough, it is a requirement to read the license before proceeding with uncompressing the file for some open source projects.)

Again the responsibility falls to the developer to properly educate management in any licensing issues that might occur. If management does not appear to understand the notion of open source, the developer may need to provide any necessary information as it pertains to open source prior to the explanation of the specific license. At times, the developer may be the sole educator in terms of open source understanding and licensure in the company.
Licensing

I know what you are saying, "Who really reads those, anyways?" Well, I do (the lawyers told me to say that). Remember what your parents said? Eat your veggies, take your vitamins, button up before you go out in the cold, and make sure to read that license before using any open source software (I would make sure to read licenses of non-open source software too, even though those are generally long and frightening).

Let's say a developer is creating an application that will be sold commercially, and let's say the developer integrates a component released under the Apache license. Using examples from a few different open source licenses, typical questions include,

    What are some of the specifics we must take into account in distributing the Mozilla software product with my application?
    Can I use parts of an Apache product in my application but not the whole thing?
    Since I am using a GNU product (which is released under the GNU GPL license), do I have to release my software under the GPL License?

Let's take the first situation. Assume the product we are selling requires Mozilla Firefox to be included in the distribution. In addition to this, let's assume the Firefox source code was modified. The first thing to determine is which Mozilla license Firefox is distributed under. According to Mozilla.org, it's products are distributed under the MPL or the Mozilla End-User-License-Agreement (EULA). Looking at the license file on my Firefox installation, I noticed the MPL 1.1 is what Firefox is distributed with for Windows.

Next, it is necessary to determine what was done to the Firefox browser. Since the Firefox source code was modified, there are procedures that must be followed to properly re-distribute the software. For instance, it is necessary to make source code available for modifications that were performed on Firefox. In addition, it is necessary to 'label' the changes that were made to the Firefox source code. According to the Mozilla.org site, it is possible to do this by way of diffs. The FAQs posted on the Mozilla.org site contain more information regarding the specifics that need to be taken into account in this situation.

The second scenario involves using a component of an Apache product, but not the whole thing. Let's say we are interested in using parts of the Apache Jakarta Commons code base. Apache makes separate components available for download such as the commons-pool component. This is a set of JAVA packages used for object pooling. The Apache License 2.0 addresses "Derivative Works" which is defined as any work in Source or Object form that is based on the specific Apache software project code. Similar to the Mozilla license, it is necessary to annotate any changes that were made to the source code if we choose to only use parts of it, and then properly include the license file as well as a place for anybody to obtain the modified source.

Our third scenario involves GNU. This one gets a little tricky. If you choose to use code that is distributed by GNU and it is distributed under the General Public License (GPL), then your code must conform to the GNU license. In fact, the license you must use to distribute your product may have to be the same GNU license. This is where open interpretation can get a little messy.

Traditionally, software that uses a library that is released under the GPL is required to be distributed under the same GPL license if the developer chooses to distribute the product (note: The GPL allows the developer not to distribute the product). This is not to say that it is not possible to use GNU-licensed software and proprietary software in conjunction with each other. To do this, the software must be logically separate as mandated by GNU (For more information, please see http://www.gnu.org).

Since these licenses all conform to the notion of what the OSI terms "open source" the answers should be very similar. However, there are slight differences that must be taken into account.

What happens if the open source license does not conform to the OSI specification? This does not necessarily mean that you should not consider using the product. But you do have to read the license and make sure you understand the terms and conditions.

Some non-OSI-compliant licenses are attached to software that is developed and released for academic purposes. This often occurs with the usage of university-based software development projects. Commercial users may find these licenses to be more strict.

You might also encounter "dual-licensing." One example of this is the MySQL database which is released under two licenses, one of which conforms to the OSI standards.
Project Management

There are also project management concerns that must be taken into account when adopting open source software as part of a development project. For example, you may need to allot more time in the initial phases of the project to allow the development team time for discovery and learning. A proof of concept/prototype phase may be a good idea. This may be very important especially if the development team has never used the product in the past.

The process should take into account risk management issues associated with the maturity of the open source product and developer knowledge of the product. Many times, the risks will only be understood by the developer, who will need to help the management team understand the risks.

Another area that can contribute to project management risk is the quality of documentation for the open source product. Although a diminishing stereotype, there are still open source projects that lack documentation. Developers will be developers. The expectation that code is good enough to understand the software may indicate that a project that is still in the inception phase.

The Proof of Concept phase becomes very important in this case because the developer will need to determine the viability of the software in question. This is not to say that all open source software is immature. In fact many open source projects contain more functionality and fewer bugs than Commercial Off the Shelf (COTS)-based equivalents.

Risk management may play an increased role depending on the maturity of the product. Although not ultimately a means of determining software maturity, a critical mass of developers on a project can speak to how stable a project is, and how much support the project will have. There are open source projects (like those of the Apache Software Foundation) that have over one thousand people looking at the source code, submitting bugs, patches, and enhancements. There is a full development lifecycle that is properly defined, documented, and followed.

Because of the high level of maturity, from a risk management standpoint many organizations treat software developed by well established organizations like the Apache Software Foundation or JBoss, Inc. as they would treat COTS-based software. There are also projects that are maintained by only one person—but these may be mature, well documented projects also. Evaluate on a case-by-case basis.
Support

Support works a little differently in the world of open source. For some projects, it is possible to purchase a support contract from either the creators of the software, or from a consulting company that is either partnered with the organization or that employs a contributing developer to the project. Other times, the only means of support include mailing lists, archives, a wiki, and documentation.

Management may see the lack of paid-for support as contributing to the overall risk of the project. The reality, however, is that having access to the developers on the project results in more expedited answers than going through the multiple levels of support one would find when contacting companies of COTS-based products. The developer may have to explain the support risks associated with an open source product and how that plays into the overall project risk matrix.
Etiquette

There are some unwritten rules a developer must follow when communicating with the open source community. First, the developer is expected to have read through the license, documentation, any available wikis, and most importantly the mail/discussion archives. It is the responsibility of the developer to seek out any answers using this medium before asking a question that was previously asked. If an error is found, it may have come up in the past and was addressed in the mail, newsgroup, or discussion forum archives.

Developers are also expected to install the software, deploy any examples available, and attempt to develop a "Hello World." If the developer is not successful at installing the software, then the expectation is that the developer will proceed to read through the installer/build scripts and any necessary source code, make modifications to the environment or build scripts and attempt to re-install the software.

If still not successful, the developer can then send mail or post a message to the community annotating what was performed, and what error is occurring. Sending a message stating that something does not work without having done the proper due diligence to figure it out for yourself is poor etiquette.

Although most people in the open source community do respond with advice regardless of whether the message was deemed intelligent, it is in your best interest to do your homework first.
Conclusion

The benefits associated with open source software can be realized as long as there is a plan to mitigate any risks that are part of using open source products. The ubiquity of open source software is increasing. Whether used as a small tool for software development, or as the core infrastructure for a COTS-based product, open source software will be incorporated into many specifics of an increasing number of software projects this year. Being aware of the specifics and issues around open source software will help the software developer plan appropriately for project success. Patterns of Enterprise Application Architecture does exactly what it sets out to do: provide the reader with a catalog of proven enterprise level patterns and the knowledge to use them. Fowler describes an architectural pattern as "...the major components of a system and how they interact" (p. 2) and "...the decisions that developers wish they could get right early on because they're perceived as hard to change" (p. 2).

While most of the patterns in this book are nothing new to experienced developers and architects, they do provide a common vocabulary that can be used to discuss these patterns. For the less experienced software developer, this book is a great opportunity to become familiar with many of the design issues you will come across and patterns that others have successfully used to overcome those issues.

Many times developers will find themselves looking for a better way to implement or improve a certain aspect of their application. The patterns in this book are those that you will find yourself using when you are looking for a way to improve your application. Generally these patterns are discovered through trial and error, however Fowler has conveniently provided us with catalog of the common architectural patterns that he and his team have successfully used.

Patterns of Enterprise Application Architecture is divided into two parts. The first part of the book is a narrative, which gives an overview of the patterns, including the pros and cons of each one. Each chapter is a logical grouping of patterns in domain areas such as Layering, Organizing Domain Logic, Mapping to Relational Databases, Web Presentation, Concurrency, Session State and Distribution Strategies.

The second part of the book is the actual catalog of patterns. Each chapter is again a grouping of similar patterns for quick reference. With each of the patterns comes a description of the pattern, a section on how the pattern works, when to use the pattern, pointers to additional reading, and examples. Most of the examples are written in Java, although the occasional C# example shows up. The examples are simplified for obvious reasons, however they are enough of an example that one can easily understand the pattern and extend and/or modify it as needed.

One pattern I found interesting was a pattern called Lazy Load, in the Object-Relational Behavioral Patterns chapter:
Lazy Load

An object that doesn't contain all of the data you need but knows how to get it.

For loading data from a database into memory it's handy to design things so that as you load an object of interest you also load the objects that are related to it. This makes loading easier on the developer using the object, who otherwise has to load all the objects he needs explicitly.

However, if you take this to its logical conclusion, you reach the point where loading one object can have the effect of loading a huge number of related objects--something that hurts performance when only a few of the objects are actually needed.

A Lazy Load interrupts this loading process for the moment, leaving a marker in the object structure so that if the data is needed it can be loaded only when it is used. As many people know, if you're lazy about doing things you'll win when it turns out you don't need to do them at all (p. 200).

Fowler then goes on to describe four different techniques for lazy loading: Lazy Initialization, Virtual Proxy, Value Holder, and Ghost Objects.

Even with this brief description you can immediately see the benefits of using the Lazy Load pattern. Maybe you've used this pattern or something similar in the past or maybe you see a way that this pattern can be a benefit if used in your current project. Either way it's important to carefully weigh the pros and cons when deciding to use a pattern. Architectural decisions are not decisions that can easily be changed once a project is in full swing and reams of code have been written. If an applications design has to be changed or modified midway through a project, serious setbacks will occur as your team struggles to fix the problem and then make up the time that was spent fixing the problem.

Overall, Fowler's narrative and explanation of each of the patterns provides more than enough information for the reader to walk away with a firm understanding of the pattern, when to use it, and when problems can occur with its use.

The best way I found to read this book was to first read a chapter from the discussion section of the book, then familiarize myself with the patterns discussed in that chapter, then go back and reread the discussion chapter if I found I wasn't familiar with one of the patterns. If you are a seasoned developer you can probably skip the discussion section of the book and go right to the catalog of patterns. If you are a new to intermediate level developer, reading the narratives will greatly help your understanding of the patterns.

This book assumes that you are familiar with UML and are familiar enough with Java to understand basic examples. A previous reading of Design Patterns (ISBN: 0201633612) is helpful but not necessary.

I would deem Patterns of Enterprise Application Architecture as a "must read" for anyone who does enterprise level development, whether new to the software field or a seasoned developer. Refactoring is essential reading for anyone involved in the construction of software using object oriented languages. Fowler and his fellow contributors do an incredible job of simultaneously introducing and formalizing a code improvement technique called "refactoring."

To quote Fowler from the Preface, "Refactoring is the process of changing a software system in such a way that it does not alter the external behavior of the code yet improves its internal structure. It is a disciplined way to clean up code that minimizes the chances of introducing bugs. In essence when you refactor you are improving the design of the code after it has been written."

This is a powerful concept since it free us as developers from the pressure of getting every absolute detail right the first time. When building any system, object oriented or not, even the best developer is not going to be able to capture every nuance and contingency in her code the first time. Designing systems and writing code is a constant process of making trade-offs. Hopefully we do our best and make all the right decisions the first time we design the system and write the code. Three things, though, prevent that from happening: one, we are not perfect; two, we don't always have all the necessary information required to make the "right" design decisions; and three, requirements change and new functionality needs to be added. Refactoring gives us a way to adjust the design without breaking the code.

After introducing the concept of refactoring, and delving into the details of the technique, the book walks through 72 "refactorings." This catalog idea is very similar to the catalog of design patterns that appears in the book Design Patterns by Erich Gamma, et al. Each refactoring has a name such as "Move Method" or "Replace Conditional with Polymorphism," and contains a detailed explanation of a design problem and solution. Performing a refactoring means making small, incremental changes to code, and retesting each step of the way so that the changes can be made with the least disruption possible. That may sound tedious, but it's actually quite straightforward and interesting to read. The book wraps up with a few chapters written by the contributing authors. These chapters tie together the material that makes up the bulk of the book, further grounding the techniques in the real world we all write code in.

I call this book "essential reading" for three reasons: first, the author's humble, almost self-deprecating, writing style sends exactly the right message to the reader: we are not perfect, and there is no need for us to be; software design and construction are not perfect processes, and there is no need for them to be. This is a message we all need to hear. Second, the book offers concrete guidance on performing the refactorings, with clear explanations, justifications, caveats, and examples. The catalog of refactorings will be the most lasting contribution of the book, and is what will keep you coming back to the book often. Third, reading through the catalog of refactorings is a suprisingly excellent primer/refresher on techniques to adopt and pitfalls to avoid when designing and building object oriented software. In this respect, it shares qualities with Steve McConnell's Code Complete and also Gamma et al's Design Patterns. Refactoring makes a perfect companion volume to these essential books.

Finally, as both a fellow writer and a reader, I'd like to commend the authors of Refactoring for bringing this important subject matter to us with a clear, conversational, decidedly non-academic style. More of this kind of writing is needed if the important ideas of software engineering are to be delivered to the majority of software developers, who have little interest in academia and its often dry, cumbersome, and wordy writing style--which serves its purpose in the world of research and academia, but which can try the patience of pratitioners. magine we are developing a web page for customizing a laptop purchase.

If you've never configured a laptop online before, take a look at Dell's "customize it" page for an entry level laptop. The web page presents eleven questions to the user that have from two to seven responses each. The user has to choose from two options in the first control, two in the second, and so on. The user has seven possible choices for the last control.

When we look at all of the controls combined, the user has to make (2,2,2,2,2,3,2,2,3,4,7) choices. This is a simple configuration problem. The number of possible laptop configurations that could be requested by the user is the product of all of the choices. In this very simple page, there are 32,256 possibilities. At the time of this writing, the page for customizing Dell's high-end laptop has a not dissimilar set of controls, with more choices in each control: (3,3,3,2,4,2,4,2,2,3,7,4,4). The user of this page can request any of 2,322,432 different laptop configurations! If Dell were to add one more control presenting five different choices, there would be over ten million possible combinations!

Creating a test suite that tries all two million combinations for a high end laptop could be automated, but even if every test took one tenth of second to run, the suite would take over 64 hours! Dell changes their product offerings in less time than that.

Then again, if we use a server farm to distribute the test suite across ten machines we could run it in about 6 hours. Ignoring the fact that we would be running this type of test for each customization page Dell has, 6 hours is not unreasonable.

Validating the two million results is where the really big problem is waiting for us. We can't rely on people to manually validate all of the outputs–it is just too expensive. We could write another program, which inspects those outputs and evaluates them using a rules-based system ("If the user selects 1GB of RAM, then the configuration must include 1GB of RAM" and "The price for the final system must be adjusted by the price-impact of 1GB of RAM relative to the base system price for this model.")

There are some good rules-based validation tools out there, but they are either custom software, or so general as to require a large investment to make them applicable to a particular customer. With a rules-based inspection system, we have the cost of maintaining the rules. The validation rules are going to have to be updated regularly, as Dell changes the way they position, configure, and price their laptops.

Since we aren't Dell, we don't have the scale (billions of dollars of revenue) to justify this level of investment. The bottom line for us is that we can't afford to exhaustively test every combination. Dell's shareholders require them to grow their business, and these configuration pages are the vehicle by which Dell generates billions of dollars in revenue. They have to test it. The cost of errors (crashes, lost sales, mis-priced items, invalid combinations of features) is too high. With this level of risk, the cost of not testing (the cost of poor quality) is extremely high.
We Can't Afford to Test It

I was able to attend a training session with Kent Beck a few years ago. I was also honored to be able to enjoy a large steak and some cold beer with him that night after the training. When asked how he responds to people who complain about the cost of quality, Kent told us he has a very simple answer: "If testing costs more than not testing then don't do it."

I agree. There are few situations where the cost of quality exceeds the cost of poor quality. These are situations where the needed infrastructure, test-development time, and maintenance costs outweigh the expected cost of having a bug. (The "expected cost" is the likelihood (as a percentage) of the bug manifesting in the field, multiplied by the cost of dealing with the bug.)

The techniques described in this article are designed to reduce the cost of quality, to make it even less likely that "not testing" is the best answer.
Just Test Everything, It's Automated!

Two "solutions" that we have to consider are to test nothing and to test everything. We would consider testing nothing if we can't afford to test the software. When people don't appreciate the complexities of testing or the limitations of automated testing, they are inclined to want to "test everything." Testing everything is much easier said than done.

Have you ever been on a project where the manager said something like, "I demand full testing coverage of the software. Our policy is zero tolerance. We won't have bad quality on my watch."?

What we struggle with here is the lack of appreciation for what it means to have "full coverage" or any other guarantee of a particular defect rate.

There are no absolutes in a sufficiently complex system–but that's ok. There are statistics, confidence levels, and risk-management plans. As engineers and software developers, our brains are wired to deal with the expected, likely, and probable futures. We have to help our less-technical brethren understand these concepts–or at least put them in perspective.

We may get asked, "Why can't we just test every combination of inputs to make sure we get the right outputs? We have an automated test suite–just fill it up and run it!"

We need to resist the urge to respond by saying, "Monkeys with typewriters will have completed the works of Shakespeare before we finish a single run of our test suite!"
Solving the Problem

There are a lot of applications that have millions or billons of combinations of inputs. They have automated testing. They have solutions to this problem. We just finished discussing how impractical it is to test exhaustively, so how do companies test their complex software?

In the rest of the article, we will explore the following approaches to solving the problem.

    Random sampling
    Pairwise testing
    N-wise testing

We will also explore the impact that changing the order of operations has on our testing approach, and the methods for testing when the sequence matters.
Random Sampling

Early on in the software testing world, someone realized that by randomly checking different combinations of inputs, they would eventually find the bugs. Imagine software that has one million possible combinations of inputs (half as complex as our previous example). Each random sample would give us 0.000001% coverage of all possible user sessions. If we run 1,000 tests, we would still only have 0.001% coverage of the application.

Thankfully, statistics can help us make statements about our quality levels. But we can't use "coverage" as our key measurement of quality. We have to think about things a little bit differently. What we want to do is express a level of confidence about a level of quality. We need to determine the sample size, or number of tests, that we need to run to make a statistical statement about the quality of the application.

First we define a quality goal–we want to assure that our software is 99% bug free. That means that up to 1% of the user sessions would exhibit a bug. To be 100% confident that this statement is true, we would need to test at least 99% of the possible user sessions, or over 990,000 tests.

By adding a level of confidence to our analysis, we can use sampling (selecting a subset of the whole, and extrapolating those results as being characteristic of the whole) to describe the quality of our software. We will leverage the mathematical work that has been developed to determine how to run polls.

We define our goal to be that we have 99% confidence that the software is 99% bug free. The 99% level of confidence means that if we ran our sample repeatedly, 99% of the time, the results would be within the margin of error. Since our goal is 99% bug free code, we will test for 100% passing of tests, with a 1% margin of error.

How many samples do we need, if there are one million combinations, to identify the level of quality with a 99% confidence, and a 1% margin of error? The math for this is readily available, and calculators for determining sample size are online and free. Using this polling approach, we find that the number of samples we require to determine the quality level with a 1% error and 99% confidence is 16,369.

If we test 16,369 user sessions and find 100% success, we have established a 99% confidence that our quality is at least at a 99% level. We only have 99% quality, because we have found 100% quality in our tests, with a 1% margin of error.

This approach scales for very large numbers of combinations. Consider the following table, where our goal is to establish 99% confidence in a 99% quality level. Each row in the following table represents an increasingly complex software application. Complexity is defined as the number of unique combinations of possible inputs). 

We can see that the very few additional tests have to be run to achieve the same level of quality for increasingly complex software. When we have a modest quality goal, such as 99/99 (99% confidence in 99% quality), this approach is very effective.

Where this approach doesn't scale well is with increasing levels of quality. Consider the quest for "five nines" (99.999% bug free code). With each increase in the desired level of quality, the number of tests we have to run grows. It quickly becomes an almost exhaustive test suite.

Each row in the following table represents an increasingly stringent quality requirement, with the complexity of the software staying constant at one million possible input combinations. The random sampling approach does not provide a benefit over exhaustive testing when our quality goals are high.
Pairwise Testing of Input Variables

Studies have shown that bugs in software tend to be the results of the combination of variables, not individual variables. This passes our "gut-check" since we know that conscientious developers will test their code. What slips through the cracks is overlooked combinations of inputs, not individual inputs.

Consider a very simple laptop configuration page, having three selectable controls: CPU, Memory, and Storage. Each control has three possible values as shown in the table below. Is software maintenance a problem?

Today's standard answer is "You bet it is."

The standard rationale for that standard answer is "Look how much of our budget we're putting into software maintenance. If we'd only built the software better in the first place, we wouldn't have to waste all that money on maintenance."

Well, I want to take the position that this standard answer is wrong. It's wrong, I want to say, because the standard rationale is wrong.

The fact of the matter is, software maintenance isn't a problem, it's a solution!

What we are missing in the traditional view of software as a problem is the special significance of two pieces of information:

    The software product is "soft" (easily changed) compared to other, "harder," disciplines.
    Software maintenance is far less devoted to fixing errors (17 percent) than to making improvements (60 percent).

In other words, software maintenance is a solution instead of a problem because in software maintenance we can do something that no one else can do as well, and because when we do it we are usually building new solutions, not just painting over old problems. If software maintenance is seen as a solution and not as a problem, does that give us some new insight into how to do maintenance better?

I take the position that it indeed does.

The traditional, problem-oriented view of maintenance says that our chief goal in maintenance should be to reduce costs. Well, once again, I think that's the wrong emphasis. If maintenance is a solution instead of a problem, we can quickly see that what we really want to do is more of it, not less of it. And the emphasis, when we do it, should be on maximizing effectiveness, and not on minimizing cost.

New vistas are open to us from this new line of thinking. Once we take our mindset off reducing costs and place it on maximizing effectiveness, what can we do with this new insight?

The best way to maximize effectiveness is to utilize the best possible people. There is a lot of data that supports that conclusion. Much of it is in the "individual differences" literature, where we can see, for example, that some people are significantly better than others at doing software things:

Debugging: some people are 28 times better than others.

Error detection: some people are 7 times better than others.

Productivity: some people are 5 times better than others.

Efficiency: some people are 11 times better than others.

The bottom line of these snapshot views of the individual differences literature is that there is enormous variance between people, and the best way to get the best job done is to get the best people to do it.

This leads us to two follow-on questions:

    Does the maintenance problem warrant the use of the best people?
    Do we currently use the best people for doing maintenance?

The first question is probably harder to answer than the second. My answer to that first question is "Yes, maintenance is one of the toughest tasks in the software business." Let me explain why I feel that way.

Several years ago I coauthored a book on software maintenance. In the reviewing process, an anonymous reviewer made this comment about maintenance, which I have remembered to this day:

Maintenance is:

    intellectually complex (it requires innovation while placing severe constraints on the innovator)
    technically difficult (the maintainer must be able to work with a concept and a design and its code all at the same time)
    unfair (the maintainer never gets all the things the maintainer needs. Take good maintenance documentation, for example)
    no-win (the maintainer only sees people who have problems)
    dirty work (the maintainer must work at the grubby level of detailed coding)
    living in the past (the code was probably written by someone else before they got good at it)
    conservative (the going motto for maintenance is "if it ain't broke, don't fix it")

My bottom line, and the bottom line of this reviewer, is that software maintenance is pretty complex, challenging stuff.

Now, back to the question of who currently does maintenance. In most computing installations, the people who do maintenance tend to be those who are new on the job or not very good at development. There's a reason for that. Most people would rather do original development than maintenance because maintenance is too constraining to the creative juices for most people to enjoy doing it. And so by default, the least capable and the least in demand are the ones who most often do maintenance.

If you have been following my line of reasoning here, it should be obvious by now that the status quo is all wrong. Maintenance is a significant intellectual challenge as well as a solution and not a problem. If we want to maximize our effectiveness at doing it, then we need to significantly change the way in which we assign people to it.

I have specific suggestions for what needs to be done. They are not pie-in-the-sky theoretical solutions. They are very achievable, if management decides that it wants to do them:

    Make maintenance a magnet. Find ways to attract people to the maintenance task. Some companies do this by paying a premium to maintainers. Some do this by making maintenance a required stepping stone to upper management. Some do this by pointing out that the best way to a well-rounded grasp of the institution's software world is to understand the existing software inventory.
    Link maintenance to quality assurance. (We saw this in the previous essay.)
    Plan for improved maintenance technology. There are now many tools and techniques for doing software maintenance better. (This has changed dramatically in the last couple of years.) Training and tools selection and procurement should be high on the concerned maintenance manager's list of tasks.
    Emphasize "responsible programming." The maintainer typically works alone. The best way to maximize the effectiveness of this kind of worker is to make them feel responsible for the quality of what they do. Note that this is the opposite of the now-popular belief in "egoless programming," where we try to divest the programmer's personal involvement in the final software product in favor of a team involvement. It is vital that the individual maintainer be invested in the quality of the software product if that product is to continue to be of high quality.

There they are...four simple steps to better software maintenance. But note that each of those steps involves changing a traditional software mindset. The transition is technically easy, but it may not be socially or politically quite so easy. Most people are heavily invested in their traditional way of looking at things.

If we are to get there at all, however, there is one vital first step which must be taken. It is the step that started off this essay.

We must see that software maintenance is a solution, not a problem. If we agree to that, then we have opened the door to profound changes in how we software people do our business. Think about it. What exactly is software development, and why is it so hard? This is a question that continues to engage our thoughts. Is software development an engineering discipline? Is it art? Is it more like a craft?

We think that it is all of these things, and none of them. Software is a uniquely human endeavor, because despite all of the technological trimmings, we're manipulating little more than the thoughts in our heads. That's pretty ephemeral stuff. Fred Brooks put it rather eloquently some 30 odd years ago[Bro95]:

    "The programmer, like the poet, works only slightly removed from pure thought-stuff. He builds his castles in the air, from air, creating by exertion of the imagination. Few media of creation are so flexible, so easy to polish and rework, so readily capable of realizing grand conceptual structures. (As we shall see later, this very tractability has its own problems.)"

In a way, we programmers are quite lucky. We get the opportunity to create entire worlds out of nothing but thin air. Our very own worlds, complete with our own laws of physics. We may get those laws wrong of course, but it's still fun.

This wonderful ability comes at a price, however. We continually face the most frightening sight known to a creative person: the blank page.
1. Writer's Block

Writers face the blank page, painters face the empty canvas, and programmers face the empty editor buffer. Perhaps it's not literally empty—an IDE may want us to specify a few things first. Here we haven't even started the project yet, and already we're forced to answer many questions: what will this thing be named, what directory will it be in, what type of module is it, how should it be compiled, and so on.

The completely empty editor buffer is even worse. Here we have an infinite number of choices of text with which to fill it.

So it seems we share some of the same problems with artists and writers:

    How to start
    When to stop
    Satisfying the person who commissioned the work

Writers have a name for difficulties in starting a piece: they call it Writer's Block.

Sometimes writer's block is borne of fear: Fear of going in the wrong direction, of getting too far down the wrong path. Sometimes it's just a little voice in your head saying "don't start yet". Perhaps your subconscious is trying to tell you that you're missing something important that you need before you can start.

How do other creative artists break this sort of logjam? Painters sketch; writers write a stream of consciousness. (Writers may also do lots of drugs and get drunk, but we're not necessarily advocating that particular approach.)

What then, is the programming equivalent of sketching?
Software Sketches

Sometimes you need to practice ideas, just to see if something works. You'll sketch it out roughly. If you're not happy with it, you'll do it again. And again. After all, it takes almost no time to do, and you can crumple it up and throw it away at the end.

For instance, there's a pencil sketch by Leonardo da Vinci that he used a study for the Trivulzio equestrian monument. The single fragment of paper contains several quick sketches of different views of the monument: a profile of the horse and rider by themselves, several views of the base with the figures, and so on. Even though the finished piece was to be cast in bronze, da Vinci's sketches were simply done in pencil, on a nearly-scrap piece of paper. These scribblings were so unimportant that they didn't even deserve a separate piece of paper! But they served their purpose nonetheless.[1]

Pencil sketches make fine prototypes for a sculpture or an oil painting. Post-It notes are fine prototypes for GUI layouts. Scripting languages can be used to try out algorithms before they're recoded in something more demanding and lower level. This is what we've traditionally called prototyping: a quick, disposable exercise that concentrates on a particular aspect of the project.

In software development, we can prototype to get the details in a number of different areas:

    a new algorithm, or combination of algorithms
    a portion of an object model
    interactions and data flow between components
    any high-risk detail that needs exploration

A slightly different approach to sketching can be seen in da Vinci's Study for the Composition of the Last Supper. In this sketch, you can see the beginnings of the placement of figures for that famous painting. The attention is not placed on any detail—the figures are crude and unfinished. Instead, da Vinci paid attention to focus, balance and flow. How do you arrange the figures, position the hands and arms in order to get the balance and flow of the entire piece to work out?

Sometimes you need to prototype various components of the whole to make sure that they work well together. Again, concentrate of the important aspects and discard unimportant details. Make it easy for yourself. Concentrate on learning, not doing.

As we say in The Pragmatic Programmer[HT00], you must firmly have in your head what you are doing before you do it. It's not at all important to get it right the first time. It's vitally important to get it right the last time.
Paint Over It

Sometimes the artist will sketch out a more finished looking piece, such as Rembrandt's sketch for Abraham's Sacrifice Of Isaac in 1635. It's a crude sketch that has all of the important elements of the final painting, all in roughly the right areas. It proved the composition, the balance of light and shadow, and so on. The sketch is accurate, but not precise. There are no fine details.

Media willing, you can start with such a sketch, where changes are quick and easy to make, and then paint right over top of it with the more permanent, less-forgiving media to form the final product.

To simulate that "paint over a sketch" technique in software, we use a Tracer Bullet development. If you haven't read The Pragmatic Programmer yet, here's a quick explanation of why we call it a Tracer Bullet.

There are two ways to fire a big artillery gun. The first way is to carefully measure the distance to the target, compensate for wind speed and direction, the weight of the ordinance, and so on, crunch all the numbers and give the orders to fire:

"Range 1000!"

whirr. click.

"Elevation 7.42!"

whirr. click.

"Azimuth 3.44"

whirr. click.

"FIRE!"

BOOM. Oh bad luck, there. Missed.

"Range 2015!"

whirr. click.

"Elevation 9.15!"

etc. . .

By the time you've set up, checked and rechecked the numbers, and issued the orders to the grunts manning the machine, the target has long since moved.

In software, this kind of approach can seen in any method that emphasizes planning and documenting over producing working software. Requirements are generally finalized before design begins. Design and architecture, detailed in exquisite UML diagrams, is firmly established before any code is written (presumably that would make coders analogous to the "grunts" who actually fire the weapon, oblivious to the target).

Don't misunderstand: if you're firing a really huge missile at a known, stable target (like a city), this works out just great and is the preferable way to go. If you're shooting at something more maneuverable than a city, though, you need something that provides a bit more real-time feedback.

Tracer bullets.

With tracer bullets, you simply fill the magazine with phosphorus-tipped bullets spaced every so often. Now you've got streaks of light showing you the path to the target right next to the live ammunition.

For our software equivalent, we need a skeletally thin system that does next to nothing, but does it from end to end, encompassing areas such as the database, any middleware, the application logic or business rules, and so on. Because it is so thin, we can easily shift position as we try to track the target. By watching the tracer fire, we don't have to calculate the effect of the wind, or precisely know the location of the target or the weight of the ammunition. We watch the dynamics of the entire system in motion, and adjust our aim to hit the target under actual conditions.

As with the paintings, the important thing isn't the details, but the relationships, the responsibilities, the balance, and the flow. With a proven base—however thin it may be—you can proceed in greater confidence towards the final product.
Group Writer's Block

Up till now, we've talked about writer's block as it applies to you as an individual. What do you do when the entire team has a collective case of writer's block? Teams that are just starting out can quickly become paralyzed in the initial confusion over roles, design goals, and requirements.

One effective way to get the ball rolling is to start the project off with a group-wide, tactile design session. Gather all of the developers in a room[2] and provide sets of Lego blocks, plenty of Post-It notes, whiteboards and markers. Using these, proceed to talk about the system you'll be building and how you think you might want to build it.

Keep the atmosphere loose and flexible; this gets the team comfortable with the idea of change. Because this is low inertia design, anyone can contribute. It's well within any participant's skills to walk up to the whiteboard and move a PostIt-note, or to grab a few Lego blocks and rearrange them. That's not necessarily true of a CASE tool or drawing software: those tools do not lend themselves readily to rapid-feedback, group interaction.

Jim Highsmith offers us a most excellent piece of advice: The best way to get a project done faster is to start sooner. Blast through that writer's block, and just start.
Just Start

Whether you're using prototypes or tracer bullets, individually or with a group, you're working—not panicking. You're getting to know the subject, the medium, and the relationship between the two. You're warmed up, and have started filling that blank canvas.

But we have one additional problem that the painters do not have. We face not one blank canvas per project, but hundreds. Thousands, maybe. One for every new module, every new class, every new source file. What can we do to tackle that multiplicity of blank of canvases? The Extreme Programming[Bec00] notion of Test First Design can help.

The first test you are supposed to write—before you even write the code—is a painfully simple, nearly trivial one. It seems to do almost nothing. Maybe it only instantiates the new class, or simply calls the one routine you haven't written yet. It sounds so simple, and so stupid, that you might be tempted not to do it.

The advantage to starting with such a trivial test is that it helps fill in the blank canvas without facing the distraction of trying to write production code. By just writing this very simple test, you have to get a certain level of infrastructure in place and answer the dozen or so typical startup questions: What do I call it? Where do I put it in the development tree? You have to add it to version control, and possibly to the build and/or release procedures. Suddenly, a very simple test doesn't look so simple any more. So ignore the exquisite logic of the routine you are about to write, and get the one-line test to compile and work first. Once that test passes, you can now proceed to fill in the canvas—it's not blank anymore. You're not writing anything from scratch, you're just adding a few routines. . . .
2. When to Stop

We share another problem with painters: knowing when to stop. You don't want to stop prematurely; the project won't yet be finished.[3] But if you don't stop in time, and keep adding to it unnecessarily, the painting becomes lost in the paint and is ruined.

There's only one way avoid either trap: feedback. Before you even start a particular task, you have to have a way to determine that you're done. For example:

A. . .
	

is done when. . .

Project
	

Customer accepts

Development
	

Passes functional tests

Module
	

Passes unit tests

Bug fix
	

Test that previously failed now passes

Meeting
	

objective for meeting achieved

Document
	

Deliver exactly what's needed

Talk
	

Done when audience throws rotten fruit

Paper
	

You are still reading this, right?

We had a client once who seemed to have some difficulty in the definition of "done" with regard to code. After toiling for weeks and weeks on a moderately complex piece of software, Matthew (not his real name) proudly announced the Code Was Done. He went on to explain that it didn't always produce the correct output. Oh, and every now and again, the code would crash for no apparent reason. But it's done. Unfortunately, wishful thinking alone doesn't help us get working software out to users.

It's easy to err on the other side of the fence too—have you ever seen a developer make a career of one little module? Have you ever done that? It can happen for any number of political reasons ("I'm still working on XYZ, so you can't reassign me yet"), or maybe we just fall in love with some particularly elegant bit of code. But instead of making the code better and better, we actually run a huge risk of ruining it completely. Every line of code not written is correct—or at least, guaranteed not to fail. Every line of code we write, well, there are no guarantees. Each extra line carries some risk of failure, carries an additional cost to maintain, document, and teach a newcomer. When you multiply it out, any bit of code that isn't absolutely necessary incurs a shockingly large cost. Maybe enough to kill the project.

How then, can we tell when it's time to stop?
Painting Murals

Knowing when to stop is especially hard when you can't see the whole thing that you're working on. Mural painting, for instance, takes a special eye. In corporate software development, you may only ever see the one little piece of detail that you're working on. If you watch mural painters up close, it's quite difficult to discern that the splash of paint they're working on is someone's hand, or eyeball. If you can't see the big picture, you won't be able to see how you fit in.

The opposite problem is even worse—suppose you're the lone developer on a project of this size. Most muralists are simply painting walls, but anyone who's ever painted their house can tell you that ceilings are a lot harder than walls, especially when the ceiling in question covers 5,000 square feet and you have to lie on your back 20 meters above the floor to paint it. So what did Michelangelo do when planning to paint the Sistine Chapel? The same thing you should do when faced with a big task.

Michelangelo divided his mural into panels: separate, free-standing areas, each of which tells a story. But he did so fairly carefully, such that the panels exhibit these characteristics:

    High cohesion
    Low coupling
    Conceptual integrity

These are things we can learn from.
Cohesion

What is cohesion? As used here, cohesion refers to the panel's focus and clarity of purpose. In the Sistine Chapel ceiling, each panel tells a single Old Testament story—completely, but without any extraneous elements.

In software, the Unix command line tool's philosophy of small, sharp tools ("do one thing and do it well") is one example. Each tool is narrowly focused on it's primary task. Low cohesion occurs when you have giant "manager" classes that try to do too many disparate things at once.
Coupling

Coupling is related to orthogonality[HT00]: unrelated things should remain, well, unrelated. Following the object-oriented principle of encapsulation helps to prevent unintended coupling, but there are still other ways to fall into the coupling trap. Michelangelo's panels have low coupling; they are all self-contained; there are no instances of figures reaching from one panel into the next, for instance. Why is that important?

If you look closely at one of the panels that portrays angels gliding about the firmament of heaven, you'll notice that one of the angels is turning his back to, and gliding away from, the other angels. You'll also notice that said angel isn't wearing any pants. He's rather pointedly "mooning" the other angels.

There is surely a tale that explains the bare tail of the mooning angel, but for now let's assume that the Pope discovered the mooning angel and demanded that it be replaced. If the panels weren't independent, then the replacement of one panel would entail replacing some adjacent panels as well—and if you had to use different pigments because the originals weren't available, maybe you have to replace the next set of panels that were indirectly affected. Let the nightmare begin. But as it stands, the panels are independent, so the offending angel (who was apparently on Spring Break) could have been easily replaced with a less caustic image and the rest of the project would remain unaffected.
Conceptual Integrity

But despite that independence, there is conceptual integrity—the style, the themes, the mood, tie it all together. In computer languages, Smalltalk has conceptual integrity, so does Ruby, so does C. C++ doesn't: it tries to be too many things at once, so you get an awkward marriage of concepts that don't really fit together well.

The trick then is to divide up your work while maintaining a holistic integrity; each Sistine Chapel panel is a separate piece of art, complete unto itself, but together they tell a coherent story.

For our projects, we have several techniques we need to use inside code, including modularity, decoupling, and orthogonality. At the project level, consider architecting the project as a collection of many small applications that work together. These interacting applications might simply use a network connection or even flat files, or a heavier-duty component technology such as Enterprise Java Beans (EJB).
Time

Up until now, we've concentrated on splitting up a project in space, but there is another very import dimension that we need to touch on briefly—time. In the time dimension, you need to use iterations to split up a project.

Generally speaking, you don't want to go more than a few weeks without a genuine deliverable. Longer than that introduces too large of a feedback gap—you can't get the feedback quickly enough in to act on it. Iterations need to be short and regular in order to provide the most beneficial feedback.

The other important thing about iterations is that there is no such thing as 80% done. You can't get 80% pregnant—it's a Boolean condition. We want to get to the position where we only ship what really works, and have the team agree on the meaning of words like "done". If a feature isn't done, save it for the next iteration. As the iterations are short, that's not too far off.

In time or space, feedback is critical. For individual pieces of code, it is vital to have competent unit tests that will provide that feedback. Beware of excuses such as "oh, that code's too complicated to test." If it's too complicated to test, then it logically follows that the code is too complicated to write! If the code seems to be too complicated to test, that's a warning sign that you have a poor design. Refactor the code in order to make it easy to test, and you'll not only improve the feedback loop (and the future extensibility and maintainability of the system), you'll improve the design of the system itself.
3. Satisfying the Sponsor

Now comes the hard part. So far, we've talked about problems that have simple, straightforward answers. Organize your system this way; always have good unit tests; look for and apply feedback to improve the code and the process; etc. But now we're headed into much more uncertain terrain—dealing with people. In particular, dealing with the sponsor: the person or persons who are paying to make this project happen. They have goals and expectations all their own, and probably do not understand the technology with which we create the work. They may not know exactly what they want, but they want the project to come out perfect in the end.

This must be the artist's worst nightmare. The person paying for the portrait is also sitting for it, and says simply "Make me Look Good". The fact that the sitter is royalty who commands a well-oiled guillotine doesn't help. Sounds pretty close to the position we find ourselves in as we write software, doesn't it?

Let's look at it from the sitter's point of view. You commission an artist to paint you. What do you get? Perhaps a traditional, if somewhat flat looking portrait such as da Vinci's Portrait of Ginevra Benci in 1474. Or maybe the realistic, haunting face of Vermeer's Girl With a Pearl Earring. How about the primitive (and topless) look of Matisse's Seated Figure, the wild and fractured Portrait of Picasso by Juan Gris, or the stick-figured jumble of Paul Klee's Captive?

All of these are portraits, all interpretations of a commonplace thing—a human face. All of which correctly implement the requirements, but all of which will not satisfy the client.
Beyond The Obvious

Each of these paintings captures the essence of a person, not just the form. More than simple photographs, each painting looks below the surface to capture something that the camera can't. As programmers, we must do the same thing, only we tend to call it abstraction.

The phrase "requirements gathering" implies that requirements are simply lying about, ready to be scooped up and worked on. That's akin to a simple photograph, in that it only examines the obvious, surface level elements. In order to emulate the painter, we need to go beyond what's asked for. We need to ask the wicked questions to help the client discover what's really needed.

Systems Thinking[Sen90] suggests asking a minimum of five "whys" beyond the first one. The classic example involves a factory floor where the consultant notices a small puddle of oil on the floor. He asks the shop manager about it, who grumbles and barks an order to the cleaning crew to get over here and clean up the oil. But the consultant persists: why is the oil there? The manager says it's the cleaning crew's fault. But where did the oil come from?

A little investigating and more than five "why" questions later, it turns out that an overly cost-conscious purchasing agent got a deal on cases of O-ring seals for the overhead pipes. Problem was, the rings were the wrong size—that's why they were such a deal. What seemed like a cost savings was in fact costing quite a bit of money in various ways.

We once were approached to develop a complex, enterprise-level data processing system that mail room staff would use to coordinate, sort, and track incoming payment checks prior to their distribution to the correct department. The company's current manual procedure was error-prone and unreliable; checks were being lost or misrouted to the destination department.

What's the real requirement here? A fancy system to sort and catalog mail for the sole purpose of delivering it to the right address? Hmm. Seems like there's already a system in place that handles that sort of thing. So instead of a nice, fat, year-long contract, we told the company to use a different postal address for each department. Let the Post Office do the sorting, hopefully without opening the pieces and losing the checks.

Requirements are rarely simple, and shouldn't be taken at face value. Remember, a portrait is more than just a picture.
Conventional Wisdom

Even stories about requirements may need deeper examination.

There's a marvelous story of technology and consultants gone wild, developing the Fisher Space Pen. The story goes that the U.S. Government spent millions of dollars of taxpayer's money developing a space pen—a pen that the astronauts could take to the moon that would operate in the harsh conditions of weightlessness, extreme heat and cold. Technology rushes to the rescue, and develops a miracle pen that can write upside down in a boiling toilet.

The Russians, by comparison, decided to use a pencil.

A marvelous tale of an inappropriate solution, except for one small problem. It's not true. Both the Russian and the U.S. astronauts used pencils at first, but there was a danger of the leads breaking and shorting out electric components, and the wood of the pencil itself was combustible as well. In a pure oxygen atmosphere, that's a really bad thing. The Fisher corporation realized this and, at its own cost, designed the Fisher Space Pen, which it then sold to NASA at reasonable cost. After the disastrous Apollo One fire, NASA made the Fisher pens mandatory.

Fisher listened to the real requirement, even before the client knew it. In time, NASA came to realize that they were right. It was an appropriate use of high-technology to solve a very real problem.
Technology For It's Own Sake

Of course, there's always the inappropriate solution: engineering for it's own sake. As luck would have it, we happen to have an anecdote for this case as well.

There was this company that had developed a sophisticated video camera that could pan and tilt, looking for a subject in its field of view. A wonderful, high-tech solution in search of a problem. In time, the company sold this technology to a government agency to help take pictures for driving licenses. You'd go into the licensing agency and have a seat in front of the machine, which would whir and click, grind and gyrate until it had locked onto your face. The flash would fire, and in a few minutes your completed driver's license would be ready.

One day, 58 year-old Fred complained that the pretty 20 year-old blonde girl on his license just didn't look much like him.

The company and the government agency kinda scratched their heads; they weren't sure what the problem was. Problems like Fred's were popping up over, but other then getting a bunch in a row, there didn't seem to be any pattern to it. Finally, the police started to complain—and got quite upset—when they started seeing driver's licenses that featured beaming, cartoon smiley faces instead of a photo.

They discovered that the technology had gone awry: in some cases, the camera wouldn't get a lock, and would simply continue to grind and whir, looking all over the room for the subject. After a few minutes of watching the camera carefully inspect the ceiling and windows, folks like Fred would get bored and wander off. The next driver comes in, and with a flourish of clicks and whirs, the camera would snap their picture—and associate it with the previous driver's license.

Now the office staff figured out pretty quickly what the problem was, but they had no feedback path to the developers. They knew that once the machine got out of sync, they'd get bad licenses all day. So one clever user figured out that one could draw a happy face with marker on a piece of white paper, stick that over the chair, and the machine would happily snap the picture.

The real requirements were ignored in the rush to be clever, with predictably poor results.
How We Do It

So how do you find out what's in the client's head? At The Pragmatic Programmer's offices, we use "special equipment" (picture a 1950's mad scientist's laboratory replete with buzzing vacuum tubes, arcing Jacob's Ladders, and cranial implants). If that doesn't work, or if we're out in the field where health and safety restrictions prevent us from using our "special equipment", we resort to the old fashioned method of asking questions, both of the client and of ourselves.

What is the user's level of sophistication? What is the context in which the software is used? Real-time on the factory floor? In a life-critical system? For a home grocery list? What is the lifetime of the application? Unused after next week, or do you need to worry about the year-2038 bug? What are the risks? Not just the development or technical risks, but what are the sponsor's risks in taking on this project?

The best way to get these questions answered, of course, is to always involve the users as you go along. Seek frequent feedback to make sure you hear stories about anyone making smiley faces as soon as it happens.[4] Maintain short iterations with frequent deliveries, and work with the real users directly as much as possible. User representatives (such as a supervisor, manager or director) generally aren't as representative as we'd all like to think.

In our perpetual rush to jump in and start coding to the first neat idea we come across, we run the risk of getting locked in to a half-baked idea too early. Instead, try to cultivate emergence: Allow the solution to find itself where you can. Part of a developer's job is to provide a fertile ground in which ideas can grow. This means having code that is agile: code that supports rapid reassembly so you can try things out. Code that is easy to refactor, or that uses flexible configuration and/or metadata to facilitate rapid—but reliable—change, bolstered by a reliable safety net of complete revision control and competent unit tests.

Does all of this really work?

Yes, it does. We've done it successfully, we know other people who've done it successfully. It's lot of work, and it's a lot of hard work, and despite our best intentions, it might still not be a success due to factors beyond our control. So why do we bother with it all?

Because, as Brooks said, we programmers create. We can create awe-inspiring works with little more than the exertion of the imagination. Why do we do it? We do it for the pleasure of watching them show it off to others, of watching them use in novel ways we'd never imagined. For the thrill of watching millions on millions of dollars in transactions flow through your application, confident in the results. For the joy of building and being part of a team, and for the satisfaction of knowing that you started with a blank canvas and produced a work of art. And if you've gone to all that trouble, we think you should "sign your work". You should be proud of it.

It is, after all, a work of art. My very first job in the computer software business was as an entry level help desk technician. I had been a computer user for many years (since Dad brought home the family’s first Tandy Color Computer), but truly I knew very little about how computers worked. Sure, in high school and college I had written a few BASIC and Pascal programs, but none of that knowledge had stuck with me. At that moment, I was vastly under qualified to support my new employer’s vertical market accounting software.

I joined this tiny software firm on the cusp of the 1.0 release of their first application. If I remember correctly, when I came on board they were in the process of running the floppy disk duplicator day and night, printing out address labels, and packaging up the user documentation. As I pitched in to help get this release out the door, little did I know that I was about to learn a lesson about software development that I will never forget.

The shipments all went out (about a thousand of them I think, all advance orders), and we braced ourselves for the phone to start ringing. In the meantime, I was poring over Peter Norton’s MS-DOS 5.0 book, which was to become my best friend in the coming months. We knew the software had hit the streets when the phone started ringing off the hook. It was insane. The phone would not stop ringing. Long story short, the release was a disaster.

Many people could not even get it installed, and those people who could were probably less happy than the ones who could not. The software was riddled with bugs. Financial calculations were wrong; line items from one order would mysteriously end up on another order; orders would disappear altogether; the reports would not print; indexes were corrupted; the menus were out of whack; cryptic error messages were popping up everywhere; complete crashes were commonplace; tons of people did not have enough memory to even run the application. It was brutal. Welcome, Dan, to the exciting world of software.

Eventually, we just turned off the phones and let everyone go to voice mail. The mailbox would fill up completely about once an hour, and we would just keep emptying it. We could not answer the phones fast enough, and when we did, people were just screaming and ranting. One guy was so mad that several nights in a row he faxed us page after page after page of solid blackness, killing all the paper and ink in our fax machine.

It took us months to dig us out of this hole. We put out several maintenance releases, all free of charge to our customers. We worked through the night many times, and I slept on the floor of the office more than once. Really the only thing that saved us was our own tenacity and the fact that our customers did not have any other place to go. Our software was somewhat unique.

It was obvious to everyone in our company what caused this disaster: bad code. The company had hired a contract developer to write the software from scratch, and, with some help from a couple of his colleagues, this guy wrote some of the worst code I have ever seen. (Thom, if by some slim chance you’re reading this, I’m sorry man, but it was bad). It was total spaghetti. As I learned over the years about cohesion, coupling, commenting, naming, layout, clarity, and the rest, it was always immediately apparent to me why these practices would be beneficial. Wading through that code had prepared me to receive this knowledge openly.

I stayed with the company for three years, and we eventually turned the product into something I am still proud of. It was never easy, though. I swear I packed ten years of experience into those three years. My time working with that software, that company, and the people there who mentored me have shaped all of my software development philosophies, standards, and practices ever since.

When I got some distance from the situation, I was able to articulate to myself and others the biggest lesson I learned there: software can have a huge impact on the lives of real people. Software is not just an abstraction that exists in isolation. When I write code, it’s not just about me, the code, the operating system, and the database. The impact of what I do when I develop software reaches far beyond those things and into people’s lives. Based on my decisions, standards, and commitment to quality (or lack of it), I can have a positive impact or a negative one. Here is a list of all of the people who were effected negatively by that one man’s bad code:

    Hundreds of customers, whose businesses were depending on our software to work, and who went through hell because of it.
    The families of those customers, who were deprived of fathers and mothers that had to stay up all night re-entering corrupted data and simply trying to get our software to work at all. (I know, because I was on the phone with them at three in the morning.)
    The employees of these customers who had to go through the same horrible mess.
    The owner of our company (who was not involved in the day-to-day operations), whose reputation and standing was seriously damaged by this disaster, and whose bank account was steadily depleted in the aftermath.
    The prominent business leaders in the vertical market who had blindly endorsed and recommended our software—their reputations were likewise damaged.
    All of the employees of our company, for obvious reasons.
    All of our families, significant others, etc.—again for obvious reasons.
    All of the future employees of the company, who always had to explain and deal with the legacy of that bad code and that disastrous first release.
    The programmer himself, who had to suffer our wrath, and who had to stay up all night for many, many nights trying to fix up his code.
    The family of that programmer (he had several children) who hardly saw him for several weeks.
    The other developers (including myself) who had to maintain and build on that code in the years to follow.

That’s a lot of people, numbering in the thousands--but only one developer’s code.

Quite often, when corporations consider the possibility of doing some or all of their software development offshore (typically in Eastern Europe or India), the decision is made without considering all of the factors necessary to arrive at the best decision.

With companies that already have internal development capability, careful consideration needs to be given to the strategic importance that this capability provides.  Unfortunately, in many companies executive management often misunderstands the process of software development.  They frequently view the software engineering community and its professionals as a management challenge and the product that they produce as a commodity without regard to the value proposition that this capability can deliver.

The promise of lower hourly rates with offshore development is typically the "public" justification given for considering the possibility of going offshore, but often the true financial comparison is not given much more consideration than this simple comparison.  This promise, combined with the desire to unburden the organization of internal development groups, forms the basis for sending the work offshore.

However, deeper consideration can often show that the perceived advantage quite often is not realized and that the organization could suffer in the long haul.  The strategic importance of the software product being developed internally can often dwarf the perceived short term savings of a lower hourly rate.  In addition, frequently the lower offshore rate does not result in lower overall development costs.

I recently did an analysis for a company that had multiple in-house development groups and one group that used offshore development exclusively.  The organization claimed that the cost savings realized going offshore was substantial, based on the fact that the offshore company charged about $18 per hour, while the in-house groups had a "rate" calculated at over $60 per hour.

In reality, a comparison of total development cost as a percentage of the sales generated in each group revealed a different conclusion.  The group doing development offshore had a development expense as a percent of sales that was over two times the groups using internal development resource.  So not only was the company not saving money with the offshore resource, but also they also truly did not understand the total picture that made up their development expenses.

So in this scenario, what were the factors that were overlooked?

    Less overhead for internal developers—typically, the relationship with offshore developers requires extremely detailed specifications in order to communicate the software product being developed.  While specs are necessary with internal development teams, the team can afford less rigor in this area, as issues are more easily resolved locally.
    The proximity of the development team to the "internal" customer (i.e. Sales, Marketing, Product Management) makes an iterative development process during the design/development portion of the project efficient and effective.
    Typically, an internal development team has knowledge of the industry, the channel, and the customer that the product is being developed for.  This knowledge improves efficiency, accuracy, and ultimately provides value that is difficult or impossible for an offshore developer to deliver.
    Qualifying the software product with the customer (assuming the customer is in the U.S.) can also be lead to more logistics difficulty, additional communication between the client and offshore developer, and overall less efficiency.

Costs aside, the organization looking for an offshore solution must also consider a host of other factors regarding intellectual property security, potential legal issues with export control policies, and potential difficulties with project oversight.

Rather than focus on cost, the organization dependent on software products may receive a larger payback from more focus on the benefits their internal team can deliver in terms of innovation and the strategic value that their current organization can deliver.  Software development is by nature an experimental and iterative process—organizations should not only accept this fact, but they should embrace it as a competitive advantage in an ever-increasing world of "me too" thinking.

Welcome to the first in a series of interviews that will appear in the developer.* web magazine. This series will be devoted to interviews with representatives from various organizations that software developers could join, support, or otherwise become involved with. There are many software- or programmer-oriented organizations out there, and many (most) software developers don’t know anything about them. The purpose of these interviews is two-fold: first, to discuss a variety of issues of interest to software developers, and second, to give each organization an opportunity to explain why they exist, what they do, and why software developers might want to become involved.

The organizations chosen for interviews will be of different types. Some may be issue advocacy or lobbying organizations, others might be social clubs or professional societies. Neither developer.* nor its editor and authors are necessarily endorsing any organization that appears in this series. The point is not to advocate for these organizations. Rather, we merely want to create a forum for ideas. If a developer.* reader decides to get involved, all the better. If you know of an organization that should be included in this series, please let us know. Enjoy!

The following interview was conducted during the last week of July via e-mail between Daniel Read of developer.* and Paul Hanrahan of the Programmer’s Guild. Paul is a founding member of the Guild and a member of its board. The Programmer’s Guild web address is www.programmersguildusa.com.

Daniel Read: What is the difference between a guild and a union, and is one better than the other for programmers in particular?

Paul Hanrahan: Historically Unions may have emerged from the medieval Guild system. Guilds in their current incarnation can be hard to distinguish from unions, as in the case of the Screen Actor's Guild. The Programmer's Guild is a not-for-profit organization. The Programmer's Guild doesn't engage in collective bargaining on behalf of its members which is a significant difference from a union. I wouldn't call a Guild better than a Union but more appropriate to the character of programmers at this point in the development of our profession and at this stage in the PGs development.

DR: When you say a guild is "more appropriate" at this point, do you mean because U.S.-based programmers as a group do not perceive themselves as needing a "union"? That appears to be the general sentiment in many categories of white collar workers. Do you anticipate a day when white collar workers start to see unions as relevant for them?

PH: I think it's because we're white collar. However, other white collar workers, such as teachers, have unionized. I think the programming profession is in worse shape now than teachers were when many of them unionized. Programmers are reportedly flocking to techsunite.org to view what it says but they aren't becoming union members in droves. I'm not sure why programmers are more hesitant to unionize than other white collar professionals. I've speculated that the lack of unionization has to do with the type of person who is initially attracted to programming and then decides to stay a couple of years instead of moving quickly into management if they have the opportunity.

DR: Why do software developers need an organization like the Programmer's Guild?

PH: Currently the programming profession in the United States is in deep trouble because of 1) Importation of labor meant to drive down the wage; 2) Legislative efforts to govern how much programmers can make and where they can and can't work; 3) Unfair performance improvement practices by H.R. departments in large corporations that discredit competent programmers; 4) Lack of standards and certification which leads to poor product and tarnishes our profession; and 5) Powerful anti-American-worker lobbying efforts by India's lobbying organizations and groups like the ITAA.

DR: Can you elaborate on these human resources practices that discredit competent programmers? What are these practices, and what motivation do corporations have for using them?

PH: I think the "permanent" employee concept is a thing of the past, yet corporations pretend that concept still exists in I.T. To the best of my ability to determine the average time in service for a person in I.T. is two years or less, yet when I started at IBM in 1979 anyone who had any status as a programmer had a minimum of ten years with the company. Despite the shift in the meaning of "permanent," the mechanisms for treating employees like they have a long term commitment to a company and visa versa, are still in place in terms of performance improvement plans (PIPs) and evaluation processes that happen only once a year, or perhaps at a six month interval if you are new.

The PIP in today’s world is nothing more than an H.R. contrivance to allow corporations to terminate employees with cause and have signed documents on file that state the employee admits to incompetence. This entire thing started with a the downsizing craze of the 80's and 90's. The downsizing craze can be traced back to business philosophies and trends preached and analyzed in our more prestigious MBA schools here in the United States. The PG has been publishing a series of articles on the downsizing craze, how it got started, and what many of the misconceptions about it are and were.

DR: Does your membership currently include people with a range of specialties other than "programming" in the traditional sense? Is there a mixture of practitioners and research-oriented developers?

PH: We have quite a mix. Recently our under 30 age group started to grow. We have those who've experienced only the technologies indigenous to the "information highway," and those who are still died-in-the-wool mainframe legacy programmers. We have those who were practicing programmers and are now owners of small consulting firms and those who've always been "in the trenches." I spent many years in R&D at IBM and know many in the PG who've spent their careers doing business applications. We have PhD’s and those who have high school degrees but years of programming experience of a practical nature.

DR: How many members does the Programmer’s Guild have?

PH: There are about 1600 subscribers but only a fraction are paying members.

DR: When was the Guild first formed, and what were the circumstances?

PH: The Programmer's Guild was formed in 1999 by John Miano. We were concerned about the declining prestige of the programming profession. A programmer is becoming regarded a interchangeable body rather than a skilled individual. We were concerned about the public's perception of the software industry and the rampant hucksterism going on, from Y2K to Internet IPOs. We were concerned about the declining quality of software, both commercial and custom.

We were concerned about the lack of minority and older workers in the profession.

We were concerned about legislative issues, such as tax laws, non-compete clauses, software patents, and immigration, while the programming profession has no voice in government.

We were concerned with improving productivity among programmers.

We were concerned with the difficulty in connecting programmers to jobs.

We were concerned that the growth in technology jobs is not being used to benefit the population at large.

DR: I too have been concerned by the market's recent efforts to transform the programmer into a commodity. However, I wonder to what degree this vision of a rationalized software development fueled by cheap, largely interchangeable labor is wishful thinking on the part of business executives and strategists. Just because someone declares that one programmer is like any other does not make it true. I wonder if we'll be looking back on this in five years laughing at how silly all these companies were to think they could, by sheer force of will, create a cheap, predictable, and orderly software development process while still producing high quality software that meets the requirements, scales up, and maintains well? Or do all craftsmen, pre-commoditization, entertain this kind of illusion? "They just *think* they can turn us into commodities!"

PH: I think it's an illusion that we can be turned into commodities. However, an entire philosophy and paradigm has sprung up for producing software and certifications for those who preach and adhere to the "manage the process and not the programmer" mentality. Those who adhere to the commoditization strategy are often in positions of authority as soon as they take their first job after getting a degree or certification. We programmers, on the other hand, don't have the same fraternal instincts as the management ranks who want to de-skill our profession and send the work elsewhere.

The question is how affordable is the "manage the risk of less skilled labor" approach to things. More than even the commoditization issue, I think one has to question if there hasn't been a real shift in "corporate culture" in general in the United States that makes unethical behavior more acceptable and puts those of us who are less organized as professionals increasingly at risk of being devoured by the more predatory people in the corporate world.

DR: Can you elaborate on what you meant when you said that the population at large is not benefiting from the growth in technology jobs?

PH: That statement has a couple of meanings for me. First, it means that our profession is narrowly defined so that industries related to programming aren't recognized and taken advantage of by other workers so that programmers are seen as a boon to society.

Second, I don't think that products produced by programmers often empower the lives of the average person and improve their lives. There are management courses on I.T. that demonstrate that intranets empower workers and give them and edge, through knowledge, that makes them more competitive and productive than non users. I haven't seen much that's clearly demonstrated the value-add for the average citizen of using software or at least I haven't seen programmers given credit for producing the products that improve lives and uplift people. We are mostly perceived as purveyors of automation that gets rid of jobs. Also, there aren't any programs in place to "share the wealth" of the technology boom with others who could use a lift in terms of earning power.

DR: What are the primary issues that the Guild focuses on? Does the Guild have an "official position" on these issues? Is it difficult to reach consensus on these issues within the membership, or are your members mostly unified in opinion?

PH: We focus on:

    Promote the profession of programming
    Conduct lobbying on issues that affect members of the programming profession
    Set Professional Standards
    Certification
    Job Placement

Some of the items we focus on are still in the formative stages and are done on a best-effort basis.

DR: What inspires you personally to give your free time to working on behalf of the Guild?

PH: When I was laid off by IBM in a purge of tens of thousands of workers in 1994 my family and I felt alone. The PG helped me to understand I'm not alone and allows me to reach out to other individuals and families who've gone through the same experiences my family and I have.

DR: There has been of late a lot of words published in newspapers, magazines, and blogs about the controversy of H-1b/L-1 visas and the "offshoring" trend. For example, Salon magazine recently ran two articles (here and here) about the offshoring controversy. I was fascinated by the range of letters from readers that Salon published a few days later (and they received a lot of letters).

Frankly I have not yet figured out my own position on this. I have worked with dozens of people who came here through the H-1b program. I have worked with many who are quite skilled, in a few cases well beyond my own skills and knowledge. My own great grandparents emigrated here from another country, so who am I to begrudge anyone else that right? On the other hand, I see the negative sides of too many H-1bs and too much offshoring.

I've seen many of the people I've worked with exploited by the agencies that bring them here, all within plain sight of the corporations who hire them. I have American programmer friends with great credentials who have been out of work for months at time. I worry about the commoditization of the programmer, which offshoring helps fuel. I worry about the what the hopes are for a recovery of the US economy if tens of thousands of jobs are being sent to other countries.

My underlying point, Paul, is that this seems to me to be a complicated issue? Do you agree? Is there some middle ground, or are these trends inherently negative?

PH: I think the intent of the visa programs were good, but the numbers were inflated by powerful lobbying groups who 1) wanted to drive down the wage, or 2) found a low wage here better than what was available at home. I don't blame any individual who comes here to improve their lives. Some of my great grandparents came here from Ireland and competed for jobs in the U.S. I think America is more saturated than it was 100 years ago. We're not a frontier anymore. I don't think my great grandparents should have been taken advantage of when they came here and neither should the H-1b's. There is always an opportunity for exploitation of immigrants. As long as there are those exploiting a situation like H-1b, etc. there will be those who oppose it.

I think in the current situation our representatives aren't listening to us despite the extenuating circumstances of 9/11 and other indications that we've been too liberal in recent decades with our immigration policies.

DR: So the main problem with the H-1b program is that the caps are too high? Is there an appropriate cap that the Guild advocates?

PH: I think the program should be abolished.

DR: Do people here on H-1b visas pay income and social security taxes?

PH: Yes.

DR: What, if anything, can or should be done legislatively at the federal or state level to stem the tide of programming and other jobs going offshore?

PH: Locally states should refuse to have H-1b's work on government contracts. That goes back to the issue of "benefiting the population at large." Instead of an H-1b program we should be training people here who need a career boost to step into technology jobs.

DR: Is there any clear division along Democratic/Republican party lines on the legislative issues?

PH: There's a lot of grousing about Democrat vs. Republican, but I've stopped trying to figure out the partisan politics of this situation. A big thing yesterday was Bush reportedly saying tech jobs are leaving because US worker's have out-dated skills. Well, then lets benefit the population at large by training Americans first in the technology arena.

DR: Who is behind the lobbying groups that advocate for upping the H-1b cap?

PH: First, management looking for cheap labor and adhering to the belief that improving the programming profession is a waste of time when programmers can be de-skilled and risk-managed. One organization is the ITAA. Often lobbyists are from foreign nations which, by the way, I believe is illegal though I'm not sure why I think this.

DR: There appears to be an interesting intersection between the H-1b controversy and the offshoring controversy. Apparently Microsoft, GE, and other large companies are setting up development centers in India and elsewhere, which allows them to hire their own developers in those countries that they can bring to the U.S. through the L-1 visa. Can you elaborate on this phenomenon? Is it happening a lot? What exactly is an L-1 visa? How many L-1 visas are used in this way compared to H-1b visas?

PH: The PG's V.P. or Research and some interested parties had provided us with figures on L-1. I'm not concerned with the figures so much as the overall picture with large corporations and trends in their behavior. To me L-1 and the offshore development centers are one more way to avoid the intent of the law. I'm not going to take the time right now to look back at the figures I've been quoted. L-1 simply allows transfer within a corporation. So a person hired in a foreign R&D facility doesn't have to go the H-1b route to make their way to the U.S., and they aren't counted against the H-1B cap.

I oppose L-1 simply as a matter of principle. The H-1b program is a sham, and from our perspective L-1 should be counted against H-1b caps and isn't. H-1b was founded on bad numbers and lies brought forth by lobbyists and L-1 is an extension of the same program based on the explanation of why L-1 should be allowed. The premise for both programs is that there's a lack of skilled and/or qualified American workers, which is grossly inaccurate.

DR: Does the Guild have a position on the issue of licensing? Some people think it is inevitable that some or all states in the US will someday require programmers, or at least certain programmers, to be licensed.

PH: We're split on this issue right now. Texas has licensing, but there's a kind of hole in the Texas licensing. For the Texas licensing you must also have a Bachelor's degree and 4 years of experience. And you must be a resident of Texas. John Miano's assessment is that the Texas licensing would not stand up to a court challenge. I think licensing like that of civil engineers would be a good thing, but others in the PG leadership disagree. Perhaps we'll come up with a position paper sometime in the future.

DR: Does the Guild have a position on drug testing?

PH: We oppose drug testing by employers not so much because of a civil rights issue as much as because it's part of a larger trend toward invading the privacy of employees. Take a quick look at The Naked Employee by Frederick S. Lane. Web surfing? Workers are being watched. E-mail? That, too. From video cameras to ID cards to background checks, employees' lives are basically open books to whomever is paying their salary. Lane's style is more clinical than impassioned, laying out the hard facts instead of editorializing.

DR: What kind of work specifically does the Guild do to advocate for issues? What strategies do you employ?

PH: We do a lot of fax and e-mail campaigns. When we find out employees are about to be "right sized" in favor of offshoring or lower paid foreign labor we inform the workers as soon as possible so that they can make informed decisions about their own futures. We challenge in court blatant advertising that looks for foreign workers to the exclusion of American workers. We on occasion defend members who are submitted to grossly biased PIP processes that have taken the place of an old fashioned lay off and admission that poor business conditions make the layoff necessary.

DR: Are there other organizations with which the Guild collaborates or shares solidarity?

PH: Yes, we're part of a larger loose coalition of organizations, some of which are other professional societies, some concerned with American workers in general and some purely concerned with immigration issues.

DR: Could you name some of these organizations?

PH: The associations are pretty loose-knit. We don't have any legally binding agreements with anyone or even letters of intention to cooperate. We cooperate on activism items that seem to be of mutual benefit and don't always agree on over all policy, strategy or tactics. In the last year we have drawn closer to the CWA and AFL/CIO. We've taken a real interest in The Screen Actors Guild, but our V.P. of Coalition Building hasn't contacted them yet to the best of my knowledge. We've partnered with The American Worker's Coalition on occasion, and with The Organization for the Rights of American Workers. We have in our membership those who also belong the WashTech, the IEEE and the ACM.

DR: If someone wants to get involved with or assist your organization, what should they do?

PH: Go to our web page (www.programmersguildusa.com) and join. We need the most assistance in forming local chapters and getting local membership to visit their representatives offices in their districts, and to show support for us.

DR: Thanks for your time, Paul.

Part 1 of this essay discussed the concept of software development specialties, including how to choose a specialty, how to evaluate the risks and benefits of a given specialty, and how to be aware of the market forces and trends that apply to technologies and specialties. This second part will concentrate more on the "strategies" part of the title of this two-part series.
Advancing Your Knowledge

The subject of monitoring trends (which was discussed in the latter section of Part 1) is directly linked to the subject of advancing one’s knowledge within a given specialty—and within the software engineering discipline as a whole. A truly rich and fruitful specialty will take years to master, and even better, a large base of adopters will lead to continuous innovation, meaning that there is still continuous learning opportunity once mastery is reached—if mastery can ever truly be reached, that is.

How much of a master one becomes is a matter of personal choice. A friend of mine named Trevor, who is also a software engineer, is a student of Shaolin Kung Fu, an ancient Chinese martial arts system. I asked him about the pathway to mastery in this system. (For those not familiar with kung fu, a student progresses through a series of "belts," with each belt having a different color—white, yellow, green, brown, black, etc. The "belt" is literally just that: a canvas belt that is tied around the waist of the kung fu uniform. Some of the higher belts, like brown and black, have multiple "degrees," which are sub classifications of progress.)

My friend tells me that, with hard work, attendance at classes several days per week, and discipline outside of class as well, a dedicated student can reach 1st degree black belt in two to three years. This includes mastery of various fighting styles, weapons, and also Tai Chi, which is a more internally focused system of movements and breathing. From the average person’s point of view, becoming a black belt in any martial art is a major achievement, and if you took a survey, I’ll be most people would equate a kung fu black belt with kung fu mastery. But is it mastery?

In the "Shaolin Do" system in which my friend Trevor is studying, there are 10 degrees of black belts. Here’s what Trevor says about the journey of mastery beyond that initial 1st degree black belt: "My instructor, Master Grooms, has been practicing for 25+ years and is a 6th degree black belt, but I think he achieved that more quickly than most, given he practices Martial Arts full time—and there are 4 more degrees beyond where he is. The current Grand Master is 10 years older than Master Grooms and has been practicing since he was a child, so you figure 50 years to get to 10th degree if the stars align just right and your focus is unwavering. There is exactly ONE 10th degree Shaolin Grand Master in the world—that's it."

I don’t know about you, but to me, that’s pretty intense. Three years to reach the first "level" of mastery, and then a lifetime to refine that mastery further and further. It certainly offers a different perspective on the idea of "mastery." This kung fu analogy, however, probably does not hold up when comparing it to any one of the fairly narrow software development specialties we’re discussing here. Since these kung fu masters are mastering a whole range of systems and techniques, it would probably be better to use the kung fu mastery analogy to compare with the discipline of software engineering as a whole, which encompasses a whole range of specialties and sub-specialties.

Regardless, my point is this: once we have a basic competency in software development (and I’ll bet a kung fu Grand Master would consider the esteemed 1st degree black belt as merely "basic competency"—it’s all relative), if we want to attain "mastery," and the higher pay, visibility, and satisfaction that comes with it, then we cannot stop our knowledge seeking. We must continually dig deeper into the discipline, yes, but we must also study outside of the discipline into non-technology-related domains, in order to introduce fresh ideas and more profound understanding. Personally, I feel like my journey is still just beginning.

Before moving on, I’d like to touch further on the concept of non-technical knowledge and growth. Gerald Weinberg is the one who really turned me on to this concept. Through his books Understanding the Professional Programmer and Becoming a Technical Leader, and through his Problem Solving Leadership workshop, which I was fortunate enough to attend, I have learned that the technical challenges we face as developers are the least of our concerns.

At a certain point, the technical stuff becomes easy—you start to see the same patterns and principles over and over again. That’s a bit of an overstatement, since if there were not continual technical challenges we would likely get bored, but it’s all the "human factors" stuff that’s really hard—and what’s hardest is improving the one human each of us has to deal with every day: ourselves.
Protecting Your Specialty

If you are good at what you do, then you are constantly going to be beset by people trying to get you to do other things that you don’t necessarily want to do. Most of the time, you can take this as a compliment, because it means that someone has recognized your talents and/or work ethic. However, knowing when and how to say "No" is important if you want to control your own career destiny.

Determining when to say "No" when someone asks you to do something can be tricky. One key factor is the duration of the task. If someone is asking for your help on a relatively minor task that you can knock out pretty quickly, then you might want to accept the task. That’s just part of being a team player: if something needs doing, then it does not reflect well upon you to say "No" because you feel you are above that sort of task. Let’s look at an example.

Mary is a software engineer in a large company that employs a high number of technology professionals. Mary’s specialty is writing object oriented C++ programs, and she would like to continue doing that for the foreseeable future. Mary’s code for the latest release of the software is pretty much done except for a few debugging tasks that are arising out of the QA process. The team is committed to pushing this release to production in a week. Mary’s boss comes to her and says, "Mary, I’m in a bind. Harry is on leave for the next month, and as you know, Harry is in charge of writing and generating the installation programs for the system. You are done with your code, and I need your help getting the installation program done and tested in time for the release. You’re the only one whose schedule is not already full for the next week, and I know you’re more than capable of doing a great job."

What should Mary do? In my opinion, she should say "Yes" to this task, since it is a one time thing, it should take a relatively short time to complete, and her team needs her to come through in a pinch. If Mary says "No," then her boss would be right to be disappointed, and, fair or not, might even feel that Mary is being selfish. What if the situation were a little bit different, however?

What if Mary’s boss comes to her and instead says, "Mary, I’m in a bind. Frank, our report developer, just quit the company. Your coding on this system is done, and as you know this is pretty much the final release of the system for awhile. Frank was scheduled to design and build forty reports over the next three months. I need someone to fill in for Frank since I don’t have time to bring on a new report developer, and the company has a hiring freeze right now anyway. I really need you to take Frank’s place for the next few months and develop these reports. Will you do it?"

Here the correct course is a little more gray. We are no longer talking about a simple matter of Mary taking on a quick task, and then getting back to work on her main specialty, C++ coding. Let’s assume that Mary recognizes that she does not really want to develop forty reports over the next few months. But can she say "No"? That depends a lot on Mary: how she feels about protecting her specialty as a C++ developer, how she feels about her future with this company, and how she feels about the demand for her skills on the open market.

Maybe she has been wanting to scale back her responsibilities and take it easy for awhile, and feels that this report building gig would be okay for a few months. Maybe Mary knows that if she says "No" to her boss that there is a high likelihood she could be laid off, since the company is going through a rough period, and there is not a ton of C++ work waiting to be done. Maybe she really wants to stay with the company long term, and the sacrifice of doing reports for a few months is a small one compared to her career with the company. Maybe she knows she has an inflated salary relative to what the market is paying right now for C++ developers and is not willing to risk losing that. These are all valid reasons that Mary might say "Yes" to her boss’s request. I certainly would not judge her for saying "Yes," since these kinds of decisions are very personal, and there really is no right or wrong answer.

Mary, however, might have different ideas about her specialty as a C++ developer. If she is confident in her skills and confident in her ability to find an equivalent or better new job if it came to that, then she might decide to politely decline her boss’s request. The danger that Mary is wary of (and rightly so) is that she might get stuck as a report developer indefinitely, which might allow her C++ skills to get rusty, might keep her from working on new skills and technologies within her C++ specialty, and might lead to her company’s forgetting entirely that her specialty is C++ and not report building. What happens if her boss is promoted, transferred, or fired two weeks after she agrees to take the reporting gig? The new boss will see her not as a C++ developer but as a report developer. Also, if she gets stuck writing reports for several months, then she will be forced to reflect that on her resume, which will look strange to potential future employers. She should also consider whether there is a hidden message in her boss’s request—could her employers be telling her something about their feelings about her C++ coding skills?
Making a Change

You may have already recognized that this situation of Mary’s might be a very positive one if she was seeking to change specialties to report development. If that were the case, she would of course jump on the opportunity to take a report developer’s position. When you are seeking to change specialties, for whatever reason, then these are exactly the kinds of opportunities you want to look for, and if possible create.

There are various ways to create opportunities like these. My favorite method is to just work on the side doing what it is you want to be doing, and then present it to your employer after it’s already done. Use an unfamiliar technology or technique to solve a problem that your employer did not even know she had. Assuming that you’ve solved this problem on your own time, and without neglecting your assigned work, a good employer will appreciate your initiative and recognize your abilities to use the new technology or technique. You can explicitly come out and tell your employer, "This new thing is what I really would like to be doing, and I was hoping that there would be opportunities here for me to do it." You have to be in the right kind of situation with the right kind of employer for that strategy to work, however. It might be that in order to change specialties, you will have to change employers. Even in that case, though, I don’t think it would be disingenuous to put that new technology or technique on your resume.
The Pull Upward

Many developers, especially good ones, will eventually feel the pull toward management. This is the ultimate specialty protection dilemma, because when a developer is promoted to management, coding opportunities tend to cease to exist. I myself have had to resist this pull many times, and in my current position, which is a quasi-management position, I decided to let the management pull win a little. It’s been about a year since I’ve had an opportunity to write any real code. Taking this position was a conscious decision, and at the time that I made it, I knew it was only going to be for a limited time. As I write this, some major changes out of my control have occurred at my current employer, and I am seeking a new position, where I will hopefully be able to get back into the trenches building software again. However, I don’t regret my recent decision to take a management position for awhile. I learned a lot about myself and the process of leading a team to build great software. Watching other people write code and work under deadlines has taught me more about myself as a developer.

Getting promoted from the trenches to the command tent can be a little scary and a little sad—scary because you are suddenly on unfamiliar territory, dealing more with people than with technologies, and sad because you are leaving behind an activity you love. People on your team that you formerly worked side-by-side with will view you a little differently, and won’t share their thoughts and feelings with you so readily as they used to. Politics and meetings will suddenly become a big part of your life. You will often feel caught in the middle between your obligations to the people working for you and your obligations to the company and your bosses. That said, management can be a rewarding job.

If you do not wish to make a move to management at this point in your career, be prepared to resist the pull and say "No."
Cultivating the Right Kinds of Generalization

Given the diversity of programming languages, platforms, techniques, etc. it is inevitable that a software developer will master a limited subset of the possible software engineering specialties. There is just too much diversity for anyone to be an expert in everything. However, too much specialization is a bad thing. We must also be generalists. A software developer who is too focused on the mechanics of writing code or making a certain technology work will find himself limited career-wise—not to mention, without certain generalist skills, you simply will not be able to develop software that is acceptable to very many people besides yourself.

This discussion is not the right place to delve into the details of proper software engineering generalization, but below is a list of knowledge areas and skills we would all do well to learn something about:

    Requirements gathering
    Design (this will take many different forms, depending on the technology(ies) in use)
    Architecture
    Quality assurance and testing
    Deployment and implementation
    Change control and configuration management
    Algorithms
    Documentation
    Usability engineering
    Transaction control
    Methodologies and process
    Metrics
    Leadership
    Diplomacy
    Conflict resolution and management
    Code and artifact reviewing
    Risk management
    Marketing

As previously mentioned, there is also a lot to be gained from focusing on learning outside the field of software engineering. I would say that just about any discipline or subject has something to teach that will be useful to any of us as software developers. Here’s a short list of a few subject areas that are worth attention (not that I can pretend to have studied these areas extensively myself—all of this advice is directed as much to myself as to anyone else reading these words):

    Sociology
    Anthropology
    Psychology
    Mathematics
    History
    Philosophy
    Religion
    Engineering
    Medicine

I have also gained insight from reading the early influential works of the software field. It’s amazing to see how software developers thirty years ago were dealing with the same issues we deal with today—the acronyms are just different. Sadly, much of this material is out of print, but seeking out a few books such as Classics in Software Engineering (edited by Edward Yourdon) and Software State-of-the-Art: Selected Papers (edited by Tom DeMarco and Timothy Lister) might be well worth your trouble.
Conclusion

The point of this whole discussion has not been to tell you what to do or how to feel. Career management is a very personal topic, and there are no right or wrong directions or decisions—there is only what is better or worse for you. My intention is merely to point out that conscious thought and strategizing are required in order to acquire, maintain, and build on a software development specialty, and thereby to build a career as a professional software developer. As I said near the beginning of this two part essay, the only one who can manage your career is you.

Career-wise, software developers really have an amazing variety of specialties to choose from. The various specialties can be differentiated in several ways:

    by programming language
    by hardware platform
    by operating system
    by technique and/or process
    by persistence type (that is, relational databases vs. object databases vs. flat file databases etc.)
    by vertical market
    by market sector (government vs. business vs. non-profit)
    internal use vs. shrink-wrap
    client/server vs. distributed n-tier
    fat client vs. thin client
    Open Source vs. proprietary

This list could go on and on. It seems as if there are as many specialties (or rather, combinations of specialties) as there are individual developers. Most developers cultivate not just one specialty, but rather a combination of specialties that cut across all of these differentiating factors. Independent consultants, especially, must diversify their portfolio of specialties in order to protect against the whims of the marketplace—or perhaps just to prevent boredom. The difference between one developer’s combination of specialties and another’s is often simply a matter of emphasis of one factor over the others.

How does one come about one’s specialty or specialties? Once we have chosen a specialty (or after our specialty has chosen us), how do we maintain that specialty, and, conversely, how do we decide whether it might be a good time to abandon our specialty for another one? If we desire to maintain a variety of specialties, which one should be emphasized, if any? Which specialty is more marketable than others? What is the "right" niche for me?

It pays to give attention to these kinds of questions. Conscious thought and strategizing are required in order to acquire, maintain, and build on a software development specialty (or combination thereof). The only one who can manage your career is you. My intention here is not to tell you the answers to these questions, since I could not if I wanted to try. I don’t even have all the answers for my own career. As I navigate my career and the landmine-filled marketplace, I struggle constantly with finding the right questions to ask and providing myself with the best answers. Hopefully, though, in the pages that follow I can shine some light on the dynamics at work in this crazy software development universe which we are lucky enough to call home.
Choosing the Right Specialty

Which is the "right" specialty? The answer to that question will, of course, be different for each person. Furthermore, there are most likely several "right" specialties. Any given person might thrive and be happy in any one of them—or even several of them over the course of a career. With so many potentially lucrative and enjoyable specialties, and potentially several decades in a career, there is no reason a person must limit herself to just one specialty. That said, I think the "right" specialty is one that

    makes you happy most of the time,
    offers you the amount and kind of security you desire,
    offers you the level and type of challenges you desire,
    and has the potential for the amount of money you require or desire (assuming money is important to you in your software development career).

These are minimal and general criteria, but there are other more specific concerns as well:

    Does the specialty have longevity? Where in its life cycle is the specialty? (see below)
    Does the specialty appeal to your sensibilities? For example, your sensibilities might lean towards using a low level language in a command line environment, while someone else might strongly prefer a higher level language in a graphical environment. One person might want to work on relatively low risk business applications, while another might thrive on the pressure of developing life-critical systems.
    Is the specialty in line with your abilities? That is, do you have a) the right kind of training and knowledge, and b) the required intelligence and/or temperament?, and/or c) the drive and dedication to acquire the right training and knowledge, whether or not you possess the required intelligence and temperament.
    Given that you have the intelligence and temperament, and that you have, or can obtain, the required training and knowledge, is the specialty otherwise within your reach? How high is the barrier to entry? That is, who do you have to know to get a job working in a particular specialty? How many of those jobs are available? What parts of the world are those jobs located in? What kind of background is required? What kind of credentials or certifications are required? What kind of government security clearances are required?
    Is the specialty too narrow for you, too wide, or just right? (see below)
    Is the specialty in line with your life principles and ethics? Particularly, are you willing to work for the companies or government agencies that will pay for the specialty?
    Is the specialty undergoing rapid evolution, or are techniques, body of knowledge, and technologies relatively stable? If the specialty is undergoing rapid evolution, are you willing to put in the extra time and energy to keep up?
    Is the specialty going to suit your ego? That is, how is the specialty regarded in the industry at large, and do you care what other people might think about the specialty? Do you care that your specialty might be ignored by technology journalists, book publishers, academics, and/or gurus? Or do you require that your specialty be a popular one?

All of these seemingly objective criteria imply that a person has the opportunity to sit down, perform an evaluation based on these criteria, and then just choose a specialty. It may be this simple for people in particular circumstances. For instance, a student who is near the beginning of her academic career can ask these questions and make some decisions about which direction to take. A person already in the non-technical working world who wants to try to break into the software development field also has the opportunity to pick what kinds of technologies, tools, and domain knowledge to acquire. Similarly, a working software developer seeking to switch to a new specialty can exercise some amount of discretion in choosing his next specialty.

However, for many people, including myself, our specialties largely choose us. At the time of this writing, my specialty is developing business software, centered around databases and data exchange, using languages and tools that operate at a fairly high level of abstraction. At the risk of dating this essay for future readers, I specialize in distributed, client/server, and component-based software for the Microsoft Windows platform. I work for the business sector, developing in-house, shrink wrap, turnkey, and/or web-based software. I am skilled in relational database design, object oriented design, and user interface design.

What’s interesting is that, while I have cultivated and refined these specialties over several years, they are 100% rooted in the very first job I was lucky enough to get in the software field. That job was with a company which developed database-centric accounting software, using high level languages on the Microsoft DOS and Windows platforms. The evolution from there to where I am now has been fairly logical. If my first job, which I landed at a time when I had close to zero experience, had been with a company that developed device drivers for the UNIX platform, my career might have been much different. So, while we can put a lot of thought into choosing a specialty, and we can strategize to navigate our careers in one direction or another, often the directions we go are determined by the opportunities that come our way.
Assessing the Risks of Having Too Narrow a Specialty

The judgment of whether one’s specialty is "too narrow" will differ from person to person, specialty to specialty. Your career strategy, temperament, economic goals, and ego will all play a part in making this judgment. That said, I think there are several categories of factors to consider. Examine these trade-offs closely when considering a very narrow specialty:

Economic and market factors For people who wish to make their living as software developers, market forces are probably the most important factors to consider in the wide vs. narrow question. Interestingly, a narrow specialization can work for and/or against you in different situations. For example, if you are an expert in a certain programming language that is used by a relatively small number of companies, then there are less customers out there to whom you can sell your services. So that works against you because it makes it harder for you to find work, and also because it increases the likelihood you will have to travel or move around to get work. However, scarcity can also work in your favor, because just as there are few companies who use the language, there are also correspondingly few developers who are experts in it, meaning that you can potentially charge a high fee for your services.

Availability of information and instruction If your specialty is narrow, then there will be fewer magazines, books, and web sites dedicated to your specialty. General technical periodicals and web sites will tend to give less, if any, coverage to your specialty. There will be fewer mailing lists, discussion forums, and user groups. There will be fewer training classes and conferences. These kinds of resources are invaluable to a developer, especially when one is just starting out, or in an intense learning phase.

Speed of innovation With fewer people and fewer companies working with in given technology or area, innovation will tend to be slower. This happens because fewer people means fewer new ideas thrown into the mix, and fewer companies means market competition is less of a factor in motivating companies to create new products. Speed of innovation is also hampered by the fact that information is flowing less quickly and less widely because of the aforementioned dearth of publications, conferences, etc.

Availability and variety of tools If the market is small for a given specialty, then fewer companies will exist who are developing tools to make it easier to work in that specialty. Similarly, fewer motivated developers might exist to develop shareware, freeware, and Open Source tools and solutions.

Risk of obsolescence If a market is small, then it might tend to grow smaller, (see the section titled "Monitoring the Trends" below). If the market grows smaller, then tools vendors might go out of business. Tools vendors might also get bought up by larger companies, who might not maintain the same level of stewardship that the original company did. Also, if a specialty’s survival depends on a particular technology, such as an operating system or hardware platform, then the obsolescence of that technology can lead to obsolescence of the specialty. This kind of dependency is definitely an important risk to consider.

Ego factors Many of us might not like to admit it, but our egos and personalities play a big part in our career decisions. If a particular specialty is especially popular, widespread, and hyped up, then that fact might appeal to one person, but repel another person. Some very narrow specialties might be looked down upon by many developers and publications, if they notice the specialty at all—this situation might really bother some people, who would want to work in a specialty that is well thought of and respected, even if it’s not popular. Also, what world do you look to for validation and guidance? Do you gravitate towards academia or the commercial sector? These groups might tend to favor certain specialties and not others. If the acceptance and approval of people in these camps is important to you, then that’s something to consider.

Here is an example from my own career: as mentioned, I specialize right now in developing for the Microsoft Windows platforms. While they are of course not perfect, I personally very much like the tools and technologies that Microsoft produces. I especially find it a pleasure to use their development tools. Also, Microsoft’s platforms and tools are very well suited for the kind of high level, business-oriented, database-oriented development that I do. If I were doing a different kind of development, I might find that their software was not well suited for what I was doing.

At the time of this writing, the vast majority of the people who are writing what I consider to be the important works on design, methodology, architecture, construction techniques, etc. look down on Microsoft tools and technologies with total disdain. (In my opinion, this disdain is rooted in hatred for Microsoft’s business practices and current monopoly status rather than in objective evaluation of the technologies themselves, but that’s a whole different subject.) My point is, right now, the Java programming language and Open Source software are hot, hot, hot, and it is not exactly considered cool in the world of software magazines, web sites, and books to be a Microsoft-oriented developer (even though there are millions of developers writing to the Microsoft platforms). Many people feel so strongly about this, they probably stopped reading this a couple pages ago simply because I revealed my Microsoft-oriented specialization.

This is where ego comes in. Does this unpopular status affect how I feel about my work? Does it affect my future career plans and strategies? Simply by switching technology platforms, I could gain the acceptance of what I perceive to be the current mainstream in software engineering thought. While I will admit to giving thought to the issue and being slightly bothered by this prevailing bias, I have decided to ignore it and proceed on my chosen path. This path makes me happy, I make a good living at it, and it has a viable commercial future—that’s enough for me.

(I look forward to re-reading this a couple decades from now and laughing about how silly and dated the above three paragraphs will sound.)

Ethics and principles With certain specialties, you might want to ask yourself some tough questions about your own ethics, morals, and principles. For example, a certain programming language might be used primarily by parts of government or industry that you might not want to work for. If serving non-profit, religious, or charitable organizations is your goal, you might want to specialize in technologies that these organizations can afford, and which will be relatively stable for a relatively long time. You might not want to use a particular company’s technologies because you disagree with that company’s business practices (see above). You might not want to use any proprietary or commercial technology at all because you have strong feelings about the effect of capitalism on technology. No one can decide these kinds of questions but you.

Another wrinkle in this is to be honest with yourself about how much you are willing to set aside certain principles in the interest of making money.
Aiming Consciously at the Right Level For You

Let’s face it: not everyone is as smart as the smartest among us. Likewise, even among people who are equally "smart," people have different talents. For example, some people just have an amazing natural ability with numbers, complex calculations, and "higher math." Other people might have less natural ability with math, but are gifted in the use of language and writing. Other people might have a gift for the graphic arts. It is not my intention here to open the nature vs. nurture debate regarding how people get the way they are, but rather to acknowledge that people have differences in these kinds of areas, and that this is normal.

Furthermore, my point here is not to make anyone feel bad about talents they may feel they lack, but rather to say that it would be wise to aim for a specialty that complements your talents. For example, there is no doubt in my mind that the world of software development is full of people a lot smarter than me, and that many of these people can work miracles with numbers and algorithms and really hardcore stuff that I have no idea about. When I look at any of Donald Knuth’s The Art of Computer Programming books, my head starts to spin. Simply put, I am not a "math person."

Given that I am not a math person, I did not choose a specialty that requires me to be one. I leave those specialties to all those people who seem to have been born for them. I chose a specialty that allows me to work at a fairly high level of abstraction, and to use talents that I do have, such as analytical thinking, logic, abstract thinking, writing, organization, and people skills. In addition to thinking about what you like to do, think also about what you have a natural talent for, and look for ways to apply that.

So that I don’t leave you with the wrong impression, I want to make two things clear: First, if you feel you don’t have any natural talents, you are mistaken. You just are not aware of them yet. Second, if you are passionate about an area that you feel you might not have a natural talent for, then absolutely do not let any self-limiting ideas about what talents you may or may not have stop you. Let me quote Theodore Sturgeon:

    Anybody can do anything he wants to if he wants to do it badly enough. Now I know that's a vast oversimplification, but in principle, it's true. For example, I think that you or I or anyone could be a wire-walker for Barnum and Bailey, and all it would take would be an absolute determination and practice, practice, practice. You have to study your field and you have to find out how other people do it, and you have to keep working and learning and practicing and ultimately, you would be able to do it.

    However, there are people who are able to do it in the first three tries. They're born with such coordination and talent, a construct of the semi-circular canals in the inner ear, that they're able to do a thing like that with great ease. Now skill is what you develop by that kind of practice and work and study. Skills can be developed and refined and brought to a very, very high pitch indeed.

    Talents you are born with. There is simply no question about it. People with a high talent unfortunately don't have to continually practice. They don't have to, so they don't do it. It is a great loss because a lot of highly talented people never work at their talents at all. In the end, their product is lesser than somebody who has worked his buns off to get somewhere and refine the talent that he has. (1)

I have used this quote in an essay before, but it is powerful enough that I decided it was worth repeating.
Monitoring the Trends

All technologies and specialties are subject to trends. Any one technology or specialty is likely to follow a bell curve from obscurity to popularity and back to obscurity again. Here’s an illustration of what I mean:

Here in Figure 1 you can see a technology over time gradually growing in popularity, peaking, and then dwindling towards obsolescence. (Please note that none of the diagrams I will be using in this discussion were scientifically or statistically generated. I conceived them for illustrative purposes only.) The curve in Figure 1 shows a relatively equal and gradual amount of time spent rising and falling in adoption and mindshare.

So what’s the point of all these graphs? The point is that, from an industry-wide point of view, your specialty, no matter what it is, is subject to these kinds of trends. Will your favorite technology look exactly like one of these curves? Probably not, but you want to stop and consider a few things about your favorite technology or specialty: What might its curve look like? How long will the curve stretch out? Months? Years? Decades? Where are we on the curve now? Is my specialty on the rise, or on the decline? If my specialty is on the rise, is it a healthy, gradual rise, as in Figure 1 or 3? Or is it a meteoric rise fueled by hype? If the rise is fueled by hype, what’s going to happen when technology journalists get bored with it and the hype dies off? Can the technology come close to living up to the hype? Asking and trying to answer some of these questions can play an important part in both choosing a specialty, and evaluating when it might be time for you to start moving away from your specialty and finding another one.

What’s interesting about these graphs is not the graphs themselves, but the underlying reasons for the rising and falling. When we examine these reasons, and examine the interrelationships between the graphs of multiple technologies and specialties, we can see how oversimplified these graphs in Figures 1, 2, and 3 are. A contemporary example: remember the period from 1994 to 1997 or so, when web pages were really hot, and everyone had to have one. The problem was that in order to make web pages, especially really slick ones, you had to write them by hand using a combination of markup language and scripting that was arcane to most people. During this period, programmers were making a lot of money as HTML developers.

So there was a meteoric rise in demand for this skill, and a corresponding meteoric rise in what people were willing to pay for it. However, over time tool vendors started coming out with better and better programs for making web pages that required less and less programming skill. Not to mention, the ability to create a web page was added to word processing and spreadsheet programs, database products, report generators, etc. Soon anyone could create a decent looking web page with no programming whatsoever. Eventually, the demand for HTML developers fell off—even though the technology itself (web pages) continued to become more and more popular, and continues to be very popular five years later. So the technology keeps rising, but the specialty (HTML coding) falls off dramatically.

If you were making a living during this period as an HTML developer, you would have hopefully been paying close attention to the market forces at work so that by the time people were no longer willing to pay for your skills as an HTML coder, you had already adopted a new specialty that people were willing to pay for. Even better, perhaps you were clever enough not to make HTML coding your only specialty. Perhaps you would have had multiple specialties: a foundation of specialties that are in periods of sustained and stable high demand, supplemented with more fleeting specialties like HTML coding. While people are paying a lot of money for the fleeting specialties, you can cash in, but when those trains leave the station, you’ll still have your bread-and-butter sustainable specialties. These are the same principles people use when making financial investments: diversify to protect yourself against the up-and-down cycles of the market.

 we see three different categories of companies. Early adopters frequently jump right on new technologies and techniques, but don’t tend to stick with them as long, because there are always other new technologies and techniques to adopt. Middle adopters take a little longer to adopt a new technology, but they stick with it a little longer than the early adopters. Late adopters only pick up on a technology or technique when it is already very well established, but then stick with it for much longer because the cost of changing technologies too frequently is too high. When late adopters make a technology choice, they have to make sure they make the right one, because they don’t want to have to change it too soon. (Note: this is very much a simplified discussion of these kinds of market forces. For a much richer view on these subjects, check out Geoffrey Moore’s popular books Crossing the Chasm: Marketing and Selling High-Tech Products to Mainstream Customers and Inside the Tornado: Marketing Strategies from Silicon Valley’s Cutting Edge.)

What does this mean to you? If you are an individual developer who tends to work for one employer at a time, you need to ask yourself what kinds of companies you want to work for. If you want to always be working with the latest technologies and techniques, you want to work for early adopters—but the price you will pay is that you will have to put in a lot of extra time and energy learning new technologies and making sure you stay ahead of the trends. If you really just want to become good at one stable specialty, and not have to be chasing rabbits down holes all the time, then that’s fine too: just pick a specialty with a long, sustained high adoption rate and go to work for a late adopter company that is committed to that specialty. If you are a consultant or consulting company with multiple clients, then you might do well to diversify, and keep a mixture of early, middle, and late adopter companies in your client list. Finally, notice the link here between you the individual and the companies who might pay for your services: just as companies are early, middle, and late adopters, so too are people. Consider your own temperament and goals, and line those up with companies who have similar temperaments and goals.

This discussion on the variations of trends and trend-based career strategies could go on for many more pages, but I will stop here. The bottom line is this: if you plan to have a multi-year career as a software developer/technologist, then you need to pay some attention to the trends going on around you. You need to develop a personal strategy that will ensure that you can use these trends to your advantage. Even if you have no desire to chase trends and cutting edge technologies, you don’t want to be caught by surprise someday to find that your only bread-and-butter specialty is on the verge of being obsolete.

How important are software design skills to a programmer? Programmers, in the traditional, and perhaps most widespread, view of the software development process, are not themselves seen as designers but rather as people who implement the designs of other people. The job of the programmer, after all, is to write code. Code is viewed as a "construction" activity, and everyone knows you have to complete the design before beginning construction. The real design work is performed by specialized software designers. Designers create the designs and hand them off to programmers, who turn them into code according to the designer’s specifications. In this view, then, the programmer only needs enough design skills to understand the designs given to him. The programmer’s main job is to master the tools of her trade.

This view, of course, only tells one story, since there is great variety among software development projects. Let’s consider a spectrum of software development "realities." At one end of the spectrum we have the situation described above. This hand-off based scenario occurs especially on larger, more complex projects, and especially within organizations that have a longstanding traditional software engineering culture. Specialization of function is a key component on these kinds of projects. Analysts specialize in gathering and analyzing requirements, which are handed off to designers who specialize in producing design specifications, which are handed off to programmers who specialize in producing code.

At the opposite end of the spectrum, best represented by the example of Extreme Programming (XP), there are no designers, just programmers, the programmers are responsible for the design of the system. In this situation, there is no room for specialization. According to Pete McBreen, in his excellent analysis of the Extreme Programming methodology and phenomenon, Questioning Extreme Programming, "The choice that XP makes is to keep as many as possible of the design-related activities concentrated in one role—the programmer." [McBreen, 2003, p. 97] This reality is also well represented in a less formal sense by the millions of one or two person software development shops in which the same people do just about everything—requirements, design, construction, testing, deployment, documentation, training, and support.

Many other realities fall somewhere in between the two poles a) of pure, traditional, segmented software engineering, where highly detailed "complete designs" are handed off to programmers, and b) Extreme Programming and micro-size development teams, where programmers are the stars of the show. In the "middle realities" between these poles there are designers, lead programmers, or "architects" who create a design (in isolation or in collaboration with some or all of the programmers), but the design itself is (intentionally or unintentionally) not a complete design. Furthermore, the documentation of the design will have wide disparities in formality and format from one reality to another. In these situations, either explicitly or implicitly, the programmers have responsibility over some portion of the design, but not all of it. The programmer’s job is to fill in the blanks in the design as she writes the code.

There is one thing that all of the points along this spectrum have in common: even in the "programmers just write the code" software engineering view, all programmers are also software designers. That bears repeating: all programmers are also software designers. Unfortunately, this fact is not often enough recognized or acknowledged, which leads to misconceptions about the nature of software development, the role of the programmer, and the skills that programmers need to have. (Programmers, when was the last time you were tested on, or even asked about, your design skills in a job interview?)

In an article for IEEE Software magazine called "Software Engineering Is Not Enough," James A. Whittaker and Steve Atkin do an excellent job of skewering the idea that code construction is a rote activity. The picture they paint is a vivid one, so I will quote more than a little from the article:

    Imagine that you know nothing about software development. So, to learn about it, you pick up a book with "Software Engineering," or something similar, in the title. Certainly, you might expect that software engineering texts would be about engineering software. Can you imagine drawing the conclusion that writing code is simple—that code is just a translation of a design into a language that the computer can understand? Well, this conclusion might not seem so far-fetched when it has support from an authority:

    The only design decisions made at the coding level address the small implementation details that enable the procedural design to be coded. [Pressman, 1997, p. 346]

    Really? How many times does the design of a nontrivial system translate into a programming language without some trouble? The reason we call them designs in the first place is that they are not programs. The nature of designs is that they abstract many details that must eventually be coded. [Whittaker, 2002, p.108]

The scary part is that the software engineering texts that Whittaker and Atkin so skillfully deride are the standard texts used in college software development courses. Whittaker and Atkin continue with this criticism two pages later:

    Finally, you decide that you simply read the wrong section of the software engineering book, so you try to find the sections that cover coding. A glance at the table of contents, however, shows few other places to look. For example, Software Engineering: A Practitioners Approach, McGraw-Hill’s best-selling software engineering text, does not have a single program listing. Neither does it have a design that is translated into a program. Instead, the book is replete with project management, cost estimation, and design concepts. Software Engineering: Theory and Practice, Prentice Hall’s bestseller, does dedicate 22 pages to coding. However, this is only slightly more than four percent of the book’s 543 pages. [Whittaker, 2002, p. 110]

(I recommend seeking out this article as the passages I have quoted are only a launching point for a terrific discussion of specific issues to consider before, during, and after code construction.)

Given a world where "coding is trivial" seems to be the prevailing viewpoint, it is no wonder that many working software professionals sought a new way of thinking about the relationship between and nature of design and construction. One approach that has arisen as an alternative to the software engineering approach is the craft-based approach, which de-emphasizes complex processes, specialization, and hand-offs.1 Extreme Programming is an example of a craft-centric methodology. There are many others as well.

Extreme Programming, and related techniques such as refactoring and "test first design," arose from the work Smalltalk developers Kent Beck and Ward Cunningham did together. The ideas Beck and Cunningham were working with were part of a burgeoning object oriented movement, in which the Smalltalk language and community played a critical role. According to Pete McBreen in Questioning Extreme Programming, "The idea that the source code is the design was widespread in the Smalltalk community of the 1980s." [McBreen, 2003, p. 100]

Extreme Programming has at its core the idea that the code is the design and that the best way to simultaneously achieve the best design and the highest quality code is to keep the design and coding activities tightly coupled, so much so that the they are performed by the same people—programmers. Refactoring, a key XP concept, codifies a set of methods for incrementally altering, in a controlled manner, the design embodied in code, further leveraging the programmers role as designer. Two other key XP concepts, "test first design" and automated unit testing, are based on the idea that, not only is the code the design, but the design is not complete unless it can be verified through testing. It is, of course, the programmer’s job to verify the design through unit testing.

It is not much of a stretch to conclude that one of the reasons Extreme Programming (and the Agile Methodology movement in general) have become so popular, especially with people who love to write code, is that they recognize (explicitly or implicitly) that programmers have a critical role to play in software design—even when they are not given the responsibility to create or alter the "design." Academics and practitioners who champion the traditional software engineering point of view often lament that the results of their research and publications do not trickle down fast enough to practicing software developers.

Perhaps this is because, as Whittaker and Atkin point out, too much of the software engineering literature neglects the role of the programmer. Turning back to the specific example of Extreme Programming, McBreen is right on the money when he writes, "XP does directly challenge some of the sacred cows of the software engineering community. By elevating the status of the programmer, it is turning nearly 30 years of software engineering orthodoxy upside-down." [McBreen, 2003, p. 159]

I don’t want to give the impression that the Smalltalk community and XP are the only places where ideas about the close relationship between designing and programming have flourished. As one would expect with an association of practitioners, academics, authors, publications, universities, and conferences, such as the software development profession enjoys, a healthy cross-pollination of ideas takes place. For example, in 1992, C++ guru Jack W. Reeves published an influential article called "What is Software Design?" in the magazine C++ Journal. McBreen identifies this article as having been influential on the Extreme Programming community, and one would expect, Kent Beck.2 Reeves offers some key insights relative to the discussion at hand:

    "C++ has become popular because it makes it easier to design software and program at the same time." [Reeves, 1992]
    "After reviewing the software development life cycle as I understood it, I concluded that the only software documentation that actually seems to satisfy the criteria of an engineering design is the source code listings." [Reeves, 1992] (Actually, Gerald Weinberg made this exact point many years earlier in an essay entitled "A Lesson from the University," published in the book Understanding the Professional Programmer.)
    "The overwhelming problem with software development is that everything is part of the design process. Coding is design, testing and debugging are part of design, and what we typically call software design is still part of the design." [Reeves, 1992]
    "Testing is not just concerned with getting the current design correct, it is part of the process of refining the design." [Reeves, 1992]

Another example: in describing his book What Every Programmer Should Know About Object Oriented Design, Meiler Page-Jones said:

    Some programmers don't think they're doing design when they program, but whenever you write code, you're always doing design, either explicitly or implicitly. One of the aims of this book is to make programmers explicitly aware of the design patterns that they're creating in their code. [Page-Jones, ?]

***

What about the role of the programmer on a maintenance project, and the resulting importance of programmer-owned design? Most of the time, I think, when developers talk about software development in the abstract and when authors write about development techniques, there is an implicit assumption that everyone is talking about the development of new software. However, many (most?) programmers are not working on the from-scratch development of brand new systems (even though that’s probably everyone’s preference). Instead, they are working on the maintenance of existing production systems, either in a bug fixing mode or in an extension mode, adding new features or otherwise evolving the software in new directions.3

It is even more important, then, that we recognize that the design skills of the programmer are of even greater import when a project is in maintenance mode. This is because, from my observations, many companies reduce the staff of a development team when a system goes into maintenance mode. Analysts and designers move on to the next assignment, while programmers are left to work on the "completed" system. In fact, when a project shifts to maintenance mode, less expensive (and probably less experienced and less skilled) programmers are often brought in to replace the original programmers. When new features need developing, or when an existing part of the system needs redesigning or performance tuning, it is often the maintenance programmers who do the design and implementation. (Incidentally, the resulting increase in learning opportunities can be one of the definite advantages of working as a maintenance developer—especially for a less experienced developer.)

In his landmark 1988 book Rethinking Systems Analysis and Design, Gerald Weinberg writes:

    Successful designers have learned to avoid the temptation to design everything in one big lump. Instead, they may build one small part at a time, analyze the actual behavior, and then repeat the process for the next part. In this way, design becomes not only evolution-like, but actually evolutionary.

    This process of incremental design takes place at two levels—in the mind of the designer, and in reality. When it takes place in reality, it is design as maintenance, which is the principle mode of design today. The vast majority of design decisions actually put into effect today are created by maintenance programmers, not designers.

    We4 do not mean to imply that this situation is a good thing—much of the ‘design’ being done in maintenance can be equally well viewed as systematic deterioration. [Weinberg, 1988, p.103]

I could go on for ten more pages in directions suggested by this passage, but I will resist and be as brief as possible.5 First, though, an aside: notice the connection to Extreme Programming, which had not been invented when these words were written. Extreme Programming fully embraces Weinberg’s idea of "design as maintenance" and joins it together with Reeves’s assertion that "it is cheaper and simpler to just build the design and test it than to do anything else." [Reeves, 1992] The result is a process that explicitly leverages that which Weinberg (rightly) labels as risky and dangerous. What makes this work for Extreme Programming is that it uses techniques such as test first design, automated testing, short iterations, pair programming, and refactoring to simultaneously mitigate the risk and transform that risk into an advantage.

But back to the topic at hand: if Weinberg is right that "the vast majority of design decisions actually put into effect today are created by maintenance programmers, not designers," then design skills for programmers on maintenance projects (which I believe is most of us, for at least some percentage of our time) are that much more important. The ultimate care, humility, and integrity are required when extending a production software system.

The maintenance programmer/designer must constantly balance competing concerns: the need to preserve consistency within the existing system, both in the code and in the external interface; the desire to use the best and most correct solution; the desire to use the latest techniques; the desire to create the best experience for the user; the absolute requirement to not break or otherwise destabilize the production system; the critical charge to protect the production data; the desire or need to improve the design or code that already exists; and the need to craft a solution that can be deployed into the existing system with the least disruption and downtime possible. If these competing concerns are not managed with care, then Weinberg’s "systematic deterioration" (as well as "systemic" deterioration) is inevitable. Is it not ironic, then, that the job of the maintenance programmer is most often viewed as being beneath the best and brightest developers?

Given everything I’ve said up to this point, I might be giving the impression that I am lobbying against the software engineering approach—with its dependency on specialization, documentation, and hand-offs—in favor of an exclusively programmer-centric, craft-based approach. This is not my position at all. I agree wholeheartedly with Alan Cooper, who said in a 2002 interview,

    These two movements in the software world—engineering and craft—appear to be moving in opposite directions. But I think they’re both right. The problem is that you can’t focus craft methods on engineering problems and vice versa. So the places they break down is misapplying RUP to something that needs three craftsmen working on it, or trying to use individual craft methods to do big projects. Where is it carved in stone that we have to use the same method for all projects? [Thé, 2002]

My point in emphasizing the role of the programmer-as-designer is not to say that specialist designers are a bad idea. Nor am I suggesting the programmers working on a project where someone else does the designing should revolt and ignore the designs they are given. My point is, other than those few projects that explicitly embrace the programmer-as-designer idea (such as XP projects), there is a danger in not recognizing the inherent design role of programmers.

The danger exists on both sides of the hand-off. Whittaker and Atkin wrote, "The nature of designs is that they abstract many details that must eventually be coded." [Whittaker, 2002, p.108] Designers then must be sensitive to the kinds of things they are abstracting in their designs (that is, what blanks they are leaving for programmers to fill), and programmers must realize that a certain amount of design is part of their job and to do their best to a) hone their design skills so that their designs will be the best possible, and b) to design consistently within the framework of the designs they are given.

On a recent project I was tasked with designing a set of PL/SQL stored procedures that would create and move a large amount of critical application data. I had three advantages: the database was designed largely by me, I had spent a lot of time in the company for which we were building this software, and I had intimate knowledge of the structure these procedures needed to have and the logic they needed to follow. I had two disadvantages: I had a lot of procedures to design under an extremely tight deadline, and the two developers who were assigned to write these procedures from my designs were brand new to the project and the domain. They had very little knowledge of the database or even what this complex system was all about. Also, having never worked with them before, I had no knowledge of what kind of coders they were. To top it off, the QA team that would be putting these procedures through their paces did not have the kind of knowledge or documentation they would need to test them.

Given this set of circumstances, I wrote very detailed designs. I spelled out every single detail, using elaborate pseudocode—even writing many of the SQL statements the programmers would need—and adding extra expository explanation wherever I could. When I delivered the designs to the developers, it was as close as it gets to those software engineering books Whittaker and Atkin were making fun of. I could not leave anything to chance in terms of the programmers’ misinterpreting the intent or logic of any part of the design. The programmers even joked about how it wasn’t too much fun writing the code when everything was spelled out for them that way. But we got those procedures done within the deadline, and they were quite solid.6

Even in this example, though, the programmers did have to interpret certain things about my designs, and they had to come back to me to get clarifications and fix mistakes I had made. Furthermore, there really was a fair amount of design decision-making for them to do. They had to turn my pseudocode into real PL/SQL code, they had to design an error handling scheme, and they had to design internal data structures.

In another situation on a different project (this one a maintenance project), I needed to design a feature that a programmer on my team was going to build. I had done the analysis with the business people, and I had a vision about how to make the new feature work. This time, however, I had a programmer who had been on the project for awhile, and he had good knowledge of the domain and the existing system. Furthermore, I knew him to be an excellent coder with great design skills and instincts. In this situation, I knew that I did not have to write a highly detailed design. I wrote up a short requirements/design document that described the feature in general terms, explained where it fit in with the existing application, spelled out a few key business rules and constraints, and described a rough user interface concept, with some specifics where necessary. I laid out some rough class and database design ideas, but explicitly spelled out in the design that the developer was free to design it the way he thought best.

The developer and I had a short meeting, and I handed the design document over to him, requesting that he keep it up to date as he finished the design and built the user interface and code. Also, I pointed him to a business person that he could go to if he needed some requirements clarifications and asked him to keep me in the loop. Here we have one of those "middle realities" we talked about earlier.

As a final example, I worked on another project where I was not the designer. I observed that the designers were not being sensitive to the kinds of concerns described in the previous two examples. The designs they created and handed to the developers were incomplete, as all designs are, but not in the appropriate ways. The developers had not been in the requirements and design meetings, they were not domain experts, and the project did not have good analysis documentation. The designers left critical business rules out of the documents, and left many crucial points open to interpretation with vague language.

As you can imagine, the developers were very frustrated in this situation. They had to scramble around asking various people questions to try to fill in the blanks. They had to go back to the designers to ask for clarifications, and the designers would get frustrated too. When the developers did make an interpretation and design things as they thought best, the designers would come back and say "No, no, that’s not the way it’s supposed to be," which of course caused even more frustration on both sides.

Now, one could argue that these designers were simply creating poor designs, but I think there was more to it than that. It was not that the designs were bad, but rather that they were incomplete at a level that was inappropriate to the situation. In my opinion, these designers were not sensitive to the fact that the developers would have responsibility for some portion of the design, and more importantly, they were not sensitive to the portions of the design for which the developers were equipped to have responsibility. The designs were not tailored to the process, the situation, or the audience, and the designers erroneously viewed their own designs as "complete."

All of this discussion about the relationship between design and construction is intended to underscore three points: first, designers who create specifications that will be handed off to other people must be aware of the inherent incompleteness of any design. Consideration must be given to the audience for the design, as well as situational concerns such as schedule and risk.

Second, it is critical that programmers have an intimate knowledge of the principles and techniques of software design; study is required. Programmers design when they code. The only difference for a programmer from one situation to the next is a matter of degree: at one time you might be working on a project that keeps as much design decision making upstream as possible, and at another time, you might find yourself on a team where most or all of the design decisions occur during construction. In either case, your knowledge of design has a direct effect on the quality of your work and the quality of the resulting product. Mastering your language, tools, and platform is not enough.

Third, it is critical for managers and leaders in charge of development projects to recognize the inevitable role of design in the construction phase and to ensure that the overall process creates opportunities to leverage that fact as an advantage. To ignore the fact that your programmers are also designers, and that the designs handed to them can never be complete, is to invite the most expensive kinds of errors, quality problems, and schedule overruns.

Automating repetitive procedures can provide real value to software development projects. In this article, we will explore the value of and barriers to automation and provide some guidance for automating aspects of the development process.

Although few experienced developers and project managers would argue the merits of automating development and testing procedures, when push comes to shove many teams place a low priority on implementing automated processes. The result is usually that, if automation is considered at all, it is given lip service early in the project life cycle, but falls quickly by the wayside.

Experience teaches us over and over again that trying to run a "simple" project by implementing a series of "simple" manual protocols, backed by "simple" written (sometimes, even just verbal) instructions, just doesn’t work well. Even so, many of us still tend to allow ourselves to start the next project with the thought that the manual, protocol-based method will "do just fine."

After all, aren’t we all professionals? Can’t we read a set of simple instructions and just be disciplined enough to follow those instructions when the time is right? Isn’t it a waste of time and money to invest in automating procedures for such a small project? The development team is only a half-dozen people after all—and the argument against automation goes on and on.

If you’re a developer or tester who enjoys spending your time actually adding value to your project, rather than repeating the same routine tasks over and over, you’ll want to consider advocating the concept of automation to your team (especially, to your project manager). If you’re a project manager who’s committed to maximizing the talents and time of the members of your technical team, as well as minimizing the risk of your project failing to deliver on time and on quality, you will want to encourage your team to invest the necessary time and effort required to automate the types of tasks that will be identified in this article.
Why Should I Automate?

You may already be familiar with many of the benefits of automating development processes. Some of the more commonly cited ones are:

Repeatability. Scripts can be repeated, and, unless your computer is having a particularly bad day, you can be reasonably certain that the same instructions will be executed in the same order each time the same script is run.

Reliability. Scripts reduce chances for human error.

Efficiency. Automated tasks will often be faster than the same task performed manually. (Some people might question whether gains in efficiency are typical, noting that they have worked on projects where, in their view, trying to automate tasks actually cost the project more time than it saved. Depending on the situation, this may be a real concern. In addition, automation might have been implemented poorly or carried too far on some projects—but keep reading for more on what to automate, and when.)

Testing. Scripted processes undergo testing throughout the development cycle, in much the same way the system code does. This greatly improves chances for successful process execution as the project progresses. Automated scripts eventually represent a mature, proven set of repeatable processes.

Versioning. Scripts are artifacts that can be placed under version control. With manual processes, the only artifacts that can be versioned and tracked are procedure documents. Versioning of human beings—the other factor in the manual process equation—is unfortunately not supported by your typical source control system.

Leverage. Another big benefit to automating is that developers and testers can focus on the areas where they add real value to a project—developing and testing new code and features—instead of worrying about the underlying development infrastructure issues.

For example, instead of requiring everyone to become intimately familiar with all the little nuances of the build procedure, you can have one person focus on automating the build and have that person provide the team with a greatly simplified method of building, hopefully as simple as running a command or two. Less time spent on builds leaves more time for the tasks that add the most value to the project.
What Should I Automate?

If you’re convinced that automating your development processes is a good idea, the next logical question is which processes should be automated. While the answer, to some extent, is different for every project, there are some obvious ones, as well as some general guidelines that I’d like to offer. Some of the typical targets for automation are:

    Build and deployment of the system under design.
    Unit test execution and report generation.
    Code coverage report generation.
    Functional test execution and report generation.
    Load test execution and report generation.
    Code quality metrics report generation.
    Coding conventions report generation.

The above list is obviously not exhaustive, and every project has its own unique characteristics. Here’s a general, and perhaps obvious, rule of thumb to help identify any process that should be considered for automation: Consider automating processes that you expect to be repeated frequently throughout a system’s life cycle. The more often the procedures will be repeated, the higher the value of automating them.

Once a process has been identified, spend a little time investigating how you might be able to automate the process, including researching tools that could assist with automation, and estimating the level of effort required to implement the automation versus the total cost and risk of requiring team members to manually perform the procedures. As with any other business decision, it really should come down to a cost versus benefit analysis.

You probably noticed that the term "report generation" appears in the above list of automation candidates. The repetition points out another important aspect of automating development processes: the end result of every automated process execution should be a report that is easily interpreted by the team. Ideally, such reports will focus on making anomalous conditions (for example, test failures) obvious at a glance. Also, these reports should be readily accessible to the appropriate team members.

In many situations, it’s even a good idea to "push" reports to the team (perhaps via email or RSS), instead of requiring people to remember to go out and search for them. The basic idea is that the development team should be notified as soon as possible when problems are introduced to the system or environment. A side benefit is that management is provided a more tangible real-time view into the progress and health of the project than is possible with team status reports alone.
When Should I Automate?

Consider automation as soon as you realize that team members are starting to execute a process that meets the criteria discussed in the previous section. For example, automating the build process when the project is nearly over provides very little benefit. It can, however, save dozens, if not hundreds of hours when automated as soon as development begins.

However, before you go off and start automating everything, one caution: be reasonably certain that the procedure will be required for your project, that it will be repeated more than two or three times during the project’s life cycle, and that you understand the steps that need to be executed for the procedure. There are some things you just don’t do very often, and in these cases the costs may outweigh the benefits.

Even for processes where automation is warranted, if you’re truly guessing at the steps involved, there’s a high likelihood that you’ll end up re-writing the whole thing. Re-writing is much different than fine-tuning or refactoring the automated scripts. Refactoring is expected, but scrapping and starting over again means you tried to automate before you understood the process you were trying to automate.

Another danger is allowing your project to become the "proving ground" for automation for the entire organization. If your organization doesn’t already have the infrastructure in place to support development automation, you should provide what makes sense for your project, but try not to allow your team to lose sight of your project’s goals. A project team whose goal is to deliver working software is in no position to develop enterprise-wide strategies and tools. Trying to do so is asking for trouble.

If outside entities begin to get involved in these efforts, I’d suggest that you recommend that a separate project be undertaken to work out the automation infrastructure for the enterprise. Members of this "automation project" team are free to review your project’s implementation for ideas or for use as a launching point. Once this infrastructure is in place, the question of When to automate? is easier to answer for future projects, and less costly.
Obstacles to Automation

If there are so many benefits to automation, why don’t we see more of it on software projects? All software development teams struggle to balance the need to show immediate results with the long-term goals of the project. There are many obstacles to implementation of development process automation. Here are a few of the more common ones:

    Stakeholder pressure. Faced with the desire to show early and rapid progress, teams often overlook or choose to ignore practices that do not seem, at first glance, to directly contribute to getting working code up and running as quickly as possible.
    Focus on core requirements. The team has an overwhelming desire to begin producing working code and tests as soon as possible. Writing code that can be demonstrated is much more satisfying to developers than writing automation scripts.
    Lack of management support. Management may not have a good understanding of automation—how it works and/or the costs and benefits.
    Lack of organizational support. There is no enterprise-wide policy or infrastructure for development automation (standards, tools, expertise, etc.)
    Lack of follow through. Even with the best of intentions, teams can quickly lose their commitment to plans for implementing automated processes.

As with any fundamental change, there must be someone who’s both committed to the concept and who has the authority to make sure that teams follow through with the plan. Not following through is much worse than never having taken the time to investigate and discuss automation. First of all, there are probably other problems that you could have successfully solved using the time and resources and, second, if you fail to implement any automation, when the idea surfaces again people in the organization will point out that it didn’t work the "last time we tried it".
Selling Automation

As implied in the preceding section, the primary obstacle to automation is short-term thinking. Therefore, your primary task is to get your team and management to pause to think about overall project costs and schedule, instead of focusing solely on meeting next week’s promised deliverables. The message should be clear. Meeting milestones is critical, but it’s very possible that you could meet that next milestone to the detriment of the project. But before you try to convince people who are under a great deal of pressure that it sometimes makes sense to slow down momentarily in order to improve the chances for success, you’d better have more than a set of maxims at hand.

This is where Return On Investment (ROI) can help. Most people in the business world, regardless of field, have at least a basic understanding of ROI and agree that, when based on valid assumptions, it can be one of the best mechanisms for evaluating whether or not to take action or adopt a particular solution. The process of calculating ROI is well beyond the scope of this series of articles. However, the Suggested Reading section at the end of the article provides a link to an outstanding article from Edward Adams that describes how to do this, as well as some additional advice on selling the case for test automation, in a very straightforward and easy to understand way.

Although many managers really like to see quantifiable benefits, such as ROI, before making major decisions about adding tasks to the project schedule, others have sufficient experience with software development and are just as comfortable with a common-sense explanation of automation’s benefits. With that in mind, rather than going into a full-blown ROI calculation, let’s just take a look at one scenario that most projects will face at some point to see, from a common sense perspective, the likely outcome of a purely manual process versus a process that had been automated early on in the project.

Manual testing approach. Your team is in the final month of a twelve-month project, and the system test team is starting to perform regression testing by manually re-running the entire set of test cases. Some test cases haven’t been executed in several months, because, frankly, the testers have just been spread too thin, and, due to perceived pressure to deliver, management chose to move testers onto other tasks as soon as they were finished wrapping up a set of test cases.

Regular regression testing was put into the schedule early on, but repeating the same tests over and over throughout the project was (whether or not anyone wants to admit it) eventually considered a "nice to have" and definitely much less critical than other, more pressing tasks. Now some of the regression tests are failing and the test team is logging defect reports.

The development team decides to manually re-run the entire unit test suite (which also hasn’t been executed in a few months, because it wasn’t automated) to hopefully zero in on the source of the problem. The developers find two problems as a result of the unit test run: first, apparently there were never any unit tests written for the parts of the system in question; and second, there are other, seemingly non-related unit tests that are failing. To make things worse, the components that are failing were developed by someone who’s no longer with the team. Even if he were available, he’d probably require significant spin-up time to re-acquaint himself with the failing code, since he hasn’t even looked at it for months.

Automated testing approach: Early on in the project, the decision was made to automate at least the following: unit testing, code coverage, and system testing. The same bug that caused the situation above was introduced in month three of the project. The code coverage report that’s automatically generated as part of the daily build clearly indicated to the development team that there were no unit tests for the related components as soon as those components were placed under source control. Therefore, the developer went back and immediately implemented the unit tests.

After implementing the unit tests and working out the bugs that were initially revealed as a result, the developer checks the unit tests into source control. The daily build automatically picks up the new unit tests, the coverage report reflects that the components are now being tested (as well as to what extent they’re being tested), and the unit test report indicates test success or failure each day. As soon as any change is made to the system and placed under source control, the team will become aware of the impact that change has on the overall system, while the changes are still fresh in the minds of the implementer.

The system tester who’s responsible for testing that same set of features now designs his test case, implements the tests, reports any defects he discovers, etc. until he feels that the code and test case are operating as expected. He then uses the test automation tools to record the execution steps for the test case and checks the resulting scripts and files into source control. The next day, the automated test runner picks up his new test case for execution and the results become available to the test team as part of the daily test run. The development and test teams are leveraging the automated tools and processes for proactive detection as problems are introduced.

Without performing a detailed ROI analysis, which of the two above scenarios is intuitively more desirable? Assuming that the exact same bug is introduced into the system at the same time—month three of the project—in each case, which approach do you think will result in the least expense and least risk? Which will result in the least stress to the team and to management?
Start Small

Unless you’re lucky enough to have a team with multiple members who’ve had a good deal of experience automating development processes, you won’t want to try to take the idea of automation to the extreme. Trying to do so with an inexperienced team or a team that’s not completely sold on the idea of automation will likely result in failure, as both expectations and barriers are set high from the start. Pick one or two areas where you believe there is both high potential value for automation and a low risk that the team will fail implementing automation. You might ask yourself the following questions:

    Which processes will likely be exercised the most frequently?
    Does someone on the team have the experience and/or skill set to implement the automated process?

It’s better to succeed in a small way than to fail in a big way. Your successes just might build on themselves, and when you suggest additional areas for automation, either later in the project or on your next project, you may face little or no resistance at all. To reach the ideal takes a long-term plan, management commitment, adequate tools and training, and of course time. If it were easy, everyone would already be doing it.
Conclusion

I’m well aware that there may be other factors that affect the decision of whether and what to automate than those we’ve addressed in this article. For instance, you may find that the only automation tools your organization will approve for your team’s use are beyond your budget or that, due to your project’s special needs, there are more important factors. Maybe you’re a vendor and your marketing department has determined that time-to-market for your first release is more critical than the quality or the long-term success of the current product.

I’m a big believer in the "it’s all about trade offs" principle. To gain one advantage, you typically accept other disadvantages, and in the end you should try to make decisions that tip the scales most in favor of the "advantages." I would say, however, that if your project is truly one of those projects that is better off not automating, you’re in the minority category—the exception, not the rule. If the primary obstacle to adoption of automation for your organization is lack or expense of tools, there are several free open source tools that can go a long way towards solving that problem. The time you spend investigating automation strategies and tools could very well make the difference in whether your projects successfully meet schedule and quality goals.

It has been six years since I started working on performance testing. Applications I have worked on include an electronic health records application used by more than two hundred thousand physicians nationwide and a backend application that processes millions of lab orders, results, and other messages daily. High availability, high reliability, and high performance are critical for both applications due to their time-sensitive nature.

From knowing very little to becoming the leading engineer of a performance testing team of five, I have learned a lot about this art—and at the same time have made many mistakes. Based on lessons learned from those mistakes, I have come up with a list of suggestions that might help individuals or teams who are new to performance testing or are interested in it.

My suggestions include the following (not listed in any specific order):

    Always start from the simple
    Be a meticulous and active observer
    Be diligent and prepared
    Get involved and make friends with people around you
    Promote awareness of performance testing
    Be disciplined
    Manage your time
    Seek guidance from the experienced
    Know a little bit about everything and keep learning

I will elaborate on these suggestions and explain why each is important here and in two follow-up articles.

Always start from the simple

Sometimes, trying to determine the cause of a performance issue in a fairly simple system is challenging enough, let alone in a complex system where data flow through many interfaces before reaching their final destinations. Therefore, when testing a new system or when investigating performance problems, it is better to start with the individual components.

During a release a couple of years ago, my regression test for a web service showed a linear increase in response time and that the database CPU usage stayed at 100 percent during the test. We devoted all the resources we could spare and attempted many fixes, including backing out all the changes made for that release, rebuilding indexes of a few heavy-hit tables, revisiting some of the queries, and running numerous tests to rule out potential causes. But all efforts were in vain.

I happened to be registered for a software testing conference that year and presented the problem to attendees during a round table session. One of the suggestions given was to simplify the problem—more specifically, to bypass the business layer by recording testing scripts using the JDBC protocol directly against the database and run them one at a time.




















 
































 











			
	
			
	
	
	
	
	
	
	
	
	
	
	
	



